[{"content":"The first time I\u0026rsquo;ve heard the term \u0026ldquo;blockchain\u0026rdquo; was around 2014. Since then, its popularity has grown rapidly. However, I\u0026rsquo;ve never actually understand what blockchain is exactly, until recently. In fact, I didn\u0026rsquo;t really understand the difference between blockchain and bitcoin. For me, blockchain is clubbed with cryptocurrencies. So here is a short summary of what blockchain is and why people use blockchain.\nWhat is Blockchain I tried reading the articles about blockchain before, and it didn\u0026rsquo;t take long before I was completely overwhelmed by technical terms: consensus, asymmetric crypto, consistency, etc. It\u0026rsquo;s hard to combine all these little pieces together and form a big picture. Instead, it\u0026rsquo;s much easier to understand block-chain from a top-to-bottom view. Even better, a small step-by-step example can clarify much of the confusion. I like Prof. Anand\u0026rsquo;s example given in the class slides:\n  Supposed we own a comic book store, and we want to sell comic books to some customer.\n  Every time we sell a book, 10 of my friends will record the action. Traditionally, we refer to such record as \u0026lsquo;\u0026lsquo;ledger\u0026rsquo;\u0026rsquo;. In the world of blockchain, we call this \u0026lsquo;\u0026lsquo;distributed ledger\u0026rsquo;\u0026rsquo;.\n  Once we sell enough comic books, the 10 records (ledgers) will be collected into a book, all of my friends will get a copy of the book. This is very important because we use duplication to achieve consensus.\n  To make things more secure, all these books are stored in a secure vault. In digital world, we achieve this though ways of encryption, digital signature, and so on. An attacker need to tamper many copies of such book to disrupt our selling records, which tends to be extremely hard in real world scenario.\n  Now we have this secure vault, which is effectively an immutable block. This block (vault) stores the record of we selling a comic book. If we decide to sell more books, each one will generate an additional block (vault). Each block is appended after the previous block, forming what we call a \u0026lsquo;\u0026lsquo;blockchain\u0026rsquo;\u0026rsquo;.\n  In essence, a blockchain is a series of immutable blocks, each storing the information of an event(s) whose validity is approved by a majority of other participants. Simple as that.\nI like using the term \u0026lsquo;\u0026lsquo;distributed ledger\u0026rsquo;\u0026rsquo; to characterize blockchain. In Prof. Anand\u0026rsquo;s slides, this graph summarizes how a distributed ledger differs from traditional centralized ledger:\nThe main difference is how consensus is achieved. In centralized ledger, we have a single that decides the \u0026lsquo;\u0026lsquo;golden record\u0026rsquo;\u0026rsquo;. In a distributed ledger, consensus is achieved is everybody agrees with it. To give an example, we would pay a 45-dollar electricity bill each month to Texas electricity company because the price standard is set by the company alone. In a distributed ledger world, we might pay 32 dollars instead, if every single residents living in the building agrees this is the best price. Essentially, we eliminate the centralized entity and distribute the ability of making decisions to each individual evenly.\nIn super-simple terms, a blockchain is just a computer file for storing data. The reason why it\u0026rsquo;s so secure is because there doesn\u0026rsquo;t exist a single central point of attack for hackers to target.\nHow is Bitcoin related to Blockchain Now we understand what a blockchain is, it\u0026rsquo;s time to find out how blockchain enables the development of digital currencies such as Bitcoin. There are many great articles talking about Bitcoin in details, but I found the original paper extremely helpful in terms of understanding the motivation behind Bitcoin. In essence, Bitcoin was introduced to eliminate one problem: the need of a trusted third party to process electronic payments. More abstractly, shift from trust-based system to cryptographic-proof-based system.\nTha paper claims that trust based model suffers from a fundamental weakness: the need of mediation. The logic is simple: mediation is required in the presence of disputes. Disputes means making non-reversible transactions more difficult, thus comes the possibility of reversal. Reversal causes the need for trust to spread. To establish trust, more price needs to be paid, in the form of money, personal information, etc. Essentially, the need of trust creates a centralized component that participants must rely on. In theory, Bitcoin resorts to a cryptographic-proof based system to replace the trust-based system, with the difference being that cryptographic-proof based system is distributed in nature.\nImaging a distributed system as a fully connected graph with \\(n\\) nodes where each node represents a buyer/buyer. A transaction represents an edge connecting two nodes together. We denote set \\(T\\) as currently ongoing transactions, there could be as many as \\(n(n-1)\\) transactions going on concurrently, and each transaction \\(t \\in T\\) is independent to each other. There enables 1) extremely scalability; 2) on reliance on central components. If a transaction is committed to a Bitcoin network, it suggests that the transaction has already gained approval from both the buyer and seller side (why this is the case is more technical, and you should Google how symmetric and asymmetric encryption work).\nIf we imaging a centralized system, where every buyer node is connected to one node \\(c\\). Node \\(c\\) in turn is connected to every seller node \\(s\\). Assume the centralized component, or node \\(c\\), has a fixed capacity limiting the amount of traffic flowing thought it in any given moment. To achieve the same level of information flow in a distributed system, we need to increase node \\(c\\)\u0026rsquo;s capacity, which represents the increased costs of mediation. Assume a buyer node \\(b\\)\u0026rsquo;s output value is different from a seller node \\(s\\)\u0026rsquo;s input value (disputes), extra information flow will be required from seller node \\(s\\), creating the need for more capacity at node \\(c\\), thus driving the cost.\nFrom an abstract point of view, I\u0026rsquo;d like to imaging a normal transaction in a Bitcoin network as follows:\n  Transaction is initiated\n  The buy, the seller, and all witnesses agree with the validity of such transaction.\n  With everyone satisfied as the precondition, transaction completes. If there exists a disagreement, transaction doesn\u0026rsquo;t happen.\n  On the other hand, in a centralized system, the transaction happens as follows:\n  Transaction is initiated\n  Seller received payments, but there\u0026rsquo;s a mismatch\n  Now the centralized component must be engaged to mitigate the issue.\n  More shit happens, the centralized component must constantly nag both the buyer and the seller until the problem is solved.\n  Until I find better terms to capture my thoughts, I would put these two types of transactions into two different catagories: condition-oriented and process-oriented. Condition-oriented (Bitcoin) means we can achieve global optimum (in our case, a successful transaction) if a precondition is met (consensus). Process-oriented (Centralized) means, instead of agreeing on a precondition, we achieve global optimum through constant trial and error, which comes at a cost of disputes.\nIn short, I think trust is not \u0026lsquo;\u0026lsquo;removed\u0026rsquo;\u0026rsquo;, it is merely achieved through a different way. I\u0026rsquo;d like to modify Prof. Anand\u0026rsquo;s summary on Bitcoin: Bitcoin is an engineering solution to solve trust issues.\n","permalink":"https://www.bodunhu.com/blog/posts/blockchain/","summary":"The first time I\u0026rsquo;ve heard the term \u0026ldquo;blockchain\u0026rdquo; was around 2014. Since then, its popularity has grown rapidly. However, I\u0026rsquo;ve never actually understand what blockchain is exactly, until recently. In fact, I didn\u0026rsquo;t really understand the difference between blockchain and bitcoin. For me, blockchain is clubbed with cryptocurrencies. So here is a short summary of what blockchain is and why people use blockchain.\nWhat is Blockchain I tried reading the articles about blockchain before, and it didn\u0026rsquo;t take long before I was completely overwhelmed by technical terms: consensus, asymmetric crypto, consistency, etc.","title":"Blockchain"},{"content":"Hoare logic forms the basis of all deductive verification. To illustrate Hoare logic, we will first consider a smaller imperative programming language IMP.\nIn IMP, we have three program constructs: expressions, conditionals, and statements:\n  Expression takes the form \\( E := Z\\ |\\ V\\ |\\ e_1 + e_2\\ |\\ e_1 \\times e_2 \\)\n  Conditional is self-explanatory: \\( C := true\\ |\\ false\\ |\\ e_1 = e_2\\ |\\ e_1 \\leq e_2 \\)\n  Statement consists of several different forms:\n \\(S := V := E\\) (Assignment) \\(S_1; S_2\\) (Composition) if \\(C\\) then \\(S_1\\) else \\(S_2\\) (If) while \\(C\\) do \\(S\\) (While)    Hoare Triple In Hoare logic, we specify partial correctness of programs using Hoare triples:\n\\[\\{P\\} S \\{Q\\}\\]\nHere \\(P\\) is the precondition and \\(Q\\) is the post-condition. $S$ is a statement in IMP.\nThe interpretation of Hoare triple is as follows:\n  if \\(S\\) is executed in a state satisfying \\(P\\)\n  and if execution of \\(S\\) terminates\n  then the program state after \\(S\\) terminates satisfies \\(Q\\)\n  Here an example, \\(\\{x = 0 \\} while\\ true\\ do\\ x := 0\\ \\{x = 1 \\}\\) is a valid Hoare triple because the execution of the statement never terminates, thus satisfying the requirement posed by Hoare triple.\nThus the specification \\(\\{P\\} S \\{Q\\}\\) is called partial correctness spec, because it doesn\u0026rsquo;t require \\(S\\) to terminate.\nThere is also a stronger requirement called total correctness. The total correctness specification is written as:\n\\[ [P] S [Q]\\]\nTotal correctness requires that if \\(P\\) is satisfied when executing \\(S\\), then \\(S\\) must terminate, and the post-conditional \\(Q\\) must be satisfied after \\(S\\) terminates.\nThus the example \\(\\{x = 0 \\} while\\ true\\ do\\ x := 0\\ \\{x = 1 \\}\\) is no longer valid because it never terminates.\nIn summary, we can say that Total correctness \\(=\\) Partial correctness \\(+\\) termination.\nProving Partial Correctness We use \\(\\vDash \\{P\\} S \\{Q\\} \\) to say a Hoare triple is valid and we use \\(\\vdash \\{P\\} S \\{Q\\} \\) to indicate we can prove validity of a Hoare triple.\nLet\u0026rsquo;s say we are given an assignment \\(x := y \\) with post-condition \\(x \u0026gt; 2\\). The question is, what do we need to know before the assignment happens so that the post-condition, \\(x \u0026gt; 2\\), holds afterwards?\nTo prove \\(Q\\) holds after the assignment \\(x := E\\), we need to show that \\(Q\\) with \\(E\\) substituting \\(x\\) holds before the assignment. Formally, we write it as:\n\\[\\vdash \\{Q[E / x]\\}\\ x := E \\{Q\\}\\]\nFor example, given \\( \\{ x+1 = n\\}\\ x := x+1 \\ \\{x=n\\} \\), we know this formula is provable because we can take \\(Q\\), which is \\(\\{x=n\\}\\), substituting \\(x\\) with \\(x+1\\) given we need to replace it with \\(E\\), and we will convert \\(x=n\\) to \\(x+1 = n\\), which matches the precondition.\nHere is another interesting example, given \\( \\{z = 2\\}y:= x \\{y = x\\} \\), this Hoare triple is valid but not provable. If we use the above substitution procedure, it will result in the precondition being \\(x=x\\) which is always true but is also different from the original precondition \\(z=2\\).\nIntuitively, we can prove the post-condition \\(y = x\\) given the statement \\(y = x\\) without any assumptions, so even if we do have assumptions like \\(z=2\\), we should still be able to prove it, and here comes proof rule for precondition strengthening.\nProof Rule for Precondition Strengthening Formally, we define precondition strengthening as:\n\\[ \\frac{ \\vDash \\{P'\\} S \\{Q\\}\\ \\ P \\Rightarrow P' }{\\vdash \\{P\\} S \\{Q\\}} \\]\nNow, with the original formula \\( \\{z = 2\\}y:= x \\{y = x\\} \\), we would derive \\( x= x \\equiv true \\). and since \\(z=2 \\rightarrow true\\) is valid, we can now prove the formula!\nA Dual: Post-Condition Weakening Formally, we define post-condition weakening as:\n\\[ \\frac{ \\vDash \\{P\\} S \\{Q'\\}\\ \\ Q' \\Rightarrow Q }{\\vdash \\{P\\} S \\{Q\\}} \\]\nWhat this means if that if we can prove a post-condition \\(Q'\\), we can always relax it to something weaker.\nFor example, given that \\(\\vdash \\{true\\}S\\{x=y \\land z=2\\}\\), we can prove \\(\\{true\\}S\\{x=y\\}\\) because \\(x=y\\) is a weaker condition of \\( x=y \\land z=2 \\).\nProof Rule for Composition For composition, we define the rule as:\n\\[ \\frac{ \\vdash \\{P\\}S_1\\{Q\\}\\ \\ \\vdash \\{Q\\}S_2 \\{R\\} }{ \\vdash \\{P\\}S_1;S_2\\{R\\} }\\]\nI won\u0026rsquo;t show why this is true, so this will be left as an exercise.\nProof Rule for If Statements Naturally, we define the rule for if statement as:\n\\[ \\frac{_{ \\vdash \\{P \\land C\\} S_1 \\{Q\\} }^{ \\vdash \\{P \\land \\neg C\\} S_2 \\{Q\\} }}{ \\vdash \\{P\\}\\ if\\ C\\ then\\ S_1\\ else \\ S_2 \\ \\{Q\\} } \\]\nIn summary, this means given we know \\(P\\) is true, no matter what \\(C\\) evaluates to, we will come to the same post-condition \\(Q\\). If you still don\u0026rsquo;t understand it, just stare at it for five minutes and you should figure out why this is the case:)\nProof Rule for While To understand the proof rule for while statement, we need to first understand a simple concept: loop invariant\nLoop Invariant Loop invariant \\(I\\) has two properties:\n  \\(I\\) holds initially before the loop\n  \\(I\\) holds after each loop iteration\n  For example, given a loop\ni := 0; j := 0; n := 0; while i \u0026lt; n do i := i + 1; j := i + j Here, \\(i \\leq n \\) is a loop invariant but \\(i \u0026lt; n \\) isn\u0026rsquo;t.\nNow, we put the properties of loop invariant \\(I\\) in formal terms. Given that the precondition before a loop executes is \\(C\\), by definition, \\(I\\) holds initially before the loop, we know \\(I \\land C\\) holds.\nFor the second property of loop invariant, it specifies \\(I\\) holds after each loop iteration. So that means \\(\\{ I \\land C\\ \\} S \\{I\\} \\) holds. Formally, we express loop invariant as \\( \\vdash \\{P \\land C\\} S \\{P\\} \\).\nNow, we know if a loop terminates, it must be that condition \\(C\\) no longer holds, meaning \\( P \\land \\neg C \\) must be true after loop terminates. This is because \\(P\\) is a loop invariant and always holds after each loop iteration, including termination.\nPutting all this together, we form the proof rule for while loop:\n\\[ \\frac{ \\vdash \\{P \\land C\\} S \\{P\\} }{ \\vdash \\{P\\} while \\ C \\ do \\ S\\{P \\land \\neg C\\} }\\]\nInductive Loop Invariant It\u0026rsquo;s not always the case that we can prove loop invariant is valid. Here is a counter example:\nConsider precondition \\( I = j \\geq 1 \\) and the code is:\n\\[i := 1; j := 1; while \\ i \u0026lt; n\\ do\\ \\{j := j+i; i ;= i + 1\\}\\]\nWe know that the precondition is \\(I = j \\geq 1\\) and \\(C\\) (loop condition) is \\(i \\leq n\\). So we have a Hoare triple:\n\\[ \\{ j \\geq 1 \\land i \\leq n \\} j =j + i;\\ i = i + 1; \\ \\{j \\geq 1\\} \\]\nWe could simply set \\(i = -100\\), then if we execute the code once we will not be sure if the post-condition \\(j \\geq 1\\) holds.\nHowever, if we have strengthened invariant such as \\(j \\geq 1 \\land i \\geq 1\\), the new Hoare triple will be valid. Then \\(I\\) will become inductive invariant because we can prove these invariant.\nTo put everything in action, here is an example showing how to find inductive loop invariant to prove the following Hoare triple:\n\\[ \\{i = o \\land j = o \\land n = 5\\} \\] \\[while\\ i \u0026lt; do\\ i := i + 1; \\ j := j + i; \\] \\[\\{j = 15\\} \\]\nIf we have \\( j = \\frac{i(i+1)}{2} \\), this is a loop invariant because we can prove that:\n\\[\\{j = \\frac{i(i+1)}{2} \\land i \u0026lt; n\\} i = i + 1;\\ j = j+ i\\ \\{j = \\frac{i(i+1)}{2}\\} \\]\nIf we conjoin this condition with \\(i \\geq n\\) as the post-condition, however, we can\u0026rsquo;t really show that \\(j = 15\\) is true for the given Hoare triple.\nIf we also add condition \\(n = 5\\) and \\(i \\leq n\\), and we conjoin this with the end-loop condition \\( i \\geq n\\), we would realize that \\( i = n = 5\\), and thus prove that \\(j = 15\\) for the given Hoare triple.\nHow we get \\(j = \\frac{i(i+1)}{2}\\) is, however, not trivial to solve, and requires some human effort in program verification.\nBasic Idea behind Program Verification Automating Reasoning in Hoare Logic It\u0026rsquo;s reasonable to automate the tedious parts of program verification: proving correctness. The basic idea to assume an oracle (human or another program) gives loop invariants but automate the rest of the reasoning.\nAutomating Hoare logic is based on generating verification conditions (VC). Essentially, a verification condition is formula \\(\\phi\\) s.t. program is correct iff \\(\\phi\\) is valid.\nThere are two way to generate verification conditions: forwards and backwards.\nAs their name suggests, a forwards analysis starts from precondition and generates formulas to prove post-condition. Forwards technique computes strongest post-conditions (sp). In contrast, backwards analysis starts from post-condition and tries to prove precondition. Backwards technique computes weakest preconditions (wp).\nHere, we start from backwards method.\nWeakest Preconditions Formally, we define the weakest precondition of \\(Q\\) with respect to \\(S\\) as \\(wp(S, Q)\\).\n\\(wp(S, Q)\\) has the property that it is the weakest condition (least amount of information we need to have) that guarantees \\(Q\\) holds after \\(S\\) in any execution.\nThus, Hoare triple \\( \\{P\\}S\\{Q\\} \\) is valid iff \\( P\\Rightarrow wp(S, Q) \\).\nWeakest preconditions are defined inductively and follow Hoare\u0026rsquo;s proof rules:\n  \\(wp(x := E, Q) = Q[E/x]\\)\n  \\( wp(s_1 ; s_2, Q) = wp(s_1, wp(s_2, Q) ) \\)\n  \\(wp(if \\ C\\ then \\ s_1\\ else \\ s_2, Q) =C \\rightarrow wp(s_1, Q) \\land \\neg C \\rightarrow wp(s_2, Q) \\)\n  However, for loops, we might not be able to compute the weakest preconditions exactly because there might be cases where we simply don\u0026rsquo;t know the number loops executed.\nThus, we relax our requirement by computing \\(awp(S,Q)\\) (\\(a\\) stands for approximate)) instead, hoping that \\(awp(S, Q)\\) is weak enough to be implied by \\(P\\) although it may not be the weakest.\nNow, assume all loops are annotated with invariants \\(while \\ C \\ do \\ [I]\\ S\\), we will just define \\(awp(while \\ C \\ do \\ [I]\\ S, Q) \\equiv I\\).\nHowever, there is another program, since \\(awp\\) is only an approximated condition, it doesn\u0026rsquo;t necessarily mean that if \\(P \\Rightarrow awp(S, Q)\\), \\( \\{P\\}S\\{Q\\} \\) is valid. There are two reasons:\n  We don\u0026rsquo;t know if the loop invariant \\(I\\) provided by the oracle is correct since it might be provided by human and we know human make mistakes.\n  Even if \\(I\\) is correct, we don\u0026rsquo;t if \\(I \\land \\neg C\\) is sufficient to establish \\(Q\\)!\n  Thus, for each statement \\(S\\), we need to generate verification condition (VC) \\( VC(S,Q) \\) which encodes additional conditions to prove.\nVerification Conditions So how do formulate VC generation rules for loops?\n\\[ VC(while\\ C\\ do\\ [I]\\ S,Q) = ?\\]\nFirst, we need to ensure that \\(Q\\) is satisfied after loop, which means \\( I \\land \\neg C \\Rightarrow Q \\).\nTo show that \\(I\\) is actually correct, we also need \\( \\{I \\land C\\} S \\{I\\} \\).\nThis implies that we need to show \\( I \\land C \\Rightarrow awp(S, I) \\). In case \\(S\\) contains nested loops, and also add \\(VC(S, I)\\)\nIn summary, to how that loop invariant \\(I\\) provided by the oracle is correct, we need to show \\( I \\land C \\Rightarrow awp(S,I) \\land VC(S, I) \\).\nTo show \\(I\\) is strong enough to establish \\(Q\\), we need to show \\( I \\land \\neg C \\Rightarrow Q \\).\nPutting this together, and to answer the two reason why \\(P \\Rightarrow awp(S, Q)\\), \\( \\{P\\}S\\{Q\\} \\) might not be valid, VC for a while loop \\( S' = while \\ C \\ do \\ \\{I\\} \\) is expressed as:\n\\[ VC(S', Q) = (I \\land C \\Rightarrow awp(S, I) \\land VC(S, I) ) \\land (I \\land \\neg C \\Rightarrow Q) \\]\nIn essence, verification condition simply stands for additional checks we need to verify before we can claim that, if an approximated precondition \\(P\\) is valid, \\( \\{P\\} S \\{Q\\} \\).\nThe verification condition for other statements is as follows:\n  For assignment, we don\u0026rsquo;t need any additional checks for precondition because if \\( P \\Rightarrow wp(S, Q) \\), it implies that \\( \\{P\\} S \\{Q\\} \\) is valid. Thus, \\( VC(x:= E, Q) = true \\).\n  For composition, we have \\( VC(s_1 ; s_2, Q) = VC(s_2, Q) \\land VC(s_1, awp(s_2 , Q)) \\).\n  For if statement, we have \\( VC(if \\ C \\ then \\ s_1\\ else \\ s_2, Q) = VC(s_1, Q) \\land VC(s_2, Q) \\).\n   Quick question: for if statement, why don\u0026rsquo;t we instead use verification condition generation rule: \\( C \\Rightarrow VC(s_1, Q) \\land \\neg C \\Rightarrow VC(s_2, Q) \\)?\n Here is a counter example. Suppose we have \\( S = if\\ (x \u0026gt; 0) \\ while (*) x - -; else \\ skip\\), and we have given loop invariant \\(x \\geq 0\\).\nIf we use the original rule \\( VC(s_1, Q) \\land VC(s_2, Q) \\), according to the verification condition generation rule for while loop, we would have to verify the loop invariant \\(I\\) is correct, and thus \\(VC(S, I) \\equiv \\{ x \\geq 0 \\} x - - \\{ x \\geq 0 \\} \\), obviously, this not true, and we can use this VC.\nHowever, if we instead use the rule \\( C \\Rightarrow VC(s_1, Q) \\land \\neg C \\Rightarrow VC(s_2, Q) \\). The VC would become \\( x \u0026gt; 0 \\Rightarrow (\\{ x \\geq 0 \\} x - - \\{ x \\geq 0 \\}) \\), which is valid, and we will include the wrong VC. Thus we can\u0026rsquo;t use this VC generation rule.\nVerification of Hoare Triple Thus, to show validity of Hoare triple \\( \\{P\\} S \\{ Q \\} \\), we need to compute:\n  \\( awp(S, Q) \\)\n  \\( VC(S, Q) \\)\n  Therefore, a Hoare triple is valid if the follow formula holds:\n\\begin{equation}\\tag{*} VC(S, Q) \\land P \\rightarrow awp(S, Q) \\end{equation}\nThus, if we can prove the validity of the above equation *, we know the program obeys specification.\n","permalink":"https://www.bodunhu.com/blog/posts/hoarelogic/","summary":"Hoare logic forms the basis of all deductive verification. To illustrate Hoare logic, we will first consider a smaller imperative programming language IMP.\nIn IMP, we have three program constructs: expressions, conditionals, and statements:\n  Expression takes the form \\( E := Z\\ |\\ V\\ |\\ e_1 + e_2\\ |\\ e_1 \\times e_2 \\)\n  Conditional is self-explanatory: \\( C := true\\ |\\ false\\ |\\ e_1 = e_2\\ |\\ e_1 \\leq e_2 \\)","title":"Hoare Logic"},{"content":"My name is Edward Hu, a Computer Science student in the SCEA lab under Dr. Christopher Rossbach at UT Austin. I spent most time doing system stuff and got confused. This site is intended to serve as the dumpster of my random thoughts. If you want to know more about me, please visit my main website.\nMy original intent to create this website is to focus on system-related matters. But I usually find myself forgetting important stuffs after a few weeks. So I often put other hard-to-read stuffs here as a reminder and confuse people. Therefore, you might find words in some posts hard to understand:)\nReach out to me The quickest way to reach me is to drop an email at bodunhu@utexas.edu. You can also shoot me a message on Twitter @BodunHu. You may feel free to shoot me a text and I will feel free to ignore you most likely.\n","permalink":"https://www.bodunhu.com/blog/about/","summary":"My name is Edward Hu, a Computer Science student in the SCEA lab under Dr. Christopher Rossbach at UT Austin. I spent most time doing system stuff and got confused. This site is intended to serve as the dumpster of my random thoughts. If you want to know more about me, please visit my main website.\nMy original intent to create this website is to focus on system-related matters. But I usually find myself forgetting important stuffs after a few weeks.","title":"About"},{"content":"This is a summary of how to compute congruence closure. I implemented the algorithm to compute congruence closure and thought I\u0026rsquo;d never forget it. But my memory starts to get blurry just after two days. So I figured I\u0026rsquo;d put things down so I don\u0026rsquo;t have to watch the entire lecture again the next time I need it.\n Equivalence Relation Equivalence relation has three properties: reflexive, symmetric, and transitive. (E.g. \\(\\geq\\) is not an equivalence relation because it break the symmetric property. $4 \\geq 6$ does not imply that $6 \\geq 4$) For example, a binary relation $R$ over a set $S$ meeting these three properties can be expressed as:\n Reflexive: $\\forall s \\in S.\\ sRs$ Symmetric : $\\forall s_1, s_2 \\in S.\\ s_1 R s_2 \\rightarrow s_2 R s_1$ Transitive: $\\forall s_1, s_2, s_3 \\in S.\\ s_1 R s_2 \\land s_2 R s_3 \\rightarrow s_1 Rs_3$  Congruence Relation Given a set $S$ equipped with functions $F = {f_1, \u0026hellip;, f_n}$, a relation $R$ over $S$ is a congruence relation if $R$ is an equivalence relation and for every $n$\u0026lsquo;ary function $f \\in F$ we have:\n\\[\\forall \\overset{\\rightarrow}{s}, \\overset{\\rightarrow}{t}.\\ \\bigwedge\\limits_{i=1}^{n}s_i R t_i \\rightarrow f(\\overset{\\rightarrow}{s}) R f(\\overset{\\rightarrow}{t})\\]\nA counter example would be given $R(x, y)$ defined as $|x| = |y|$ on all integers. If we have $R = {2, 2}$ and $f(x) = x + 1$ (successor function), then we know it violates the equivalence relation we mentioned above\nEquivalence Closure In short, the equivalence closure $R^E$ is the smallest equivalence relation that includes $R$. This is illustrated through an example. Given a set $S = {a, b, c}$ and binary relation $R:{\\langle a, b \\rangle , \\langle b, c \\rangle, \\langle d, d \\rangle}$, $R^E$ would contain all elements extended from $R$ based on the three properties of equivalence relation.\nCongruence Closure Naturally, congruence closure $R^C$ would be the smallest set that contains congruence relation $R$. What this means is $R^C$ contains $R^E$ (the equivlance closure we derived before), and any element generated from $R^E$ by a given function that produces element which also satisfies equivelance relation. For example, Given $S = {a, b, c}$ and function $f$ such that $f(a) = b$, $f(b) = c$, $f(c) = c$, the congruence closure would contain nine elments in total. First, we would use the procedure above to generated equivalence closure. Then, because $f(a) = b$ and $f(b) = c$ due to congruence relation, we know $b = c$, now we apply the procure for generating equivalence closure again.\nAlgorithm to Compute Congruence Closure The high-level description of the algorithm is as following:\nTo decide satisfiability of $T_{=}$ (equality theory) formula:\n\\[F\\ : \\ s_1 = t_1 \\land \u0026hellip; s_m = t_m \\land s_{m+1} \\neq t_{m+1} \\land \u0026hellip; s_n \\neq t_n\\]\n Compute subterms and construct initial DAG (each nodeâ€™s representative is itself) For each $i \\in [1,m]$, process equality $s_i= t_i$ as described. (Essentially, process all equiv expression first) For each $i \\in [m + 1,n]$, check if $Rep(s_i) =Rep(t_i)$. (Check if any nequiv expression contradicts any equiv expression) If there exists some $i \\in [m + 1, n]$, for which $Rep(s_i) =Rep(t_i)$, return UNSAT if for all $i$, $Rep(s_i) \\neq Rep(t_i)$, return SAT  This is an example for illustration purpose borrowed from Prof. Dillig\u0026rsquo;s slides:\nGiven formula $F\\ : \\ f^3(a) = a \\land f^5(a) = a \\land f(a) \\neq a$\nThe initial DAG would be:\nProcess equality $f^3(a) = a$ gives us:\nRecursively merging the parents results in:\nProcess equality $f^5(a) = a$ gives us:\nNow in this step, $f^2(a)$ and $a$ are in the same congruence class, thus we will perform the same operation on their parents, processing equality $f^3(a) = f(a)$:\nWe find $f(a) \\neq a$ has a conflict because node $a$\u0026rsquo;s representative is $f(a)$, indicating they are in the same congruence class, meeting congruence relation. Thus the formula is UNSAT.\n","permalink":"https://www.bodunhu.com/blog/posts/congruence_closure/","summary":"This is a summary of how to compute congruence closure. I implemented the algorithm to compute congruence closure and thought I\u0026rsquo;d never forget it. But my memory starts to get blurry just after two days. So I figured I\u0026rsquo;d put things down so I don\u0026rsquo;t have to watch the entire lecture again the next time I need it.\n Equivalence Relation Equivalence relation has three properties: reflexive, symmetric, and transitive. (E.","title":"Congruence Closure"},{"content":"The goal here is to familiarize yourself with how programs are loaded, dynamically paged, and some of the mechanics of signal handling and memory mapping in Linux.\n execve Syscall The operating system, as one of itsd basic services, loads programs into memory for them to execute. Programs rely on execve syscall to get the OS to load the program into memory and start it executing as a process. The kernel version we used to testing is 5.4.0. Doing a quick search inside Elixir gives us:\nSYSCALL_DEFINE3(execve, const char __user *, filename, const char __user *const __user *, argv, const char __user *const __user *, envp) { return do_execve(getname(filename), argv, envp); } Follow the function call, we will eventually reach the call to __do_execve_file, the comment of this function says \u0026ldquo;sys_execve() executes a new program\u0026rdquo;, which is pretty straighforward. This function first checks the filename pointer. Then it checks the flags of the current process that limit of running processes is not exceeded:\nif (IS_ERR(filename)) return PTR_ERR(filename); /* * We move the actual failure in case of RLIMIT_NPROC excess from * set*uid() to execve() because too many poorly written programs * don\u0026#39;t check setuid() return code. Here we additionally recheck * whether NPROC limit is still exceeded. */ if ((current-\u0026gt;flags \u0026amp; PF_NPROC_EXCEEDED) \u0026amp;\u0026amp; atomic_read(\u0026amp;current_user()-\u0026gt;processes) \u0026gt; rlimit(RLIMIT_NPROC)) { retval = -EAGAIN; goto out_ret; } /* We\u0026#39;re below the limit (still or again), so we don\u0026#39;t want to make * further execve() calls fail. */ current-\u0026gt;flags \u0026amp;= ~PF_NPROC_EXCEEDED; The next important task is to allocate the struct linux_binprm structure defined here. This structure is used to hold the arguments that are used when loading binaries.\nbprm = kzalloc(sizeof(*bprm), GFP_KERNEL); if (!bprm) goto out_files; Next, the function performs a seireis of tasks to prepare the bprm struct. Refer to the linux-insides book to find more information on how exactly the bprm structure is filled up.\nThe most important function called by __do_execve_file is search_binary_handler. Based on the comment, this function cycles the list of binary formats handler, until one recognizes the image. We can find one section of the code surrounded by binfmt_lock:\nlist_for_each_entry(fmt, \u0026amp;formats, lh) { if (!try_module_get(fmt-\u0026gt;module)) continue; read_unlock(\u0026amp;binfmt_lock); bprm-\u0026gt;recursion_depth++; retval = fmt-\u0026gt;load_binary(bprm); bprm-\u0026gt;recursion_depth--; read_lock(\u0026amp;binfmt_lock); put_binfmt(fmt); if (retval \u0026lt; 0 \u0026amp;\u0026amp; !bprm-\u0026gt;mm) { /* we got to flush_old_exec() and failed after it */ read_unlock(\u0026amp;binfmt_lock); force_sigsegv(SIGSEGV); return retval; } if (retval != -ENOEXEC || !bprm-\u0026gt;file) { read_unlock(\u0026amp;binfmt_lock); return retval; } } We can see it calls into load_binary:\nretval = fmt-\u0026gt;load_binary(bprm); Here, the load_binary is a pointer in a linux_binfmt struct. For elf format, it can be found here:\nstatic struct linux_binfmt elf_format = { .module\t= THIS_MODULE, .load_binary\t= load_elf_binary, .load_shlib\t= load_elf_library, .core_dump\t= elf_core_dump, .min_coredump\t= ELF_EXEC_PAGESIZE, }; We can find the load_elf_binary function defined in the fs/binfmt_elf.c file. Then the function will check the magic number in the ELF file header. You can find the ELF format from wiki. We can see for both 32-bit and 64-bit systems, the e-ident field should contain the magic number for ELF format files.\n/* Get the exec-header */ loc-\u0026gt;elf_ex = *((struct elfhdr *)bprm-\u0026gt;buf); retval = -ENOEXEC; /* First of all, some simple consistency checks */ if (memcmp(loc-\u0026gt;elf_ex.e_ident, ELFMAG, SELFMAG) != 0) goto out; Then, load_elf_binary will do some tasks to prepare for the executable file. After that, it will try to load the program header table:\nelf_phdata = load_elf_phdrs(\u0026amp;loc-\u0026gt;elf_ex, bprm-\u0026gt;file); if (!elf_phdata) goto out; Then it will traverse the program header table and find the interpreter which is responsible of setting up the stack and map elf binary into the correct location in memory. After the interpreter is obtained, the function will perform simple consistency checks on the interpreter. It will load the interpreter program headers:\n/* Load the interpreter program headers */ interp_elf_phdata = load_elf_phdrs(\u0026amp;loc-\u0026gt;interp_elf_ex, interpreter); if (!interp_elf_phdata) goto out_free_dentry; This function will call setup_arg_pages to finalize the stack vm_area_struct:\n/* Do this so that we can load the interpreter, if need be. We will change some of these later */ retval = setup_arg_pages(bprm, randomize_stack_top(STACK_TOP), executable_stack); if (retval \u0026lt; 0) goto out_free_dentry; It will also mmap the elf image into the correct location in memory. The bss and brk sections are prepared for the executable file:\n/* Now we do a little grungy work by mmapping the ELF image into the correct location in memory. */ for(i = 0, elf_ppnt = elf_phdata; i \u0026lt; loc-\u0026gt;elf_ex.e_phnum; i++, elf_ppnt++) { ... /* There was a PT_LOAD segment with p_memsz \u0026gt; p_filesz before this one. Map anonymous pages, if needed, and clear the area. */ retval = set_brk(elf_bss + load_bias, elf_brk + load_bias, bss_prot); if (retval) goto out_free_dentry; nbyte = ELF_PAGEOFFSET(elf_bss); if (nbyte) { nbyte = ELF_MIN_ALIGN - nbyte; if (nbyte \u0026gt; elf_brk - elf_bss) nbyte = elf_brk - elf_bss; if (clear_user((void __user *)elf_bss + load_bias, nbyte)) { } It will also call elf_map to map the segment to [vaddr, vaddr + file size] and align and then perform some checks:\nerror = elf_map(bprm-\u0026gt;file, load_bias + vaddr, elf_ppnt, elf_prot, elf_flags, total_size); The interpreter is then loaded:\nelf_entry = load_elf_interp(\u0026amp;loc-\u0026gt;interp_elf_ex, interpreter, \u0026amp;interp_map_addr, load_bias, interp_elf_phdata); Finally, the elf talbe is created:\nretval = create_elf_tables(bprm, \u0026amp;loc-\u0026gt;elf_ex, load_addr, interp_load_addr); After everything is prepared, we can call the start_thread function, which prepares the new task\u0026rsquo;s registers and segments for execution. We will pass the set of registers for the new task, the address of the entry point of the new task, and the address of the top of of the statck for the new task to this function.\nstart_thread(regs, elf_entry, bprm-\u0026gt;p); A lot of the information here can also be found at the linux-insides book. I found it very helpful clearing my confusion.\nIn our own implementations, we will not call the loaded program\u0026rsquo;s main function. Instead, our loader will transfer control to the entry point of the loaded program via the jmp instruction. It has two major differences:\n Jumping to the entry point indicates we are going to execute the glibc start up functions before main is called. This includes setting up thread local storge. main simply jump to the main with the loader\u0026rsquo;s TLS, no other setups are involved. jmp doesn\u0026rsquo;t push return address on stack. When the loaded prgoram finishes execution, it exits the loader program, instead of giving cntrol back to the caller.  ","permalink":"https://www.bodunhu.com/blog/posts/programloadingandmemorymappinginlinux/","summary":"The goal here is to familiarize yourself with how programs are loaded, dynamically paged, and some of the mechanics of signal handling and memory mapping in Linux.\n execve Syscall The operating system, as one of itsd basic services, loads programs into memory for them to execute. Programs rely on execve syscall to get the OS to load the program into memory and start it executing as a process. The kernel version we used to testing is 5.","title":"Program Loading and Memory Mapping in Linux"},{"content":"What is a thread? A thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler.\nKernel Level Threads Pros/Cons  Good functionality, system wide integration Threads are seen and scheduled only by the kernel. A lot of kernel information should be invisible to user thread and can be useful for scheduling Poor performance, every thread_related call traps. This situation is a lot worse in the 1990s than it is now mainly due to clock speed. The scheduling quanta are roughly the same, but because the clock speeds are much faster today, you can execute orders of magnitude more instructions per quanta today than you could in 1990. Even if traps, let\u0026rsquo;s say, costs 10 cycles to complete, it would be a much bigger fraction of the quanta in 1990 than it is today.  User Level Threads Pros/Cons  Good performances. (most threads operations don\u0026rsquo;t involve kernel) Good scheduling policy flexibility: done by thread lib Poor system-wide integration Multi-programmed workloads are hard to schedule I/O, page faults invisible Potetntial for incorrect behavior  User level scheduler may not be cooperative. With user threads running on kernel threads, it may be that kernel threads block when a user-thread blocks, thus an application can run out of kernel threads to run their user threads.May be gilding the lily.    Some Problems about User-Level Threads on Kernel Interface  Insufficient visibility between the kernel and user thread lib Kernel event such as pr-emption or I/O are not visible to user lib  For example, if user level threads block, then the kernel thread serving it also blocks.   Kernel threads are scheduled with respect to user-level thread library, we can have this interferences between two schedulers. Kernel time-slicing of threads  For example, user level threads holding a spin-lock can be pre-empted, which can potentially cause all other user threads to wait.    Scheduler Activation The basic principle about scheduler actication is to expose revocation: telling me when you take something away. This is basically the same idea as the exokernel. For example, interfaces like\n add_processor() has_blocked()  The basics about scheudler activation are\n Multi-threaded programs are still given an address space Facilitate flow of kernel information between user and kernel threads Kernel explicily vectors kernel events to the user-level thread  via scheduler activation (upcall)   Extended kernel interface for processor allcoation-related events  Essentially exchanging information    Scheduler Activation vs Kernel Threads Key differences:\n Pre-empted threads never resumed by the kernel direcly.  Essentially, every new SA is a brand new context. For example, if you do blocking I/O, the kernel will provide a new scheduling activagtion and vector into that application space. There isn\u0026rsquo;t a notion of \u0026ldquo;resume\u0026rdquo;. The kernel is simply going to find some new schedule activation to notify you that a work has unblocked. In modern kernels, you would do somehting like stack unwinding to get back into user space.    An important problem is what happened if a user thread is forced to be de-scheduler while it\u0026rsquo;s in a scheduler. The user thread will hold a a lock on user level run queue. That means no other user thread can be scheduled to run because none of them can acquire the lock. Because there\u0026rsquo;s no notion of \u0026ldquo;resume\u0026rdquo; in scheduling activation, we can\u0026rsquo;t really resume the execution in the scheduler. Thus, we run into a deadlock situation.\nOne solution is to detect whether we are using a lock and keep executing until we leave the locked region. Of course, there are too many gotchas in this solution.\nAnother solution is that the kernel can make a copy of the critical section and execute the critical section itself regardless of what the user thread chooses to do. Therefore, we can guarantee by the time you vector back into user space the lock is no longer held. So the kernel is basically executing the user code! Crazy, right? Now we ran into more gotchas. What if the code is written in Java? How to find a locked region in userspace? What if \u0026hellip;\nAnother thing we want to mention is page fault. Page fault indicates that you are missing part of your address. So there will be a notification with a new scheduler activation. Once you do soemthing with it, you will liekly touch that same piece in the space and double fault again.\nWhat is the solution?\n","permalink":"https://www.bodunhu.com/blog/posts/scheduleractivation/","summary":"What is a thread? A thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler.\nKernel Level Threads Pros/Cons  Good functionality, system wide integration Threads are seen and scheduled only by the kernel. A lot of kernel information should be invisible to user thread and can be useful for scheduling Poor performance, every thread_related call traps. This situation is a lot worse in the 1990s than it is now mainly due to clock speed.","title":"Scheduler Activation"},{"content":"I was using Mathjax v2 for a while and I heard v3 perform significantly better than v2. Many great tutorials explains explains how to add Mathjax support to Jekyll websites. Some of them only cover Mathjax v2. So here is the brief summary on how to add Mathjax v3 support to your Jekyll website (Recently I\u0026rsquo;ve migrated to Hugo but adding support to Hugo is also pretty similar).\n  In the _config.yml located in your root directory, add this line:  markdown: kramdown  Create a file called mathjax.html insides _includes/, add these lines (these settings come from the Mathjax documentaion.):  \u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [ [\u0026#39;$\u0026#39;, \u0026#39;$\u0026#39;], [\u0026#39;\\\\(\u0026#39;, \u0026#39;\\\\)\u0026#39;] ] } ,svg: { fontCache: \u0026#39;global\u0026#39; } }; \u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; id=\u0026#34;MathJax-script\u0026#34; async src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js\u0026#34;\u0026gt; \u0026lt;/script\u0026gt; For Hugo website, the script will be exactly the same. The only difference is instead of putting mathjax.html into _includes/, you would want to put it inside layouts/partials. For example, I put my mathjax.html into the theme directory themes/ezhil/layouts/partials.\n Add this line in your _includes/head.html before \u0026lt;/head\u0026gt;:  {% include mathjax.html \u0026gt;}} For Hugo, we would add\n{{ partial \u0026#34;mathjax.html\u0026#34; . }} to layouts/partials/header.html before \u0026lt;/head\u0026gt;. For example, I added {{ partial \u0026quot;mathjax.html\u0026quot; . }} to my themes/ezhil/layouts/partials/header.html.\n Now you can write in-line math equations in your markdown file like:  \\\\(f(x) = x^2\\\\) or\n$f(x) = x^2$ It will be render to: \\(f(x) = x^2\\)\nIf you are already using Mathjax v2 and wish to just convert it to v3, you may also try this configuration converter. The most useful resource is the official Mathjax documentation.\n","permalink":"https://www.bodunhu.com/blog/posts/mathjaxjekyll/","summary":"I was using Mathjax v2 for a while and I heard v3 perform significantly better than v2. Many great tutorials explains explains how to add Mathjax support to Jekyll websites. Some of them only cover Mathjax v2. So here is the brief summary on how to add Mathjax v3 support to your Jekyll website (Recently I\u0026rsquo;ve migrated to Hugo but adding support to Hugo is also pretty similar).\n  In the _config.","title":"Add MathJax v3 Support to Jekyll and Hugo"},{"content":"This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to cpu world. This is the system spec:\n   Attribute Value     Processor name (BIOS) Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz   Cores 6   Logical processors 12   TLB/Cache details 64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries. Plus, 1-GB pages, 4-way, 16 entries   RAM 32GB   Operating System Ubuntu 20.04.1 LTS   Kernel Version 5.4.0-47-generic    \n 8-way set associative means the CPU cache is made up of sets that can fit 8 blocks each.\n  Here are the details for the CPU cache, which we will need later:\n   Cache L1 data L1 instruction L2 L3     Size 6 x 32 KB 6 x 32 KB 6 x 256 KB 15 MB   Associativity 8-way set associative 8-way set associative 8-way set associative 20-way set associative   Line size: 64 bytes 64 bytes 64 bytes 64 bytes   Comments: Direct-mapped Direct-mapped Non-inclusive Direct-mapped Inclusive Shared between all cores    \nMemory Map To print the /proc/self/maps file for a process, we use the sprintf to construct the file name and then use the system from stdlib to cat the contents of the running process\u0026rsquo;s address space. If we execute the program, it shows (also available on gist)\naddress perms offset dev inode pathname 559e3e51f000-559e3e520000 r--p 00000000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 559e3e520000-559e3e521000 r-xp 00001000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 559e3e521000-559e3e522000 r--p 00002000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 559e3e522000-559e3e523000 r--p 00002000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 559e3e523000-559e3e524000 rw-p 00003000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 7faf5c477000-7faf5c49c000 r--p 00000000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c49c000-7faf5c614000 r-xp 00025000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c614000-7faf5c65e000 r--p 0019d000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c65e000-7faf5c65f000 ---p 001e7000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c65f000-7faf5c662000 r--p 001e7000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c662000-7faf5c665000 rw-p 001ea000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c665000-7faf5c66b000 rw-p 00000000 00:00 0 7faf5c685000-7faf5c686000 r--p 00000000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c686000-7faf5c6a9000 r-xp 00001000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c6a9000-7faf5c6b1000 r--p 00024000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c6b2000-7faf5c6b3000 r--p 0002c000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c6b3000-7faf5c6b4000 rw-p 0002d000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c6b4000-7faf5c6b5000 rw-p 00000000 00:00 0 7ffcddb8d000-7ffcddbae000 rw-p 00000000 00:00 0 [stack] 7ffcddbe0000-7ffcddbe3000 r--p 00000000 00:00 0 [vvar] 7ffcddbe3000-7ffcddbe4000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] Based on the linux man page, we can see each column has different definition. The address field is the address space in the process that the mapping occupies. The perms field is a set of permissions:\n r = read w = write x = execute s = shared p = private (copy on write)  The offset field is the offset into the file/whatever; dev is the device (major:minor); inode is the inode on that device. 0 indicates that no inode is associated with the memory region, as would be the case with BSS (uninitialized data).\nThe pathname field will usually be the file that is backing the mapping. For ELF files, you can easily coordinate with the offset field by looking at the Offset field in the ELF program headers (readelf -l). In addition, we can see a few other pseudo-paths:\n  [stack]: the initial process\u0026rsquo;s (also known as the main thread\u0026rsquo;s) stack.\n  [vdso]: The virtual dynamically linked shared object. More detailed descriptions can be found on lwn.\n  [vvar]: location of kernel space variables mapped in user space needed by virtual system calls. Essentially, a kernel-space physical address is mapped into the userspace.\n  [vsyscall]: similar to vDSO, vsyscall is another segment used to accelerate certain system calls in Linux. Vsyscall has some limitations; among other things, there is only space for a handful of virtual system calls. More detailed descriptions can be found on lwn.\n  One thing interesting here is that when we execute the same program twice, we can see after the first run, the output is\n7fffbc92f000-7fffbc930000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] Type the same command again:\n7ffd6a94d000-7ffd6a94e000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] Note that the vDSO area has moved, while the vsyscall page remains at the same location. The location of the vsyscall page is nailed down in the kernel ABI, but the vDSO area - like most other areas in the user-space memory layout - has its location randomized every time it is mapped. The vsyscall is legacy implementation of user-space sys call acceleration. Since it has fixed addresses, it is vulnerable to security issues. Because applications depend on the existence and exact address of that page, most functions are simply removed and replaced by a special trap instruction. More detailed explanation can be found on lwn.net.\nAnother interesting thing we observed is the base address of the executable (the start of the text section) and the start address of libc is rather different. This is also the result of using ASLR which is used to prevent return-to-libc attack.\ngetrusage Then, we call getrusage at the end of our program and print out the fields. We will need getrusage later. Here is a sample output for some fields inside struct rusage:\nutime: 1306 stime: 0 maxrss: 2692 minflt: 76 majflt: 0 inblock: 0 oublock: 0 nvcsw: 2 nivcsw: 0 Here is a short list of descriptions for each of these fields. More detailed information can be found on gnu website\n utime: time spent executing user instructions. stime: time spent in operating system code on behalf of processes. maxrss: the maximum resident set size used, in kilobytes. That is, the maximum number of kilobytes of physical memory that processes used simultaneously. minflt: the number of page faults which were serviced without requiring any I/O. majflt: the number of page faults which were serviced by doing I/O. inblock: the number of times the file system had to read from the disk on behalf of processes. oublock: the number of times the file system had to write to the disk on behalf of processes. nvcsw: the number of times processes voluntarily invoked a context switch (usually to wait for some service). nivcsw: the number of times an involuntary context switch took place (because a time slice expired, or another process of higher priority was scheduled).  perf_event_open perf_event_open interface is useful to measurement numerous system events. However, glibc doesn\u0026rsquo;t provide wrapper for this system call. Instead, we need to use syscall directly.\nTo use perf_event_open, we call create a function wrapper that does the actual syscall for us. Take the example from the Linux man page\nstatic int perf_event_open(struct perf_event_attr *hw_event, pid_t pid, int cpu, int group_fd, unsigned long flags) { int ret; ret = syscall(__NR_perf_event_open, hw_event, pid, cpu, group_fd, flags); return ret; } Here the __NR_perf_event_open specifies the syscall number. On our local machine, we can go to /usr/include/x86_64-linux-gnu/sys/syscall.h, which specifies the location of __NR_perf_event_open. In our case, it is located at /usr/include/x86_64-linux-gnu/asm/unistd_64.h.\nIf we call objdump -d on the binary file, we will see something like this\n000000000000119a \u0026lt;perf_event_open\u0026gt;: 119a:\t55 push %rbp 119b:\t48 89 e5 mov %rsp,%rbp 119e:\t48 83 ec 30 sub $0x30,%rsp 11a2:\t48 89 7d e8 mov %rdi,-0x18(%rbp) 11a6:\t89 75 e4 mov %esi,-0x1c(%rbp) 11a9:\t89 55 e0 mov %edx,-0x20(%rbp) 11ac:\t89 4d dc mov %ecx,-0x24(%rbp) 11af:\t4c 89 45 d0 mov %r8,-0x30(%rbp) 11b3:\t48 8b 7d d0 mov -0x30(%rbp),%rdi 11b7:\t8b 75 dc mov -0x24(%rbp),%esi 11ba:\t8b 4d e0 mov -0x20(%rbp),%ecx 11bd:\t8b 55 e4 mov -0x1c(%rbp),%edx 11c0:\t48 8b 45 e8 mov -0x18(%rbp),%rax 11c4:\t49 89 f9 mov %rdi,%r9 11c7:\t41 89 f0 mov %esi,%r8d 11ca:\t48 89 c6 mov %rax,%rsi 11cd:\tbf 2a 01 00 00 mov $0x12a,%edi 11d2:\tb8 00 00 00 00 mov $0x0,%eax 11d7:\te8 84 fe ff ff callq 1060 \u0026lt;syscall@plt\u0026gt; 11dc:\t89 45 fc mov %eax,-0x4(%rbp) 11df:\t8b 45 fc mov -0x4(%rbp),%eax 11e2:\t48 98 cltq 11e4:\tc9 leaveq 11e5:\tc3 retq We notice there\u0026rsquo;s one interesting line\ncallq 1060 \u0026lt;syscall@plt\u0026gt; The plt stands for Procedure Linkage Table. This lines indicates a call to the syscall in the procedure linking table. The PLT allows us to resolve the absolute addresses of shared libraries at runtime.\nTake a look at the \u0026lt;syscall@plt\u0026gt; section of the disassembly of section .plt, we see\n0000000000001060 \u0026lt;syscall@plt\u0026gt;: 1060:\tff 25 62 2f 00 00 jmpq *0x2f62(%rip) #3fc8\u0026lt;syscall@GLIBC_2.2.5\u0026gt; 1066:\t68 03 00 00 00 pushq $0x3 106b:\te9 b0 ff ff ff jmpq 1020 \u0026lt;.plt\u0026gt; Notice this jump is a pointer to an address. The address lies inside the GOT (Global Offset Table). The GOT will eventually hold the absolute address call to syscall. On the first call the address will point back to the instruction after the jump in the PLT - 0x1066. Then we see another jump instruction. This jump is a jump into the eventual runtime linker code that will load the shared library which has syscall.\nWe also see the comment for the first jump instruction\n#3fc8\u0026lt;syscall@GLIBC_2.2.5\u0026gt; Use objdump -R, we see the dynamic relocation entries in the file\nDYNAMIC RELOCATION RECORDS OFFSET TYPE VALUE 0000000000003d98 R_X86_64_RELATIVE *ABS*+0x0000000000001190 0000000000003da0 R_X86_64_RELATIVE *ABS*+0x0000000000001150 0000000000004008 R_X86_64_RELATIVE *ABS*+0x0000000000004008 0000000000003fd8 R_X86_64_GLOB_DAT _ITM_deregisterTMCloneTable 0000000000003fe0 R_X86_64_GLOB_DAT __libc_start_main@GLIBC_2.2.5 0000000000003fe8 R_X86_64_GLOB_DAT __gmon_start__ 0000000000003ff0 R_X86_64_GLOB_DAT _ITM_registerTMCloneTable 0000000000003ff8 R_X86_64_GLOB_DAT __cxa_finalize@GLIBC_2.2.5 0000000000003fb0 R_X86_64_JUMP_SLOT getpid@GLIBC_2.2.5 0000000000003fb8 R_X86_64_JUMP_SLOT __stack_chk_fail@GLIBC_2.4 0000000000003fc0 R_X86_64_JUMP_SLOT system@GLIBC_2.2.5 0000000000003fc8 R_X86_64_JUMP_SLOT syscall@GLIBC_2.2.5 0000000000003fd0 R_X86_64_JUMP_SLOT sprintf@GLIBC_2.2.5 : 1020:\tff 35 7a 2f 00 00 pushq 0x2f7a(%rip) # 3fa0  1026:\tff 25 7c 2f 00 00 jmpq *0x2f7c(%rip) # 3fa8  102c:\t0f 1f 40 00 nopl 0x0(%rax) ``` -- Monitor Events Next, we are going to look at L1 data cache metrics. We are interested in L1 data cache accesses, misses, and data TLB misses. We will measure this code in our experiment. CACHE_LINE_SIZE is defined as 64 to match our CPU specs.\n// p points to a region that is 1GB (ideally) void do_mem_access(char* p, int size) { int i, j, count, outer, locality; int ws_base = 0; int max_base = ((size / CACHE_LINE_SIZE) - 512); for(outer = 0; outer \u0026lt; (1\u0026lt;\u0026lt;20); ++outer) { long r = simplerand() % max_base; // Pick a starting offset  if( opt_random_access ) { ws_base = r; } else { ws_base += 512; if( ws_base \u0026gt;= max_base ) { ws_base = 0; } } for(locality = 0; locality \u0026lt; 16; locality++) { volatile char *a; char c; for(i = 0; i \u0026lt; 512; i++) { // Working set of 512 cache lines, 32KB  a = p + (ws_base + i) * CACHE_LINE_SIZE; if((i%8) == 0) { *a = 1; } else { c = *a; } } } } } What this routine does is essentially pick a working set of 512 cache lines, periodically perform a write or otherwise read operation. This process is repeated 16 times during each interaction. Each read or write access will operate on a new cache line. The innermost loop will perform this set of operations for the entire L1 data cache.\nWhen opt_random_access is true, the starting base address of the cache line is randomly picked. Otherwise, it is incremented by 512 cache lines (or one working set) during each outer iteration. The main difference is that with opt_random_access set to true, the starting base address of the cache line can\u0026rsquo;t be precomputed by the hardware, thus likely increase miss rate.\nTo measure L1 data cache metrics, we will use the perf_event_open interface we discussed above. To measure L1 data cache read misses, we will configure our struct perf_event_attr as follows:\n#define CALC_CONFIG(perf_hw_cache_id, perf_hw_cache_op_id, perf_hw_cache_op_result_id) \\ ((perf_hw_cache_id) | (perf_hw_cache_op_id \u0026lt;\u0026lt; 8) | (perf_hw_cache_op_result_id \u0026lt;\u0026lt; 16))  hw_event.type = PERF_TYPE_HW_CACHE; hw_event.size = sizeof(struct perf_event_attr); hw_event.disabled = 1; // disable at init time hw_event.exclude_kernel = 1; hw_event.config = CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS); The exact details can be found in linux man page. The important part is:\nhw_event.config = CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS); These configurations allows us to measure the L1 data cahe read misses. The arguments passed to perf_event_open is\npid_t pid = 0; int cpu = -1; int group_fd = -1; unsigned long flags = 0; The choice of these parameters can also be found on the linux man page. After perf_event_open is called, we will re-enable event measurements by calling\nioctl(fd, PERF_EVENT_IOC_RESET, 0); ioctl(fd, PERF_EVENT_IOC_ENABLE, 0); What it does is resetting the event count specified by the file descriptor argument to zero, then enables the individual event specified by the file descriptor argument. After do_mem_access(p, size) is executed, we call ioctl(fd, PERF_EVENT_IOC_DISABLE, 0) to disable the event and then read the result by read(fd, \u0026amp;result, sizeof(long long)). How result is defined is up to how PERF_FORMAT_* was specified. You can also check lxr to see how __perf_event_read_size calculates the size of event that is read. In our case, it\u0026rsquo;s simple a u64.\n Be aware that simply executing the binary might cause perf_event_open to fail (in which case will always return -1). Using sudo is one workaround. Execute cat /proc/sys/kernel/perf_event_paranoid and see what returns. -1 means you have raw access to kernel tracepoints. Otherwise, you might have trouble accessing the performance counter without root privilege. Check this stackexchange post for more details.\n To be even more careful about generating repeatable results we should flush the level 1 data cache before enabling the performance counters. We will do this by reading a memory buffer larger than per-core L1 data cache size\nsize_t buffer_size = 32 * 1024 + 1024; char *buff = malloc(buffer_size); for (int i = 0; i \u0026lt; buffer_size; i++) { buff[i] = rand(); } We will also lock the process onto a single processor by using the sched_setaffinity function. Our example is\ncpu_set_t set; CPU_ZERO(\u0026amp;set); CPU_SET(7, \u0026amp;set); int aff = sched_setaffinity(0, sizeof(cpu_set_t), \u0026amp;set); We perform the each of the above experiments 5 times. First, we turn on random cache line base address generation. On average, we have around 1010665367 L1 data cache read misses wtih standard deviation to be 61010967 misses. When random access is disabled, we have on average 964420324 read misses with standard deviation of 65787193 misses. We can also measure the number L1 data cache write misses by using the PERF_COUNT_HW_CACHE_OP_WRITE config instead. Use PERF_COUNT_HW_CACHE_OP_PREFETCH gives us prefetch misses, in our case, both of these metrics are unavailable. We can check the /arch/x86/events/intel/core.c in lxr and we can see these metrics are not available.\nWe can also use the PERF_COUNT_HW_CACHE_DTLB config option for data TLB measurement. For read access we have on average 3390719 misses with std dev being 17579, while write access has 1486451 misses with std dev being 13455. The prefetch metrics for TLB are unavailable in our case. To find out more about available metrics supported, please check the constant static __initconst const u64 skl_hw_cache_event_ids for specific kernel version.\nWith random cache line access turned off, we have 517335 read misses data TLB with standard deviation of 3820 misses. For write we have on average 809671 misses with standard deviation being 9580 misses. It is a significant reduction compared to the random access implementation.\nTo calculate the L1 cache miss rate and data TLB miss rate, we can use 100.0 * cache misses / cache_accesses and 100.0 * tlb misses / cache_accesses to calculate the results. With random access turned off, we get L1 read access miss rate to be $$miss_{cache} = 1.5%$$ and TLB read miss rate $$miss_{tlb} \\approx 0$$. When random access is turned on, we have $$miss_{cache} = 1.4%$$ and $$miss_{tlb} \\approx 0$$. We can see the miss rate in all scenarios is really low. This is mainly because the inner most loop in our routine is performing operations on working set already presented in L1 cache and TLB. The read/write operations use continous cache lines, which means there will almost be no faults while we access the 512 cache lines. If one fault causes the entire new working set to be cached, then there would be no subsequent faults until the entire working set is iterated.\nIf we use getrusage we can see the metrics listed below:\n   Metrics Mean std dev     utime 868629 126044   stime 253586 20112   maxrss 1049691 43   minflt 262214 1   majflt 0 0   inblock 0 0   oublock 0 0   nvcsw 0.4 0.54   nivcsw 47 7    \nmmap Next we are going to explore the behavior of mmap. Previously, we used malloc for data allocation. Next, we are going to instead use mmap and see what happens. Here we will only use read access for benchmark metrics since it\u0026rsquo;s available in both L1 and TLB metrics.\nFirst, we use the MAP_ANONYMOUS as a flag passed to mmap. This flag means the mapping is not backed by any file; its contents are initialized to zero. The complete call is\nmmap(NULL, length, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, fd_ignore, offset); For more details, refer to mmap man page for information.\nWhen we turn on the random access and use perf_event_open interface to collect metrics, we see the L1 data cache read misses are 956148031 (std dev 84631843). The TLB data cache read misses are 3370309 (std dev 17792). We see it is not really different to the malloc approach we used before. Doing a simple strace shows malloc calls mmap. The memory that backs malloc() allocations is handled by the kernel in much the same way as the memory that backs private anonymous mappings created with mmap().\nThen, we try to use mmap() to create mapping in the virtual address space backed by a file instead of using MAP_ANONYMOUS.\nWe first test mmap with MAP_PRIVATE. According to the man page, this flags means creating a private copy-on-write mapping. Updates to the mapping are not visible to other processes mapping the same file, and are not carried through to the underlying file. It is unspecified whether changes made to the file after the mmap() call are visible in the mapped region.\n Note we should call fallocate() for the newly created file, otherwise mmap is gonna throw bur error.\n When we measure the L1 data cache miss, it\u0026rsquo;s around 946128512 (std dev 956148031), nothing special happens. When we use MAP_SHARED flag, the result was similar. The result seems to fluctuates as time passes, but overall they are not much different. After all, it\u0026rsquo;s just reading from the memory, whether the address is backed by a file or not doesn\u0026rsquo;t play a big role in affecting the cache miss rate. The L1 data cache misses is shown below:\n   Flag PRIVATE PRIVATE+POPULATE SHARED SHARED+POPULATE     Mean 783864673 769314361 842915231 816749524   Std dev 77816766 53913082 54613278 60580595    \nIf we take a look at TLB data cache, the result is\n   Flag PRIVATE PRIVATE+POPULATE SHARED SHARED+POPULATE     Mean 3372303 3370740 3381755 3377370   Std dev 9884 13567 17626 11776    \nStill, there doesn\u0026rsquo;t seem have any significant fluctuation in the number of misses in data TLB. This pattern also applies to sequential access, except the TLB data cache misses is alot lower in sequentual access.\nNow If we instead use getrusage(), we will get something like this\n   Flag PRIVATE PRIVATE+POPULATE SHARED SHARED+POPULATE     Usec/std dev 20/0 20/0 20/0 20/0   usec/std dev 801512/ 78346 793452/ 143556 872342/ 124124 671957/ 229314   Ssec/std dev 0/0 0/0 0/0 0/0   ssec/std dev 475977/ 54355 475678/ 134253 445467/ 99345 536041/ 98797   oublock/std dev 0/0 0/0 2997152/ 82256 2097152/ 19760    \nThe most interesting part here is when MAP_SHARED is enabled, the oublock immediately changes. As we mentioned previously, oublock specifies the number of times the file system had to write to the disk on behalf of processes. Because the address is now backed by a file, all write operations will cause the file system to write the contents back to the file.\nmmap() creates a new mapping in the virtual address space of the calling process. However, it doesn\u0026rsquo;t allocate RAM. If we call memset() then followed by msync() with MS_SYNC flag, we can get some interesting results in getrusage, these observations are summarized here:\n kernel space time is much higher. It usually take 1 sec (no std dev) as opposed to 0. Synchronizing to files on disk will require more kernel participation. minflt (the number of page faults which were serviced without requiring any I/O) was muich higher, the value is around 540782(std dev 3). More memory mapped means the faults by I/O will be less likely. oublock is much higher, the value is around 4196512(std dev 1). The sync operating means there will be approximatly double amount of writes to disk. nvcsw was higher, there are more voluntary context switches. Writing results to disk has delay, and thus the process likely need to context switch while waiting for I/O to be finished.  We may notice the number data TLB misses is lower than the total number of page the application uses. One obvious answer the use of huge page. One huge page can cover many small pages. Also, because we have prefetching TLB and the working set access pattern is contiguous, TLB hit rate will be high. Because we have a set-associative TLB cache, and we access the memory in a fairly deterministic way, it\u0026rsquo;s easy to predict where the next access is pointing to. For example, if the replacement policy is FIFO, then each cache line will remain untouched for exact same clock cycle before replaced. This also applies to other policies. One way to determine the replacement algorithm is using P-Chase.\nstrace We then use strace to trace syscalls of our application. The output contains some interesting information, one is\naccess(\u0026quot;/etc/ld.so.preload\u0026quot;, R_OK) = -1 ENOENT (No such file or directory) ... arch_prctl(ARCH_SET_FS, 0x7fdc6ad83540) = 0 According to arch_prctl man page, arch_prctl() sets architecture-specific process or thread state. The ARCH_SET_FS option sets the 64-bit base for the FS register to addr, in our case it\u0026rsquo;s 0x7fdc6ad83540. Let\u0026rsquo;s set a break point at arch_prctl and backtrace from there\n#0 0x00007ffff7febb55 in ?? () from /lib64/ld-linux-x86-64.so.2 #1 0x00007ffff7fd104c in ?? () from /lib64/ld-linux-x86-64.so.2 #2 0x00007ffff7fd0108 in ?? () from /lib64/ld-linux-x86-64.so.2 #3 0x0000000000000001 in ?? () #4 0x00007fffffffe2fa in ?? () #5 0x0000000000000000 in ?? () We can see the FS segment base is set by the ld-linux, which is a part of glibc, during the program loading. A simple google seach tells us /lib64/ld-linux-x86-64.so.2 is a dynamic linker. A more detailed description can be found on this post and lwn.net. During the startup, the loader initalizes TLS. The includes memory allocation and setting FS base value to point to the TLS beignning, which is done via the arch_prctl syscall. More can be found here. This init_tls() is called here, which subsequently calls the actuall syscall in tls.h.\nThe /etc/ld.so.preload has similarities to LD_PRELOAD, in addition, it doesn\u0026rsquo;t suffer security limitation posed by LD_PRELOAD (explanation here). This a feature of glibc.\nCompeting for Memory Next we are going to fork another process that will compete for memory with our process under test. We will use this code snippet which is going to be executed by both the parent and the child process\nint compete_for_memory(void* unused) { long mem_size = get_mem_size(); int page_sz = sysconf(_SC_PAGE_SIZE); printf(\u0026#34;Total memsize is %3.2f GBs\\n\u0026#34;, (double)mem_size/(1024*1024*1024)); fflush(stdout); char* p = mmap(NULL, mem_size, PROT_READ | PROT_WRITE, MAP_NORESERVE|MAP_PRIVATE|MAP_ANONYMOUS, -1, (off_t) 0); if (p == MAP_FAILED) perror(\u0026#34;Failed anon MMAP competition\u0026#34;); int i = 0; while(1) { volatile char *a; long r = simplerand() % (mem_size/page_sz); char c; if( i \u0026gt;= mem_size/page_sz ) { i = 0; } // One read and write per page  //a = p + i * page_sz; // sequential access  a = p + r * page_sz; c += *a; if((i%8) == 0) { *a = 1; } i++; } return 0; } The get_mem_size() is implemented using this portable code\n#if defined(_WIN32) #include \u0026lt;Windows.h\u0026gt; #elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) \u0026amp;\u0026amp; defined(__MACH__)) #include \u0026lt;unistd.h\u0026gt;#include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/param.h\u0026gt;#if defined(BSD) #include \u0026lt;sys/sysctl.h\u0026gt;#endif  #else #error \u0026#34;Unable to define getMemorySize( ) for an unknown OS.\u0026#34; #endif  /** * Returns the size of physical memory (RAM) in bytes. */ size_t getMemorySize( ) { #if defined(_WIN32) \u0026amp;\u0026amp; (defined(__CYGWIN__) || defined(__CYGWIN32__))  /* Cygwin under Windows. ------------------------------------ */ /* New 64-bit MEMORYSTATUSEX isn\u0026#39;t available. Use old 32.bit */ MEMORYSTATUS status; status.dwLength = sizeof(status); GlobalMemoryStatus( \u0026amp;status ); return (size_t)status.dwTotalPhys; #elif defined(_WIN32)  /* Windows. ------------------------------------------------- */ /* Use new 64-bit MEMORYSTATUSEX, not old 32-bit MEMORYSTATUS */ MEMORYSTATUSEX status; status.dwLength = sizeof(status); GlobalMemoryStatusEx( \u0026amp;status ); return (size_t)status.ullTotalPhys; #elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) \u0026amp;\u0026amp; defined(__MACH__))  /* UNIX variants. ------------------------------------------- */ /* Prefer sysctl() over sysconf() except sysctl() HW_REALMEM and HW_PHYSMEM */ #if defined(CTL_HW) \u0026amp;\u0026amp; (defined(HW_MEMSIZE) || defined(HW_PHYSMEM64))  int mib[2]; mib[0] = CTL_HW; #if defined(HW_MEMSIZE)  mib[1] = HW_MEMSIZE; /* OSX. --------------------- */ #elif defined(HW_PHYSMEM64)  mib[1] = HW_PHYSMEM64; /* NetBSD, OpenBSD. --------- */ #endif  int64_t size = 0; /* 64-bit */ size_t len = sizeof( size ); if ( sysctl( mib, 2, \u0026amp;size, \u0026amp;len, NULL, 0 ) == 0 ) return (size_t)size; return 0L; /* Failed? */ #elif defined(_SC_AIX_REALMEM)  /* AIX. ----------------------------------------------------- */ return (size_t)sysconf( _SC_AIX_REALMEM ) * (size_t)1024L; #elif defined(_SC_PHYS_PAGES) \u0026amp;\u0026amp; defined(_SC_PAGESIZE)  /* FreeBSD, Linux, OpenBSD, and Solaris. -------------------- */ return (size_t)sysconf( _SC_PHYS_PAGES ) * (size_t)sysconf( _SC_PAGESIZE ); #elif defined(_SC_PHYS_PAGES) \u0026amp;\u0026amp; defined(_SC_PAGE_SIZE)  /* Legacy. -------------------------------------------------- */ return (size_t)sysconf( _SC_PHYS_PAGES ) * (size_t)sysconf( _SC_PAGE_SIZE ); #elif defined(CTL_HW) \u0026amp;\u0026amp; (defined(HW_PHYSMEM) || defined(HW_REALMEM))  /* DragonFly BSD, FreeBSD, NetBSD, OpenBSD, and OSX. -------- */ int mib[2]; mib[0] = CTL_HW; #if defined(HW_REALMEM)  mib[1] = HW_REALMEM; /* FreeBSD. ----------------- */ #elif defined(HW_PYSMEM)  mib[1] = HW_PHYSMEM; /* Others. ------------------ */ #endif  unsigned int size = 0; /* 32-bit */ size_t len = sizeof( size ); if ( sysctl( mib, 2, \u0026amp;size, \u0026amp;len, NULL, 0 ) == 0 ) return (size_t)size; return 0L; /* Failed? */ #endif /* sysctl and sysconf variants */ #else  return 0L; /* Unknown OS. */ #endif } The important line is\nreturn (size_t)sysconf( _SC_PHYS_PAGES ) * (size_t)sysconf( _SC_PAGESIZE ); One thing to notice in the routine for competing for memory is we used fflush after the printf. The purpose of fflush(stream) is to make the operating system flush any buffers to the underlying file. This mainly because stdout is buffered. The buffer is not flushed until newline. fflush will cause this process to happen with the absense of newline. stderr is unbuffered and thus fflush would not be necessary.\nFor this experiment, we tested it on a VM. The reason is because the contending process will take all RAM and completely hault the mahcine if tested on the host. To ensure our VM has enough swap space, we follow this tutorial to create 4GB of swap area (we allocated 2GB RAM for VM).\nOne thing we observe is that the execution time of the program become significantly longer to run. In our experiement we need to limit the number of iterations from 1 \u0026laquo; 20 to 1 \u0026laquo; 8 to get some sensible results without running for days.\nWhen we use PRIVATE and ANONYMUS option and random access turned on, the misses in data TLB is 335009(std dev 7298). We can\u0026rsquo;t get access to L1 cache data because it will cause the session to be automatically logged out whenever L1D is used. here are some interesting things to notice:\n MAP_PRIVATE + MAP_ANONYMOUS: TLB misses:335009(std dev 17298)\nminflt: 4220(std dev 231)\noublock: 8(std dev 4)\nnivcsw: 19(10) MAP_SHARED: TLB misses:251284std dev 103292)\nminflt: 2784(std dev 231)\nmajflt: 247(std 65)\noublock: 18200(std dev 2987)\nnivcsw: 8(7)  The most important difference here is that the oublock is much easier to trigger because the constant swapping. When file backed memory is used we also notice that majflt is much higher. Because pages are constantly traveling between swap area and memory, the page fault rate becomes a lot higher. The oublock also follows previous patterns as the file backed memory requires filesystem involvement.\nFinally, we also modify the kernel\u0026rsquo;s head (or more precisely its LRU page replacement algorithm). Look in mm/vmscan.c there\u0026rsquo;s a function calleed shrink_page_list. In it, you will see a switch statement with a PAGEREF_ACTIVATE case, which is the case where the kernel sees the page has been recently accessed. In this case the kernel gotos activate_locked, but you will change it to to do the same thing as the PAGEREF_RECLAIM case. We can simply move the case down and change its default behavior to direct to the PAGEREF_RECLAIM case. After that, we need to recompile the kernel for VM. We also summarize the most interesting results:\n MAP_PRIVATE + MAP_ANONYMOUS: TLB misses:308031(std dev 17298)\nminflt: 4223(std dev 791)\noublock: 8(std dev 1)\nnivcsw: 11(5) MAP_SHARED: TLB misses: 251284std dev 103292)\nminflt: 2724(std dev 231)\nmajflt: 0(std 0)\noublock: 18200(std dev 2987)\nnivcsw: 8(7)  We can see that the most of the pattern follow the previous result after the modified kernel is installed. One main difference is majflt value is reduced back down.\n","permalink":"https://www.bodunhu.com/blog/posts/linuxkernelmeasurementandmmap/","summary":"This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to cpu world. This is the system spec:\n   Attribute Value     Processor name (BIOS) Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz   Cores 6   Logical processors 12   TLB/Cache details 64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries.","title":"Linux Program Measurement and mmap"},{"content":"VMWare ESX Server is a software layer designed to multiplex hardware resources among virtual machines running unmodified commodity operating systems. ESX Server, different to VMware Workstation, is a type 1 hypervisor, which means it runs directly on bare metal. ESX Server focuses on running guest VMs without modifying the guest OSes at all, which is challenging.\n  Memory Virtualization is done by interposing an extra abstraction layer between a physical address from the VM\u0026rsquo;s point of view, and a machine address which represents the actual hardware memory. ESX Server maintains a pmap data structure for each VM to translate PPMs to MPNs. A separate shadow page table, consistent with the physical-to-machine mappings, is used to map virtual-to-machine page mappings. This avoids additional overheads as the hardware TLB will cache direct virtual-to-machine address translations read from the shadow page table.\n Key features  Ballooning is a technique used by the server to achieve memory reclamation. As its name suggests, the hypervisor inflates the balloon by instructing the balloon driver module to allocate pinned physical pages and deflates it by instructing it to deallocate previously-allocated pages. The idea behind this technique is that the hypervisor is unaware of the specific usage patterns of policies of its guests, therefore the making page replacement decisions is best done in the guest VM. When the hypervisor over commits memory, it needs some way to claim memories from the VMs. By consuming some of the memory that the guest OS believes is physically present in the virtual machine. The guest OS will then swap memory to disk reducing the load on the host\u0026rsquo;s physical memory. The host will them reallocate that memory to other VMs. A details description of ballooning can be found in this post.\n  Page Coloring can be used to reduce cache misses or partition resources. But it might complicates memory management, especially with the presence of huge pages. Because coloring enforces ownership, thus might result in distinct L2 cache entries.\n Sharing memory is achieved by comparing the content of each page, since modifying guest operating system internals is not possible. Because comparing each page would be \\(O(n^2)\\), hashing is used to identify pages to make the progress more efficiently. By letting VMs share pages based the contents, the host can potentially save spaces dramatically. For example, the presence of zero pages is a great opportunity for page sharing by mapping one zero page to multiple VMs. Hint is hash hit, but it doesn\u0026rsquo;t guarantee the content of the page doesn\u0026rsquo;t change at that moment.\nCopy-on-write (CoW) -- Idle Memory presents a problem in pure proportional-share algorithms because they do not incorporate any information about active memory usage. More specifically, the memory demand might change dynamically. ESX Server collects idle memory tax from VMs to mitigate this issue. A client is charged more for an idle page than the active one. The cost of idle memory is inflated by tax rate. The metrics of idles pages in guests is collected by hypervisor without guests' involvement. The idle page information in virtual page table inside VMs is periodically sampled on random bases.\nQuestions a. What is the overhead of ballooning? Triggering memory management in the VM by \u0026ldquo;tricking\u0026rdquo; it into thinking the the memory resource is scarce/plentiful may have unexpected behaviors.\nb. Do content-based sharing pose security vulnerabilities?\nc. Remapping hot I/O pages to low memory can be a bottleneck if the page number is high. How does modern hypervisor solution cope with this issue?\n","permalink":"https://www.bodunhu.com/blog/posts/vmwareesxserver/","summary":"VMWare ESX Server is a software layer designed to multiplex hardware resources among virtual machines running unmodified commodity operating systems. ESX Server, different to VMware Workstation, is a type 1 hypervisor, which means it runs directly on bare metal. ESX Server focuses on running guest VMs without modifying the guest OSes at all, which is challenging.\n  Memory Virtualization is done by interposing an extra abstraction layer between a physical address from the VM\u0026rsquo;s point of view, and a machine address which represents the actual hardware memory.","title":"Memory Resource Management in VMware ESX Server"},{"content":" Xen is an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, without sacrificing either performance or functionality. Xen is type I hypervisor, which directly runs on top of bare metal.\n paravirtualization - presents a virtual machine abstraction that is similar but not identical to the underlying hardware.\n  The Virtual Machine Interface Memory is hard to virtualize mostly because x86 doesn\u0026rsquo;t support software-managed TLB. A tagged TLB entry allows both guest OS and hypervisor to coexist because it can be associated with an address-space identifier. This is not possible on x86, thus address space changing likely requires flushing the TLB. Thus, to achieve better performance, guest OSes are responsible to managing hardware page tables. Batching can be used by the guest OS to reduce constantly requesting new pages from the hypervisor when new processes are created.\nCPU virtualization has implications for guest OSes. Principally, OS the most privileged entity on top of hardware. A hypervisor in the middle means the guests OSes must be modified to run a lower privilege level. On x86, this is not a problem since OSes executes in ring 0 while applications execute in ring 3, leaving ring 1 and ring 2 unused. Privileged instructions executed by the guest has to go through the check of hypervisor in general. For performance reasons, system call exceptions can be handled directly by the CPU. As for paging faults, this needs to go through the hypervisor because only code in ring 0 can result the faulting address from CR2.\nDevice I/O is implemented by transfer data between guest and Xen using shared-memory async buffer-descriptor rings. Event delivery is achieved by hypervisor sending notification to its guest asynchronously. When and whether to hold off these callbacks is at the discretion of the guest.\n Essentially, the virtualization interface design is based on a number of factors. The hypervisor acts as a security guard that validates the guest\u0026rsquo;s request which would go directly to hardware normally if running in ring 0. The bottom line is the hypervisor shouldn\u0026rsquo;t be involved unless the there are hardware limitations, or when resource validation or management are required. The goal is to separate policy from mechanism wherever possible. This similar to exokernel in that the hypervisor merely provides basic functionalities without understanding higher level issues.\nQuestions a. Why does x86 make it hard to support efficient virtualization?\nb. How does Xen exists in 64MB section at the top of every address space avoid TLB flushes when entering and leaving the hypervisor?\n","permalink":"https://www.bodunhu.com/blog/posts/xenvirtualization/","summary":"Xen is an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, without sacrificing either performance or functionality. Xen is type I hypervisor, which directly runs on top of bare metal.\n paravirtualization - presents a virtual machine abstraction that is similar but not identical to the underlying hardware.\n  The Virtual Machine Interface Memory is hard to virtualize mostly because x86 doesn\u0026rsquo;t support software-managed TLB.","title":"Xen and the Art of Virtualization"},{"content":" This is a summary of how to compile and boot the Linux kernel on the KVM-qemu virtual machine. It covers how to get a VM running in KVM, how to build a customized kernel, and how to use GDB with the Linux kernel. The experiment is conducted on an amd64 architecture CPU. We use Ubuntu as our testing environment but the steps covered here should apply to other distros as well.\n Getting a VM running in KVM The Ubuntu ISO image is downloaded from the Canonical website. The kernel is downloaded directly from kernel.org. The specs of our test environment is:\n CPU: Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz RAM: 32 GB Host and Guest OS: Ubuntu 20.04.1 LTS Host Kernel Version: 5.4.0-47-generic GCC: 7.5.0 QEMU emulator version: 4.2.0 Guest Kernel Version: 5.8.6  After we obtained the Ubuntu ISO image, we use GUI virt-manager to install the OS. One thing to notice here is the default directory for virtual disks is /var/lib/libvirt/images, since my system partition is located on a separate SSD with limited space, the virtual disk directory is changed to my /home directory instead.\nWe also create the new virtual disk inside virt-manager. We chose raw format instead of qcow2. Creating a new image file can also be done in command line using:\nqemu-img create -f raw -o preallocation=full vmdisk.img 40G The preallocation can be turn either on or off depends on personal choices. After the disk image is created, we proceeds in virt-manager to install Ubuntu on the newly allocated virtual disk. We enabled storage for this virtual machine so that we don\u0026rsquo;t need to repeat the installation process every time we launch the VM. One thing to be noticed here is we don\u0026rsquo;t need swap area inside a virtual machine. We can simply use the whole virtual disk for / partition.\nTo start the VM from cmd, you might need to change the owner of the disk image. We add the user to both kvm and libvirt. The image created or accessed by virt-manager seems to change the file owner to libvirt-qemu, which may cause problems when starting from cmd.\nAfter the installation is finished, we can simply launch the virtual machine inside virt-manager through its GUI interface. We can also use command line to start the VM:\nkvm -accel kvm -m 8G -smp 6 --snapshot -drive format=raw,file=/home/ed/virtimg/ubuntu20.04 The argument -accel kvm enables Kernel-based Virtual Machine full virtualization, which uses hardware acceleration. Without this option the VM will become extremely slow. The -m 8G assigns the given amount of memory to the VM. The -smp 6 assigns the given number of cores to the guest if the host has multiple cores. The --snapshot ensures that no changes are made to your image during an execution so you can do something dangerous and have the original image file preserved. The -drive option specifies the location of the virtual disk and its format. We will use some of these options later.\nTo confirm the VM has internet access, simply execution apt install pkg-name in the guest terminal. No error message would indicates properly functioning network access from the guest VM. For example, when we execute sudo apt install llvm it shows:\nReading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: llvm-runtime The following NEW packages will be installed: llvm llvm-runtime 0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded. Need to get 6,796 B of archives. After this operation, 128 kB of additional disk space will be used. Do you want to continue? [Y/n] Building the Kernel We can use out customized kernel for our newly created VM. After we obtain the Linux kernel from kernel.org, we extract the source into \u0026lt;kernel dir\u0026gt; and create a separate build directory \u0026lt;kbuild\u0026gt; (outside \u0026lt;kernel dir\u0026gt;).\nThen we enter the \u0026lt;kbuild\u0026gt; directory, run\nyes \u0026#34;\u0026#34; | make -C /home/ed/Desktop/linux_kernel/kbuild O=$(pwd) config This will create a .config file inside \u0026lt;kbuild\u0026gt; with the default options selected. We then open the configuration file and ensures CONFIG_SATA_AHCI=y, which builds the SATA disk driver into the kernel. That will allow your kernel to boot off a (virtual) SATA drive without having to load a module to do it.\nNext we build the kernel by running make in \u0026lt;kbuild\u0026gt;. We use the -j 6 option speedup the building process using multiple processor cores. This process can take a long time.\nBuild and Install Kernel Modules To build modules locally on host, we create another separate \u0026lt;install_mod_dir\u0026gt; directory for building kernel modules. Then in \u0026lt;kbuild\u0026gt;, execute\nmake INSTALL_MOD_PATH=/home/ed/Desktop/linux_kernel/install_mod_dir modules_install Now there is a lib directory inside /home/ed/Desktop/linux_kernel/install_mod_dir, which holds all the kernel modules we are about to install.\nThe complete list of modules can be listed using cat modules.builtin inside lib/moduels/5.8.6. Here is a link to all the modules being built. We didn\u0026rsquo;t modify anything in the configuration.\nThen we use guestmount to mount the virtual disk to a mount point on the host\nguestmount -a /home/ed/virtimg/ubuntu20.04 -i ~/vm/linux/ In Ubuntu this step yields the following message:\nlibguestfs: error: /usr/bin/supermin exited with error status 1. To see full error messages you may need to enable debugging. Do: export LIBGUESTFS_DEBUG=1 LIBGUESTFS_TRACE=1 and run the command again. For further information, read: http://libguestfs.org/guestfs-faq.1.html#debugging-libguestfs You can also run 'libguestfs-test-tool' and post the *complete* output into a bug report or message to the libguestfs mailing list. The underlying problem is that the kernel cannot be read and according to the post and the bug report on Ubuntu Launchpad.\nTo fix the issue, we need to run\nsudo chmod +r /boot/vmlinuz-* We can verify the contents inside ~/vm/linux by simply cd into it.\nTo install the modules we just built, we can copy the \u0026lt;install_mod_dir\u0026gt;lib/modules into the mounted filesystem \u0026lt;mount_point\u0026gt;/lib/modules.\nFinally, we unmount the filesystem by doing\nfusermount -u /mnt/hdd1/vm/linux Booting KVM with new Kernel To boot up the VM with the new kernel, we will add a few extra command line options to kvm. For convenience, we put the scripts into a file. It\u0026rsquo;s also available on gist:\n#!/bin/bash  kvm \\  -s \\  -display gtk \\  -cpu host \\  -vga qxl \\  -accel kvm \\  -kernel \u0026#34;/home/ed/Desktop/linux_kernel/kbuild/arch/x86/boot/bzImage\u0026#34; \\  -append \u0026#34;root=/dev/sda1 console=ttyS0,115200n8 nokaslr\u0026#34; \\  -drive format=raw,file=/home/ed/virtimg/ubuntu20.04 \\  -m 8G \\  -smp 6 \\  --snapshot \\  -S Aside from the command line arguments we discussed before, there are a few new members here. the -s switch is a shorthand for -gdb tcp::1234. The -display gtk is optional. It enables the opengl context in the display device for gtk display output. -cpu host says the guest should emulate the host processor. -vga qxl enables 3D acceleration on the guest system. -vga virtio also offers good performance in our case. -kernel allows bootloader to pickup the new kernel. The -append along with its arguments specifies where the root partition of the hard disk is and the console parameter adds a serial console at boot so you can see boot messages. The --snapshot in QEMU says the images that refer to an original image will use Redirect-on-Write to avoid changing the original image. The -S means the kernel won\u0026rsquo;t start executing unless we attach a debugger to it. We only use it later in the debugging stage.\nAgain, we can verify there is internet access using the new kernel using apt update. There are no errors shown, which indicates the network is functioning correctly.\nBooting Process Now we are able to boot up the VM successfully, we can first measure how much time the kernel spends in booting. Running dmesg -d shows the timestamp and time delta spent between messages. The final line shows [10.842998]. If we use systemd-analyze, it outputs\nStartup finished in 795ms (kernel) + 5.451s (userspace) = 6.247s graphical.target reached after 5.439s in userspace The reason why there is a gap between these two measurement is because dmesg is not a reliable test of how long a boot-up process goes. dmesg itself merely collects information. The drivers and other system processes can output messages at any point in time. There may or may not be processes spawning between those messages.\nNext, we are going to look at how PCI device is involved in kernel startup. lspci outputs the follow\n00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02) 00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II] 00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II] 00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03) 00:02.0 VGA compatible controller: Red Hat, Inc. Virtio GPU (rev 01) 00:03.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 03) We can use the PCI address here to search for corresponding information in dmesg. For example, if we use the domain value \\(0000:\\) as query, we get something like:\n[ 0.295026] PCI host bridge to bus 0000:00 [ 0.299055] pci 0000:00:00.0: [8086:1237] type 00 class 0x060000 [ 0.300133] pci 0000:00:01.0: [8086:7000] type 00 class 0x060100 [ 0.301163] pci 0000:00:01.1: [8086:7010] type 00 class 0x010180 [ 0.311006] pci 0000:00:02.0: [1af4:1050] type 00 class 0x030000 [ 0.319650] pci 0000:00:03.0: [8086:100e] type 00 class 0x020000 The full result is also available as gist.\nThe lspci command specifies the type of device right after the address. For example, the first one is host bridge. We specifically selected the message in the type 00 class format here. The significance here is that the class value actually telss us the type of the corresponding device. We can check the include/linux/pci_ids.h for each macro respectively. For example,\n#define PCI_CLASS_NETWORK_ETHERNET\t0x0200 this line shows the value 0x0200 corresponds to a network PCI device. This aligns with our dmesg output as well as the lspci result.\nDebugging Kernel To build KVM+GDB-friendly kernel, we need to have proper CONFIG_DEBUG* options set in the .config file. More specifically, we need to have the following options enabled:\n CONFIG_DEBUG_INFO y: compile the kernel with debug info. The full list of definitions can be found here. CONFIG_DEBUG_INFO_DWARF4 y: generate dwarf4 debug info. Definition can be found here. CONFIG_GDB_SCRIPTS y: creates the required links to GDB helper scripts in the build directory. Full definition can be found here. CONFIG_GDB_INFO_REDUCED n: disable reduced gdb info. CONFIG_KGDB y: kernel debugging location. Full list of definitions found here. CONFIG_FRAME_POINTER y: compile the kernel with frame pointers. Full list of definitions found here. CONFIG_SATA_AHCI y: this option enables support for AHCI Serial ATA. Definition found here. CONFIG_KVM_GUEST y: this option enables various optimizations for running under the KVM hypervisor. Definition found here. CONFIG_RANDOMIZE_BASE n: drop support for Kernel Address Space Layout Randomization (KASLR). Definition found here. We also added nokaslr in our qemu arguments. CONFIG_SMP y: enable Symmetric multi-processing support. Definition found here.  Now we can recompile the kernel and attack gdb to it. We simply add -S option to kvm to only start the VM when gdb is attached. Then we enter our \u0026lt;kbuild\u0026gt; directory and execute:\ngdb vmlinux (gdb) target remote:1234 The step is also documented in the kernel community documentation.\nSet Breakpoints Spin lock is easy to find in a kernel. Therefore, we will set break points on spin_lock. For kernel 5.8.6, we see that spin_lock is defined in https://elixir.bootlin.com/linux/v5.8.6/source/include/linux/spinlock.h#L351 as a inline function. If we trace the function, we can see the actual function we should use is _raw_spin_lock defined here:\n#ifndef CONFIG_INLINE_SPIN_LOCK void __lockfunc _raw_spin_lock(raw_spinlock_t *lock) { __raw_spin_lock(lock); } If we need to break the execution only when a given program is executed, we can use the program PID to as the condition. The problem is, how do we get the program PID if it doesn\u0026rsquo;t last for long?\nWe could instead first set a breakpoint on fork. We can break its kernel call at _do_fork which is defined here. After that, we can simply continue executing the kernel until we run the program.\n Note: we need to compile the program and open a new terminal first. Since they both involves forking new processes, which will hit _do_fork before our program runs.\n Then we print the process PID using p $lx_current().pid, we then use this value as the condition for b _raw_spin_lock if $lx_current().pid == pid_value inside gdb.\nIf we want _raw_spin_lock to break under different contexts, we can simply use PID as different contexts. We can also set break points in functions in different contexts that calls spin_lock and see what they do. For example, we can set break point at expand_downwards defined in here, if we back trace this function, we will get a series of calls, we mention the important ones here\n#1 0xffffffff81284c4e in expand_stack #3 0xffffffff813843db in load_elf_binary #8 do_execve #12 0xffffffff81b1f658 in do_syscall_64 We also added a helper script in .gdbinit to print our the name of the function, which is \u0026lsquo;\u0026lsquo;anacron\u0026rsquo;\u0026rsquo; in this case. In short, this process execute commands periodically, and it performs a sys call which loads elf binary, thus requiring stack expansion.\nAnother example is timer interrupt. The get_next_timer_interrupt calls _raw_spin_lock. We select some messages from backtrace:\n#1 0xffffffff8113b224 in get_next_timer_interrupt #2 0xffffffff8114d52e in tick_nohz_next_event #4 tick_nohz_idle_stop_tick () #5 0xffffffff810df567 in cpuidle_idle_call () In short, the is a timer interrupt that gets called when CPU is idle.\nThe last example is hrtimer_interrupt. The selected messages are:\n#4 0xffffffff8114d80c in tick_sched_timer #7 0xffffffff8113c8e7 in hrtimer_interrupt #12 run_on_irqstack_cond #14 0xffffffff81c00cc2 in asm_sysvec_apic_timer_interrupt In summary, hrtimer_interrupt is called as event handler. This function is responsible to select all timers that have expired and either move them to the expiration list (if they may be processed in softIRQ context) or call the handler function directly.\nSyscall Essentially, processor switches from the user mode to kernel mode and starts execution of the sys call entry - entry_SYSCALL_64, we can find its definition at here. This is the only entry point used for 64-bit system calls. We can set a break point here. When the break point is hit, we use info registers in gdb to get the value of cr3. In our case, it is 0x22a6d5806. Then we simply step from this breakpoint, and will likely reach SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp. After this call the value in cr3 is changed to 0x22a6d4006. The macro is defined here.\nWe can see whenever the processor switch from the user mode to kernel mode the value of cr3 is changed. The root cause the Page Table Isolation (PTI). It is a countermeasure against attacks on the shared user/kernel address space such as the \u0026lsquo;\u0026lsquo;Meltdown\u0026rsquo;\u0026rsquo; approach. To mitigate this class of attacks, two independent page table copies are created, one in kernel space, one in user space. The cr3 register enables the processor to translate linear addresses into physical addresses by locating the page directory and page tables for the current task. So whenever the process enters kernel mode, the kernel copy requires its page directory address to be loaded into cr3 register.\nIf we add nopti in -append in the QEMU cmd argument and perform the same steps. We get 0x231466005 before and after SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp is executed. Based on the desciption in the linux kernel tree, the nopti on X86_64 is equivalent to pti=off, therefore explaining the constant value of cr3.\n","permalink":"https://www.bodunhu.com/blog/posts/kernelhacking/","summary":"This is a summary of how to compile and boot the Linux kernel on the KVM-qemu virtual machine. It covers how to get a VM running in KVM, how to build a customized kernel, and how to use GDB with the Linux kernel. The experiment is conducted on an amd64 architecture CPU. We use Ubuntu as our testing environment but the steps covered here should apply to other distros as well.","title":"Start Linux Kernel Hacking"},{"content":"This research is conducted by Martin Heusse, Franck Rousseau, Cilles Berger-Sabbatel, Andrzej Duda on analyzing the performance of the IEEE 802.11b wireless local area networks. Degraded transmitting rate is caused by CSMA/CA channel access method.\n Overview The performance of the IEEE 802.11b wireless local area networks have degraded performances when some mobile hosts use a lower bit rate than the others, which is caused by CSMA/CA channel access method. When one host changes it modulation type which degrades bit rate, it occupies the channel for a longer time, causing other hosts still using higher bit rate to be penalized. The paper Performance Anamoly of 802.11b analyzes how such anomaly works.\nTransmission Overhead Consider there is only a single host in a 802.11b cell transmitting a single data frame. The overall transmission time is expressed as:\n$$T = t_{tr} + t_{ov}$$\nwhere the constant overhead\n$$t_{ov} = DIFS + t_{pr} + SIFS + t_{pr} + t_{ack}$$\nThe transmission process can be represented by the graph\n When there are multiple hosts attempting to transmit, a host will execute the exponential backoff algorithm - it waits for a random interval to avoid saturating the channel, resulting in extra time spent in the contention procedure:\n$$T = t_{tr} + t_{ov} + t_{cont}(N)$$\nFinally, the useful throughput obtained by a host depends on the number of hosts:\n$$p(n) = t_{tr} / T(N)$$\nThis indicates the useful throughput is smaller than the nominal bit rate and largely depends on the number of competing hosts.\nAnomaly Assume there are \\(N\\) hosts, \\(N-1\\) hosts use high transmission rate \\(R=11\\)Mb/s, one hosts transmits at rate \\(r=5.5\\), \\(2\\), or \\(1\\) Mb/s. We can deduce the transmission time of the fast ones:\n$$T_f = t_{ov}^{R} + \\frac{s_d}{R} + t_{cont}$$\nThe transmission time of the slow host is:\n$$T_s = t_{ov}^{R} + \\frac{s_d}{r} + t_{cont}$$\nThe short term behavior of CSMA/CA is shown to be not fair, thus we have\n$$U_f = \\frac{T_f}{(N-1)T_f + T_s + P_c(N)\\times t_{jam} \\times N}$$\n\\(t_{jam}\\) is the average time spent in collisions, calculated between the all possible pairs between the fast hosts and the slow one:\n$$t_{jam} = \\frac{2}{N}T_s + (1 - \\frac{2}{N})T_f$$\nThe throughput at he MAC layer of each fast hosts is:\n$$X_f = U_f \\times p_f(N) \\times R$$\ngiven that:\n$$p_f(N) = \\frac{s_d}{RT_f}$$\nWe apply the same process for the slow host, given \\(p_s(N) = \\frac{s_d}{rT_s}\\), what we get eventually is:\n$$X_f=X_s = X$$\nThis key point here is that the fast hosts transmitting at the higher rate R obtain the same throughput as the slow host transmitting at the lower rate.\nSimulation and Measurement Results In general, the experimental value of \\(P_c(N)\\) seems to match the theory model. One thing the paper could illustrates better is to show how experimental value matches the equation as the number of hosts increases. The average and cumulative throughput value also seems reasonable compared to the expression discussed before.\nThe throughput is measured using three different tools: netperf, tcpperf, and udpperf. This idea of duplication makes the data collected more reliable and persuasive, which is especially useful in benchmarking since the results can be sensitive to environmental variable changes.\nThe presented results justify the statement made in the paper. For example, the measured TCP throughput for two hosts is shown to degrade as time passes:\n One thing the paper can articulate more is how this seemly periodic pattern is related to the model. Another concern is the number of device used to conduct these experiments. The number of devices used seems to be much smaller than what would be in real-world scenario. It will be interesting to see how the performances are affected with a lot devices competing for a channel. This can be further extended to measuring performances with multiple devices having lower bit rate, which is more likely to capture real-world use cases. The potential performance impact is not clear given the present measurement.\nThe paper also claims the useful throughput strongly depends on the number of competing host. More data related to how the number of hosts is related to performance impact will make this paper more interesting. It may be hard to achieve as many papers resort to simulation.\nThis paper has made improvements over previous work in that it studies the performance of 802.11 WLANs, with one host having lower bit rate, whereas many other assume that all hosts communicate using the same bit rate. This is a step forward to capture more realistic situations. Overall, the paper does a good job in terms of proving its point. It captures the most critical information and it\u0026rsquo;s easy to follow the concept. However, the neat structure can make readers without sufficient background to spend more time catching up since the background section may not be enough for starters.\nConclusion Overall, this paper brings novel approach to analyze the performance of 802.11 WLANs with varying bit rate. It brings new insights into studying the 802.11 standard. The paper focuses on TCP and UDP protocols. Applying the method discussed in paper to a lesser known protocol such as DCTCP can yield more insights into the different protocols can affect the throughput. Another direction is to generalize this model to multiple bit rate degrading and study their behaviors.\nThe bit rate used in the paper also seems to be pretty low compared to modern standards. With the introduction of 5G network, the bit rate becomes a lot higher, it will be interesting to see how extremely high bit rate can affect the performance of 802.11.\n","permalink":"https://www.bodunhu.com/blog/posts/wireless_anomaly/","summary":"This research is conducted by Martin Heusse, Franck Rousseau, Cilles Berger-Sabbatel, Andrzej Duda on analyzing the performance of the IEEE 802.11b wireless local area networks. Degraded transmitting rate is caused by CSMA/CA channel access method.\n Overview The performance of the IEEE 802.11b wireless local area networks have degraded performances when some mobile hosts use a lower bit rate than the others, which is caused by CSMA/CA channel access method. When one host changes it modulation type which degrades bit rate, it occupies the channel for a longer time, causing other hosts still using higher bit rate to be penalized.","title":"Performance Anamoly of 802.11b"},{"content":"Exokernel is a term every system researcher has heard of at some point in life. However, according to the PDOS group at MIT, there aren\u0026rsquo;t any exokernel-based operating systems in active use today. It\u0026rsquo;s interesting to discover what ideas exokernels brought to the OS high-level design and some potential drawbacks of such design choice.\n Perhaps the most important thing to keep in mind is that exokernel operating system architecture pushes management of physical resources to the application level, contrary to what most monolithic kernel would do: providing hardware resource management through some form of abstraction, usually hiding hardware-related details.\nLimitations of Traditional Approaches Monolithic kernels usually enforce centralized resource management via a set of abstractions. In microkernel-based system, they are usually provided through some form of trusted user-level servers. There are several drawbacks:\n  Too general. Over generalizing can limit application diversity and have performance implications (domain/application-specific approach usually have performance improvements, in the cost of, well, being more \u0026ldquo;specific\u0026rdquo;.). For example, in UNIX, two applications exhibiting rather different memory access patterns are subject to the general-purpose OS scheduler and page replacement policy. Letting applications define such policies can open doors for performance improvements since applications have better knowledge of their behaviors.\n  Hide information. This is further expanded from the previous point. Applications tend to have better \u0026ldquo;self-awareness\u0026rdquo; and can implement custom policies that outclass the general-purpose ones provided by the kernel.\n  Limited functionality. Having limited resources in hand can inhibit implementation of new ideas.\n  However, generalization may not be a bad thing. As discussed in the UNIX the Timesharing System paper, having a generalized and unified yet limited file system API can simplify programming efforts. Accessing both ordinary files and I/O devices is achieved by utilizing a unified interface. Nobody today wants to implement a different set of policies just for character device or block device.\nDesign Essentially, exokernel consists of thin veneet that multiplexes and exports physical resources through a set of primitives. The libraries, running in the application space, use them to implement with special-purpose functionalities in a higher abstraction level. The architecture is shown in the Paper:\n There are three majors tasks to separate protection from management:\n  Tracking ownership of resources.\n  Ensure protection by guarding resource usage.\n  Revoke access.\n  The paper presents three techniques to achieve these goals:\n  secure binding: lib OS can securely bind to machine resources.\n  visible revocation: lib OS can participate in a resource revocation protocol. (Keep in mind why the revocation needs to be visible)\n  abort protocol: exokernel itself can break secure binding of uncooperative lib OS.\n  In general, exokernel should expose hardware resources such as disk memory, CPU, interrupts, through low-level primitives with as few abstractions as possible. The resource management policy should be enforced by the library OS instead. The policy control boils down to whether the exokernel permits resource allocation.\nSecure Binding One of the primary tasks of an exokernel is to multiplex resources securely, providing protection for mutually distrustful applications. Secure binding allows the kernel to protect resources without understanding them.\nThere are three techniques to implement secure bindings: hardware mechanisms,software caching, and downloading application code.\nUnderstanding Secure Binding through Examples Secure binding is rather abstract and hard-to-comprehend concept without concrete examples. Here are some examples illustrating how secure multiplying is achieved through secure binding.\nTake memory allocation for an example. When a library OS tries to allocate a physical memory page, the exokernel creates a secure binding for that page by recording the owner and the capabilities specified by the library OS. Essentially, accessing memory resources is achieved through capability. The exokernel acts as a door-keeper that checks the validity of the capability from the library OS.\nI personally like to think the role of the exokernel in memory system is to act as a security guard that protects resources that can be access by the library OS through some form of interface. For example, if the hardware defines a page-table interface, which can be accessed by the lib OS, the exokernel must guard the page table. If the lib OS tries to enter a new virtual-to-physical memory mapping, then the exokernel must check the corresponding memory capability.\nIn summary, privileged machine operations must be guarded by the exokernel.\nAegis: the exokernel Up to this point I find it still hard to full understand what exokernel is capable of. Having a concrete system to study for is much more helpful. So here comes Aegis.\nHere is a subset of Aegis\u0026rsquo;s primitives and sys call interfaces that encapsulate these exported primitives. Having a concrete list feels so much better than reading a list of abstract terms!\nHere is a sublist of primitives:\n   Primitive Operations Description      TLBwr Insert mapping into TLB    TLBvadelete Delete virtual address from TLB     And here is a sublist of system call interfaces:\n   System Call Description      Yield Yield processor to named process    Alloc Allocation of resources    Scall Synchronous protected control transfer     Address Translation It\u0026rsquo;s important to first mention that Aegis provides a small number of guaranteed mappings by partitioning an application\u0026rsquo;s virtual address space into two segments. The first segments hold normal application data; the other one has guaranteed mapping and holds exception code and page-table. (Guaranteed mapping is sort of a safe lock.)\nWhen a TLB miss happens, there are several steps happening:\n  Aegis checks which segment the virtual address resides in. If it\u0026rsquo;s in the standard user segment the exception is dispatched to the application. Otherwise, the exokernel handles the exception or forwards it to the application depends on whether there\u0026rsquo;s guaranteed mapping.\n  The application looks up the address in it page table, inserts TLB entry and creates capability, then invokes Aegis system routine.\n  Aegis validifies the capability. Upon approval, the mapping is installed.\n  Application resumes execution from kernel mode.\n  The key takeaway here is the exokernel itself is involved in very few privileged operations such as interacting directly with the hardware via low-level primitives. the bulk of the work is done in the application level.\nBecause the kernel contains minimal functionalities, it can be extremely fast compared to a monolithic kernel. However, does that mean the overhead is shifted to the library OS instead?\nExOS: the Library OS The most prominent feature about library OS is that it manages operating system abstractions at application level.\nThe GEMM operation on both ExOS and Ultrix (a monolithic kernel OS) doesn\u0026rsquo;t seem to have much difference since GEMM doesn\u0026rsquo;t use any special abilities of both OSes. It does indicates that the performance gain from the minimal design of exokernel is somewhat cancelled out by the application-space overhead.\n The exokernel paper mentions that in the context of networking, the major reason for ExOS to download code is that the network buffers on our machines cannot be easily mapped into application space in a secure way. Downloading the code into the kernel allows applications integrating operations such as checksum during the copy of the message from these buffers to user space. However, I\u0026rsquo;m a little bit skeptical of this statement today. Usually a highly performant TCP stack will be implemented in userspace, along with some polling (DPDK for example). But it will be interesting to compare the exokernel approach to the gigantic Linux TCP stack. The second reason is downloaded code is bounded, thus allowed full context switch to an unscheduled application.\n I do find the graph in the exokernel paper interesting. It shows that when application-level message handlers are downloaded into the kernel, the roundtrip latency is almost not affected by the number of processes. Since the operation is performed inside kernel upon message arrival, no handling is needed from the application. This has the advantage that application handler is subject to scheduling, which has performance implications. (The choice of scheduler is the key bottleneck here.)\n Modularity It a natural property of exokernel since the exokernel itself is simplistic. Thus, operating system abstractions can be redefined simply by changing the library OS. Thus, applications have finer-grained control over resources. However, I think it comes at a cost. In a monolithic kernel, applications are subject to general purpose scheduler. Having modular domain-specific schedulers can indeed improve performances, however, it might also leads to multiple scheduler contention, which is not covered in the paper.\nConclusion Exokernel does offer some new insights into system design. The simple design concept of the exokernel itself has major performance benefits as well as a limited set of primitives which gives much freedom to the application. However, that means the library OS has to take more responsibility. The paper didn\u0026rsquo;t cover enough analytics on more general use cases. The performance gain seems to come from some highly specialized, exokernel-specific implementations of OS abstractions (such as IPC, VM, etc.). The more general case, such as GEMM, seem to be much less performance, when compared to traditional approaches. It will be good to see how exokernel performs under more diverse workloads.\nI\u0026rsquo;ve also heard that one reason microkernels never took off was partially due to the performance slowdown compared to monolithic kernels. Since exokernel shared many similarities with microkernels (seems like exokernel is a more stripped-down version of microkernel since it barely has an OS core), it will likely fall into the same caveat. However, there doesn\u0026rsquo;t seems to have a comprehensive benchmarking trials to compare all major types of kernels.\n","permalink":"https://www.bodunhu.com/blog/posts/exokernel/","summary":"Exokernel is a term every system researcher has heard of at some point in life. However, according to the PDOS group at MIT, there aren\u0026rsquo;t any exokernel-based operating systems in active use today. It\u0026rsquo;s interesting to discover what ideas exokernels brought to the OS high-level design and some potential drawbacks of such design choice.\n Perhaps the most important thing to keep in mind is that exokernel operating system architecture pushes management of physical resources to the application level, contrary to what most monolithic kernel would do: providing hardware resource management through some form of abstraction, usually hiding hardware-related details.","title":"Exokernel"},{"content":"Unix is general-purpose, multi-user, interactive operating system, it offers several new features hardly found in other larger operating systems back in the day. These features include (1) a hierarchical file system incorporating demountable volumes; (2) compatible file, device, and inter-process I/O; (3) the ability to initiate asynchronous processes; (4) system command language selectable on a per-user basis; and (5) over 100 subsystems including a dozen languages.\n Simplicity at its Core Simplicity was engraved into the gene of Unix since its birth, as the paper states: \u0026ldquo;Perhaps the most important achievement of UNIX is to demonstrate that a powerful operating system for interactive use need not be expensive either in equipment or in human effort\u0026rdquo;. Therefore, it is important to keep in mind how simplicity is reflected in the design of Unix.\nThe File System Perhaps the singly most important part of Unix. The \u0026ldquo;everything is file\u0026rdquo; concept that influences all modern system designs. Here is a short description of each major file types.\n Ordinary Files: no particular structuring is expected by the system. The structure of files is controlled by the programs which use them, not by the system. Directories provide the mapping between the names of files and the files themselves, inducing a structure on the file system. The only difference between directory and normal file is the the directory can\u0026rsquo;t be written on by unprivileged programs, meaning the contents of directories are controlled by the system.   linking allows the same non-directory file to appear in several directories under possibly different names; a directory entry for a file is sometimes called a link. All links to a file have equal rights. A directory entry for a file consists merely of its name and a pointer to the file metadata. Therefore a file exists independently of any directory entry. Directory can be considered as link.\n  Special Files: perhaps the most prominent feature of the \u0026ldquo;everything is a file\u0026rdquo; principle. They are read and written just like ordinary disk files, but requests to read and write will result in activation of the I/O device. It blurs the line between file and device I/O since they share identical interfaces and are subject to the same protection mechanism.  Removable File System The Unix file system has a mount system request which, in effect, replaces a leaf of the hierarchy tree (the ordinary file) by a whole new subtree (the hierarchy stored on the removable volume). It provides a unified abstraction of the file system hierarchy where the underlying storage components become transparent to the user.\n One exception to the identical treatment of files on different devices: no link may exist between one file sys hierarchy and another. Otherwise, some form of bookkeeping would be required to when a removable volume is dismounted from one file system but not the other.\n Protection Each user is assigned a unique user ID. A file, upon its creation, is marked with the user ID of its owner. Also given for new files is a set of seven protection bits. Six of these specify independently read, write, and execute permission for the owner of the file and for all other users. This is a perfect example of ACL (access control list) system.\nI/O Calls Once again, we see how Unix is trying to provide a unified interface such that performing I/O on different devices doesn\u0026rsquo;t would not require different accessing patterns or styles. There is no distinction between \u0026ldquo;random\u0026rdquo; and sequential I/O, nor is any logical record size imposed by the system. Calls like open, seed, read, and write can be found in all major Unix-like systems today.\nI found it interesting that the authors were arguing why there are no user-visible locks in the file system. The first argument says: \u0026ldquo;they are unnecessary because we are not faced with large, single-file data bases maintained by independent processes\u0026rdquo;. It might be different today on modern systems so I have some doubts on that argument. The next one is \u0026ldquo;they are insufficient because locks in the ordinary sense, whereby one user is prevented from writing on a file which another user is reading, cannot prevent confusion when, for example, both users are editing a file with an editor which makes a copy of the file being edited.\u0026rdquo; This certainly is true because the the copies are separate files with distinct metadata during editing but once the editing is finished then it becomes tricky when the updated content needs to be written back to the original file without some form of synchronization or ordering.\nThe paper further explains the the system has sufficient internal interlocks to prevent these situations from happening. The exact details of how it works is not quite clear at this stage.\nImplementation As we\u0026rsquo;ve already known, a directory entry contains only a name for the associated file and a pointer to the file itself. This pointer is an integer called the i-number. When the file is accessed, its i-number is used as an index into a system table (the i-list) stored in a known part of the device on which the directory resides.\n Directory entry -\u0026gt; (File Name, i-number) -\u0026gt; i-list -\u0026gt; i-node -\u0026gt; description of the file\n Because the file is described by its corresponding i-node, any copy and deleting operations are circulating around modifying directory entry or i-node link-count field without actually touching the bulk of the file itself.\n It important to distinguish between file descriptor and inode. By definition, files are represented by inodes. The inode of a file is a structure kept by the filesystem which holds information about a file, like its type, owner, permissions, inode links count and so on. Other other hand, the file descriptor is the value returned by an open call is termed a file descriptor and is essentially an index into an array of open files kept by the kernel. There is an inode in the i-list but every process can have its own file descriptor for one file.\n Processes A process is the execution of an image. An image is a computer execution environment. It includes a core image, general register values, status of open files, current directory, and the like. An image is the current state of a pseudo computer. You can imagine the image as a motionless snapshot of current state of the processor, or you can image as the content saved to the main memory when a currently executing process is preemptied by another one.\nThe user-core part of an image has three logical segments. The program text segments starting from location 0. At the first 8K byte boundary above the text segment is a non-shared, writable data segment. The highest address in the virtual address space is a stack segment.\nOne key feature of UNIX is a new process can come into existence only by ise of the fork system call. Another system primitive is invoked by execute. This call resembles a \u0026ldquo;jump\u0026rdquo; machine instruction rather than a sub-routine call.\nShell Shell is a command line interpreter. Programs executed by the Shell start off with two open files which have file descriptors 0 and 1, representing files for reading and writing. The symbol \u0026ldquo;\u0026lt;\u0026rdquo; and \u0026ldquo;\u0026gt;\u0026rdquo; represent what files the file descriptor 0 and 1 will refer to for the duration of the command passed to shell.\nA filter, represented by \u0026ldquo;|\u0026rdquo;, is a program that copies its standard input to its standard output (without processing).\nCommand separator, represented by \u0026ldquo;;\u0026rdquo;, is used to separate multiple commands. A related feature is \u0026ldquo;\u0026amp;\u0026rdquo;, which execute the command in the background. When the shell doesn\u0026rsquo;t wait for the completion of a command, the identification of the process running that command is printed. In addition, parentheses can be used to enforce order of execution.\n It\u0026rsquo;s worth noting the shell is itself a command, and may be called recursively.\nSince it\u0026rsquo;s a command, it also shared the luxury of having standard I/O file descriptor. Thus, command such as:\nsh \u0026lt; file_containing_shell_commands would work.\n The last step in the initialization of UNIX is the creation of a single process and the invocation of a program called init. init have various sub-instances prompting for user login information. If the login succeeds, init performs an execute of the Shell. Essentially, init is the parent process of Shell.\n","permalink":"https://www.bodunhu.com/blog/posts/unix/","summary":"Unix is general-purpose, multi-user, interactive operating system, it offers several new features hardly found in other larger operating systems back in the day. These features include (1) a hierarchical file system incorporating demountable volumes; (2) compatible file, device, and inter-process I/O; (3) the ability to initiate asynchronous processes; (4) system command language selectable on a per-user basis; and (5) over 100 subsystems including a dozen languages.\n Simplicity at its Core Simplicity was engraved into the gene of Unix since its birth, as the paper states: \u0026ldquo;Perhaps the most important achievement of UNIX is to demonstrate that a powerful operating system for interactive use need not be expensive either in equipment or in human effort\u0026rdquo;.","title":"Sketch on the UNIX Timesharing System"},{"content":"I\u0026rsquo;ve scratched my head for quite a while trying to understand the concept of monad in Haskell. This is a brief summary of monads. I take William Cook\u0026rsquo;s Anatomy of Programming Languages as my reference.\n Definitions of Monads A monad is defined as a computational structure that involves three parts:\n A generic data type \\(m\\) A return function \\(return_m\\) :: \\(t\\rightarrow mt\\) A bind function \\(\\triangleright_mt\\rightarrow (t\\rightarrow ms)\\rightarrow ms\\)  Here the symbol \\(m\\) gives the name of the monad as well as the shape of the computation. We can call the program that uses the monad \\(m\\) as an m-computation. The instantiation of the generic type \\(mt\\) at a particular type \\(t\\) represents n m-computation that produces a value of type \\(t\\). The \\(m\\)-computation indicates that in addition to value \\(t\\), some additional requirements or effects will take place. This is the essence of monads.\nThe definition of the return function states that how values are converted into m-computations. The return will just return the value of type \\(t\\). For example, if we pass in a stateful memory information, return shouldn\u0026rsquo;t modify the actual but only provide a context to which the value lies in. The reason we convert value into m-computation is that if any error occur then return will catch the error without adding additional error checking codes.\nThe bind function \\(\\triangleright_m\\) specifies how computations are combined together. THe general idea is that the computation behavior of \\(A\\triangleright_m F\\) indicates the m-computation \\(A\\) is performed first, the value it produces wil be passed to the function \\(F\\) to create a second m-computation. Because \\(A\\) is a m-computation, if an error happens, the computation will stop and \\(F\\) will not be performed.\nMonads in Haskell In Haskell, we can use Monads using type class. A type class is defined as:\nclass Monad m where (\u0026gt;\u0026gt;=) :: m t -\u0026gt; (t -\u0026gt; m s) -\u0026gt; m s return :: t -\u0026gt; m t For a object of generic type \\(m\\) to be a Monad, it must have those two functions defined. A type class allows us to overload functions according to their type.\nSo why do we need Monads in the first place? If we are given a function \\(func1\\) which takes in an Int value and produces an Int output, we could link the function together to form a chain of computation. If we make a function like this:\nfunc1 :: Int -\u0026gt; (Int, Int) -\u0026gt; (Int, Int) x \u0026amp; func1 = func1 x we could use the output of the function as the input to the same function to produce another value. This process can be repeated and thus form a chain of operation:\n(0, 0) \u0026amp; func1 1 \u0026amp; func1 2 \u0026amp; func1 3 ... However, the function \\(func1\\) could potentially return a Nothing if the given input doesn\u0026rsquo;t meet certain standards (exp. devide by 0). Therefore, \\(func1\\) can modified to:\nfunc1 :: Int -\u0026gt; (Int, Int) -\u0026gt; Maybe (Int, Int) The previous definition of \\(func1\\) says \\(func1\\) takes a (Int, Int) tuple as one input, but now if we feed the output of \\(func1\\) directly to the next \\(func1\\) in the chain, error would occur because \\(func1\\) takes a raw (Int, Int) tuple as the input, but now we have (Int, Int) wrapped in a Maybe context. The \u0026amp; operator is not able to pass the argument with a context to the next func1. Fortunately, we have the bind operator defined.\nIf we look at the definition of the \u0026raquo;= in Monad definition, we see:\n(\u0026gt;\u0026gt;=) :: m t -\u0026gt; (t -\u0026gt; m s) -\u0026gt; m s This means \u0026raquo;= is able to take a value within certain context and map a function that takes the raw value as input to the it. We can simply switch the \u0026amp; operator to \u0026raquo;= such that the chaining would still work:\nreturn (0, 0) \u0026gt;\u0026gt;= func1 1 \u0026gt;\u0026gt;= func1 2 \u0026gt;\u0026gt;= func1 3 ... If an error occurred in one part of the chain (let\u0026rsquo;s assume one computation yields Nothing). Then the Nothing value will be propagated to the next function, whill will automatically generate an error, or Nothing. Otherwise we would have written error checking code at the end of each single computation to check their output.\nIn short, \u0026raquo;= is just a way to chain functions with parametric polymorphism together.\nHaskell do Notation Using the do notation can simply the use of bind operator. The basic pattern of do notation is:\ndo x \u0026lt;- e1 e2 which is equivalent to:\ne1 \u0026gt;\u0026gt;= (\\lambda x.e2) The \u0026lt;- notation simply indicates \\(x\\) is bind to the value the computation generates. In other words, \\(x\\) doesn\u0026rsquo;t lie in a context. if \\(e1\\) returns Nothing, \\(x\\) is not bind to anything. It\u0026rsquo;s important to remember that do expressions are just different syntax for chaining monadic values.\nFor a more detailed explaination of Monads, I found A Fistful of Monads to be extremely helpful in terms of clarifying the concept.\n","permalink":"https://www.bodunhu.com/blog/posts/monad/","summary":"I\u0026rsquo;ve scratched my head for quite a while trying to understand the concept of monad in Haskell. This is a brief summary of monads. I take William Cook\u0026rsquo;s Anatomy of Programming Languages as my reference.\n Definitions of Monads A monad is defined as a computational structure that involves three parts:\n A generic data type \\(m\\) A return function \\(return_m\\) :: \\(t\\rightarrow mt\\) A bind function \\(\\triangleright_mt\\rightarrow (t\\rightarrow ms)\\rightarrow ms\\)  Here the symbol \\(m\\) gives the name of the monad as well as the shape of the computation.","title":"Monads in Haskell"},{"content":"Unitary matrices and the Singular Value Decomposition (SVD) are two important concepts in linear algebra. In order to fully understand these concepts, we will need to first discuss orthogonality. Most materials are convered in Advanced Linear Algebra: Foundations to Frontiers taught by professor Robert van de Geijn. This is a brief summary over the important concepts covered in Chapter 2.\n Components in the direction of a vector By Pythagorean theorem, we know that \\(b = \\chi a + c\\) where \\(a\\) is a unit vector orthogonal to \\(c\\) and \\(\\chi\\) is a scaler. Then we have\n$$a^T (b-\\chi a) = 0$$\nSolving it gives us \\(\\chi = \\frac{a^T b}{a^T a}\\). We have \\(\\frac{a^T b}{a^T a}a = \\frac{a a^T}{a^T a}b\\). And \\(\\frac{a a^T}{a^T a}\\) can map vector \\(b\\) in the direction of \\(a\\). The orthogonal component of \\(a\\) can thus be calculated as \\(I-\\frac{a a^T}{a^T a}\\).\nThe linear transformation can be simplified by letting \\(\\left\\lVert a\\right\\rVert_{2}=1\\) because this will render \\(a^T a = 1\\).\nUnitary Matrix A matrix \\(U\\) is said to unitary matrix is if \\(U\\) is a square matrix and satifies \\(U^H U= I\\).\nIn addition, unitary matrix has some nice properties. First, the product of a sequence of unitary matrix is also unitary matrix. This can be proven by first explore the product of \\((U_0 U_1)^H (U_0 U_1)= I\\), showing \\(U_0 U_1\\) is a unitary matrix, and then perform induction.\nUnitary matrix also preserves length. This is done by showing \\(\\left\\lVert Ux \\right\\rVert _2^2 = (Ux)^H (Ux) = x^H x= \\left\\lVert x \\right\\rVert _2^2\\).\nChange of orthonormal basis We mentioned we can map a vector \\(x\\) another vector in the same direction as vector \\(a\\). Now we extend it to express a vector \\(x\\) using a set of orthonormal basis \\(U\\).\nWe know that \\(x = Ix= UU^Tx=U(U^Tx)=u_0^Hxu_0+\u0026hellip;+u_{m-1}^Hxu_{m-1}\\). We notice that \\(u_0^Hx\\) is a scalor so we can write then equation as \\(U(U^Tx)=a_0u_0+\u0026hellip;+a_{m-1}u_{m-1}\\). We successfully expressed the vector \\(x\\) based on the orthonormal basis.\nTODO ","permalink":"https://www.bodunhu.com/blog/posts/svd/","summary":"Unitary matrices and the Singular Value Decomposition (SVD) are two important concepts in linear algebra. In order to fully understand these concepts, we will need to first discuss orthogonality. Most materials are convered in Advanced Linear Algebra: Foundations to Frontiers taught by professor Robert van de Geijn. This is a brief summary over the important concepts covered in Chapter 2.\n Components in the direction of a vector By Pythagorean theorem, we know that \\(b = \\chi a + c\\) where \\(a\\) is a unit vector orthogonal to \\(c\\) and \\(\\chi\\) is a scaler.","title":"Singular Value Decomposition"},{"content":"This post is meant to discuss the probabilistic clock synchronization technique. The main goal of this technique is to bound the difference between systems by setting up an upper bound. In short, \\(|P(t)-Q(t)|\\leq \\varepsilon\\). We will discuss what these symbols represent later.\n Perfect Synchronization The motivation behind this technique is that synchronization always involves overheads. In a perfect environment where network delay and request processing time are both 0, the clocks can be synchronized with ease. A slave P will send \u0026ldquo;Time = ?\u0026rdquo; at global time t to master Q and master Q replies \u0026ldquo;Time = Q(t)\u0026rdquo; instantaneously at global time t . Then P will adjust its clock P(t) according to Q(t). However, such case only exists in imagination.\nAmortization Suppose the difference between the clock of P and Q is \\(\\Delta\\) at synchronization, our goal is to adjust P\u0026rsquo;s logical clock C(t) to mitigate the difference. The adjustment is simple:\n\\[C(t)=H(t)+A(t)\\]\nHere C(t) is P\u0026rsquo;s logical clock, H(t) is P\u0026rsquo;s hardware clock, and A(t) is the adjustment function(can also be A(H(t))).\nA naive method will be simply subtract or add \\(\\Delta\\) to C(t) to mitigate the difference. However, it will create a discontinuity in P\u0026rsquo;s clock, which may disrupt systems services. For example, if \\(\\Delta = 2\\) seconds,the logical clock will instantly jump ahead 2 seconds and a stopwatch will skip one second.\nSo the adjustment function is as follows:\n\\[A(t)=m\\cdot H(t)+N\\]\nNow the logical clock can be derived as follows:\n\\[C(t)=(1+m)\\cdot H(t)+N\\]\nThis process is called amortization.\nHowever, how do we know the value for m and N? Let\u0026rsquo;s take a look at the time when amortization process starts, the logical time of P at this moment is:\n\\[L=(1+m)\\cdot H+N \\qquad (1)\\]\nAt the end of the amortization (lasts for time period \\(\\alpha\\)) we have reached \\(M=H+\\alpha\\) . Here M is the master logical clock sent by master Q. So at the end of the amortization, the slave P should be able to catch up with its master\u0026rsquo;s logical clock after \\(\\alpha\\) period of time. Therefore, we have:\n\\[M+\\alpha = (1+m)(H+\\alpha)+N \\qquad (2)\\]\nSolving (1) and (2) together, we now get:\n\\[m = \\frac{M-L}{\\alpha}\\]\n\\[N = L - (1+m)H\\]\nThus, at the end of amortization at time t where \\(t \u0026gt; H+\\alpha\\) , we would want the following to be true:\n\\[C(t)=C(H+\\alpha)+(H(t)-H(H+\\alpha))=H(t)+M-H\\]\nHere is a question, why is N required in this case. Couldn\u0026rsquo;t we simply use m to amortize the time difference? Here\u0026rsquo;s my interpretation(feel free to pin me if you have something else in mind): if N is set to be 0, then at the beginning of amortization, we would have:\n\\[L=(1+m)H\\]\nTherefore, \\(m = \\frac{L-H}{H}\\) . Now, m is settled by L and H. Compared to \\(m=\\frac{M-L}{\\alpha}\\) , we can see that now m is a constant and not determined by the value of \\(\\alpha\\) . We lost control of the amortization rate m, which is not desirable.\nGeneral Case We now return to the general case where network delay and processing time are both present. The sitution is represented below:\nLooking at this graph, we can see slave P takes 2d real time to for a round-trip. Let\u0026rsquo;s also assume that 2D is the round-trip delay measured by P\u0026rsquo;s clock between sending and receiving. Then we can bound the clock time 2D based on the drift rate \\(\\rho\\) of the clock:\n\\[2d(1-\\rho)\\leq 2D \\leq2d(1+\\rho)\\]\nIgnoring higher order terms of \\(\\rho\\), we now have \\(2d\\leq(1+\\rho)2D\\).\nWhen looking at the graph above, one thing to notice is we are not sure of the time \\(\\alpha\\) and \\(\\beta\\). However, if we are going to pick one, \\(\\beta\\) will be more important than \\(\\alpha\\). This is because if we know the value of \\(\\beta\\), then we know the lower bound of the round-trip delay. Here we assume min is the minimum amount of time required for network transfer, \\(\\beta\\) will be the time master Q spends between processing the request and responds the result back to P.\nNow we\u0026rsquo;ve narrowed down our focus to \\(min+\\beta\\). The time interval between \\(Q(t)=T\\) and the arrival of \u0026lsquo;\u0026lsquo;Time=T\u0026rsquo;\u0026rsquo; at P will be at least \\(min(1-\\rho)\\). This is based on \\(\\beta=0\\) and clock drift rate.\nThe upper bound of the interval will be \\((min+\\beta)(1+\\rho)\\), assuming no time is wasted for \\(Q\\) to wait until it starts processing the request from P. The time required will be \\(min+\\beta\\) and we need to take Q\u0026rsquo;s drift rate \\(\\rho\\) into account. We can also see that the total round-trip real time is \\(2d=2min+\\alpha+\\beta\\). Thus we get:\n\\[\\beta=2d-2min-\\alpha \\leq 2d-2min\\]\nWith this equation, we can see that the upper bound measured from Q(t)=T is also bounded. Thus, we have:\n\\[ \\begin{eqnarray} (min+\\beta)(1+\\rho) \u0026amp;\\leq\u0026amp; (min+2d-2min)(1+\\rho) \\nonumber \\newline \u0026amp;=\u0026amp; (2d-min)(1+\\rho) \\nonumber \\newline \u0026amp;=\u0026amp;(1+\\rho)2d-min(1+\\rho) \\nonumber \\newline \u0026amp;\\leq\u0026amp;(1+\\rho)2D(1+\\rho)-min(1+\\rho) \\nonumber \\newline \u0026amp;=\u0026amp;(1+2\\rho +\\rho^2)2D-min(1+\\rho) \\nonumber \\newline \u0026amp;\\approx\u0026amp;(1+2\\rho )2D-min(1+\\rho) \\nonumber \\end{eqnarray} \\]\nNow we can see that master Q\u0026rsquo;s clock time when P receives the response is bounded in the interval \\([T+min(1-\\rho), T+2D(1+2\\rho )-min(1+\\rho)]\\). The take away here is that we can\u0026rsquo;t use real time t in a distributed system because it\u0026rsquo;s merely an abstract concept since all systems in a network essentially rely on their own clock time. We need to find the relationship between T and master\u0026rsquo;s clock cycle because P will rely on T, not real time t.\n","permalink":"https://www.bodunhu.com/blog/posts/probabilistic_clock_synchronization/","summary":"This post is meant to discuss the probabilistic clock synchronization technique. The main goal of this technique is to bound the difference between systems by setting up an upper bound. In short, \\(|P(t)-Q(t)|\\leq \\varepsilon\\). We will discuss what these symbols represent later.\n Perfect Synchronization The motivation behind this technique is that synchronization always involves overheads. In a perfect environment where network delay and request processing time are both 0, the clocks can be synchronized with ease.","title":"Understanding Probabilistic Clock Synchronization"},{"content":"I was recently trying to put my research paper draft on ArXiv. I thought it would be as simple as submitting the pdf file, which should take approximately less than ten minutes. I was wrong. It took several hours to figure what was going on. I included some tips here to prevent mistakes I made from happening again.\n The first mistake I made was assuming a single submission of pdf file would be sufficient. ArXiv apparently has mechanisms detecting whether the submitted pdf file is generated using Tex/Latex. According to ArXiv:\n a PDF file created from a TeX/LaTeX file will be rejected. There are good reasons why arXiv insists on TeX/LaTeX source if it is available. arXiv produces PDF automatically from all TeX submitted source. For information on viewing the PDF provided by arXiv, see our PDF browsing help.\n So, the first thing I came up with was to somehow make the pdf appearing \u0026ldquo;anonymous\u0026rdquo; to ArXiv. The were several methods but none of them appear to be practical. If you are interested there is a link to some methods that might be useful. pdfprivacy is package used to remove or suppress pdf meta-data and it sounds promising but I haven\u0026rsquo;t tried yet.\nSo the only option left was to follow the restriction described. It was confusing at the beginning because the everything worked like a charm on Overleaf but it completely fell apart when I tried to compile the sources locally. I was under the impression that if it worked on Overleaf i should work everywhere else, which cause many hours of searching for potential problems related to local environment.\nAfter hours of frustration, it started to appear that there was nothing wrong with my local environment. The pdf produced by Overleaf was only \u0026ldquo;appearing\u0026rdquo; correct. There were several syntax issue in my .bib file, mostly caused by careless copy-and-paste and duplicate records. Overleaf simple suppressed some of those errors, which leads me to think everything was fine.\nThere were also error messages popping up during compilation. Most of them are related to undefined references. Something like:\nWarning--empty journal in article\nThe problem was that bibliographic information obtained from Google Scholar night include serious mistakes. The warning message was telling that entries of type @article require a non-empty journal field. For example, the entry could look like:\n@article{article, title={Something Cool}, author={Somebody}, year={2019}, publisher={IET} } The four required fields for entries of type @article are author, title, journal, and year. This is why the warning message showed up. But it doesn\u0026rsquo;t really affect the compilation on ArXiv.\nWhen I finally compiled all sources locally with success, I immediately moved all source on ArXiv hoping it would finally work. It didn\u0026rsquo;t.\n! LaTeX Error: File 'shoc.pdf' not found.\nI had no idea why this occurred. All sources I used to compile were uploaded to ArXiv so there were no reasons for it to fail. More surprisingly, the references only failed for my .eps files but not .png files. According to ArXiv there are several reasons why PostScript (PS/EPS) figures might fail on ArXiv. Due to the error message, it appears the system is trying to find a file called shoc.pdf to insert into the main pdf but somehow couldn\u0026rsquo;t locate the file.\nThe solution was to upload the pdf files produced locally to ArXiv. However, the locally generated files have slightly different name. All files names are modified to \u0026ldquo;name-eps-convert-to.pdf\u0026rdquo;. What a hassle!\nOverall, uploading to ArXiv was not the most pleasant experience. Latex\u0026rsquo;s compilation system is the one to blame.\n","permalink":"https://www.bodunhu.com/blog/posts/arxivsubmition/","summary":"I was recently trying to put my research paper draft on ArXiv. I thought it would be as simple as submitting the pdf file, which should take approximately less than ten minutes. I was wrong. It took several hours to figure what was going on. I included some tips here to prevent mistakes I made from happening again.\n The first mistake I made was assuming a single submission of pdf file would be sufficient.","title":"How to Put Papers on ArXiv"},{"content":"The memory management has been mentioned numerous times and still remains huge topic. virtual vs. physical memory, physical frame allocation, MMUs, page faults, address space layout, and demand paging and swapping are familiar terms for every undergrad in college.\nIn monolithic kernels such as Linux, much of the functionality is handled in kernel. However, there are OSes that push these functionalities to user space such as Barrelfish. Many concept here will thus be borrowed from the Barrelfish OS. I will also borrow some materials from the main pdf from Barrelfish course materials provided by Professor Simon Peter.\n Memory Management in General Microkernels like L4, Mach, Chorus, and Spring trapped page faults in the kernel but then reflected them up to other processes which carried out the actual page fault handling. This was done on a per-region basis, so each area of virtual memory was associated with some paging server. Memory objects could be shared between different processes and mapped differently in different address spaces.\nSuch abstraction means that what happens when a page fault happens is entirely dependent on the code in the user-level pager. This design is highly extensible since it\u0026rsquo;s all user code and thus isolated, which means that if a user-level pager crashes, there\u0026rsquo;s a good chance the rest of the OS can continue quite happily since much of the functionality is moved away from the kernel.\nHowever, moving functionality out of the kernel an important question: if user-space processes can manipulate virtual address spaces, how can we make sure that one user\u0026rsquo;s program can\u0026rsquo;t manipulate another address space and memory? Here we will introduce the concept of capabilities.\nCapabilities Capabilities are introduced to solve the access control problem in operating systems. Access control is the problem of specifying, and enforcing, which subjects (or principals) can perform particular actions on particular objects in an operating system.\nThe Barrelfish documentation does a good job illustrating capabilities: abstractly, access control can be thought of as a matrix, which represents all possible combinations of operations in the system. Each row of the matrix represents a different subject, and each column represents a different object. Each entry in the matrix contains a list of permissible actions.\nThus, we have two targets to emphasis: the subject and the object. The ACL(access control list) focuses on the object being operated on.\nA good example will be whenever you enter ls -a in a Linux terminal, you will get list of entries specifies the attributes of a file. Here the attributes represent how a object (in this case, a file) may be accessed.\nOn the other hand, a capability can be thought of as a \u0026ldquo;key\u0026rdquo; or \u0026ldquo;licence\u0026rdquo;. It is an unforgettable token which grants authority. Possession of a capability for an object gives the holder the right to perform certain operations on the object.\nA good example will be the file descriptor in Linux. A file is accessed through its file descriptor. Here the file descriptor serves as the \u0026ldquo;key\u0026rdquo; to gain access to the file itself. Capabilities provide fine-grained access control: it is easy to provide access to specific subjects, and it is easy to delegate permissions to others in a controlled manner.\nNote that to be correct, any capability representation must protect capabilities against forgery. Capabilities can be implemented in various ways such as tagged capabilities, sparse capabilities, or partitioned capabilities. In Barrelfish we used the partitioned capabilities.\nIn partitioned capabilities, the kernel ensures that memory used to store capabilities is always separated from that used by user processes to store data and code, for example by using the MMU or ensuring that capability memory is only accessible in kernel mode. The OS maintains the list of capabilities each user principal holds (the clist), and explicitly validates access when performing any privileged operation. Thus, whenever the user accesses memory, the operation can only be done through the resources' corresponding capability. For example, one can map a page frame in the page table page through functions calls with only capabilities.\nCaprefa-\u0026gt;install(Caprefb, slot, flags) Capabilities in Barrelfish According to Barrelfish documentation, all memory in Barrelfish (and some other system resources which do not occupy memory) is described using capabilities. Capabilities are typed, and capabilities can be retyped by users holding them according to certain rules and restrictions. The official documentation has very good explanation on the capability management in Barrelfish. Here is the permissible types for the retype invocation capability retyping:\nImage source\nCapabilities referring to memory regions. Capabilities can also be split, resulting 2 new capabilities of the same type, one for each half of the region. Some of the more important capability types in Barrelfish are shown in figure below. The picture is from the Barrelfish manual provided in CS378 Mutlicore class by Simon Peter:\nAllocation and management of physical memory is achieved by retyping and splitting operations on capabilities. For most kernels, the implementation is to constantly allocate and deallocate memory for a wide variety of purposes, much as any large C program relies heavily on malloc and free.\nThe problem is what the kernel should do when this runs out. The current solution in Linux is little more than \u0026ldquo;kill a random process and reclaim its memory\u0026rdquo;, which can be a problem for system stability. In Barrelfish, all kernel objects are actually allocated by user programs. If a user process wants to create another process (or dispatcher in Barrelfish parlance), it has to get a capability to a DRAM area of the right size, retype this capability to type Dispatcher, and hand this to the kernel. This will be covered in later posts. To access different types of memory resources, the corresponding capability has to be retyped to the right type.\nMore On Implementation In Barrelfish, every capability resides in a slot in a CNode, so a pair (CNode, slot) would identify a capability. It is important to point out that the CNode is another capability itself. Each process in Barrelfish has a CSpace which is structured as a two-level table. So there are actually two different CNode capability types - one for the first level of the table, and one for the second. Every process has, within its \u0026ldquo;dispatcher control block\u0026rdquo;, a pointer to the top-level or root CNode which the kernel can traverse.\nA capability reference in Barrelfish is very similar to VA: the first few bits can represent an index into the first level L1CNode, while the next few bits refer to a slot in a CNode referred to by the capability in the L1CNode slot. Here is a picture from the main pdf showing how the the CSpace is represented in Barrelfish:\nThoughts on Design Decisions Even though it is pretty straight forward to understand the CSpace structure, the actual implementation is a lot more complicated than that. Since the CSpace is not directly accessible by user space program, there are additional data structures used to keep track of available memory resources.\nIn our implementation, the user process keeps a doubly linked list of struct mmnode to indicate the memory available for allocation. Each element in the free list tracks the information corresponding to one capability. However, there is a big problem with this seemingly simple implementation. Every time we allocate a practical memory space from the memory region, a new capability is created while the old capability still remain in the physical memory pointing to a memory range before the allocation happens. Therefore, the old capability would cover extra memory spaces that are already allocated and managed by other capabilities.\nTo solve this problem, we maintain the allocation information in the struct mmnode each time an allocation occurs. If a capability covering physical address space from 0 to 100 is requested for 20 units of memory space, then the memory available for the next allocation would be from 20 to 100 even though the capability itself still manages 0 to 100. By restricting subsequent accesses only to the new memory range, the old capability can still be kept around and used later for retyping.\nAnother Problem emerges when we try to free a memory. Since everything is managed by capabilities, freeing a piece of memory also involves managing the capability responsible for the memory. So an intuitive thought could be whenever a memory space is freed, the corresponding capability is merged back to a piece of memory adjacent to it, managed by a different capability.\nHowever, since capabilities can not be merged, an alternative choice would be to simple destroy it during free. However, this is even a bigger problem in Barrelfish.\nImagine the scenario where capability A is partially allocate from memory space 30 to 100. Later on another memory is freed and that piece of memory is managed by capability with base 100 and size 20, so the memory range covers 100 to 120, which indicates these the two capability could be \u0026ldquo;merged\u0026rdquo;.\nIn this case, if the first capability is destroyed, all children of the first capability will also be destroyed, thus the already allocated memory from 0 to 20 will be thrown away, which is not desired. If the second capability is destroyed, the first one will also be destroyed to create a new capability covering 20 to 120, which will still results in the destruction of capability A.\nOur assumption here is that the parent or root capability is never destroyed when added to the free list. Whenever a capability needs to be freed, the memory manager is responsible to make sure the capability is only merged with another capability from the same parent capability.\nThis is done by creating another list of nodes that tracks all parent capabilities. It is only added when the memory manager adds new capabilities to the free list. After the user initializes free, the memory manager actually creates a new free struct mmnode first, then it find the node\u0026rsquo;s parent node, copying the parent\u0026rsquo;s capability and attributes to the newly created node with updated offset to indicate that the memory hasn\u0026rsquo;t been freed yet.\nAfter that, the memory manager insert the node into the free list. If the memory manager finds out that there are capabilities adjacent to the just-added node, then we simply need to update the attributes of the corresponding mmnode to indicate that merging succeeds. The old mmnode is simply thrown away.\nThe advantage of this implementation is that root or parent capabilities are kept around and the next retype will be fairly simple. The implementation is also very straightforward.\nThere is of course more efficient solution than a linked list. For example, Linux uses both linked list and red-black tree to store thread information. The redundant data structures can be used in different scenarios when appropriate. However, we only use this simplified version to prove our concepts. Optimizations vary but the general concept still works pretty well.\n","permalink":"https://www.bodunhu.com/blog/posts/operatingsystemmemorymanagement/","summary":"The memory management has been mentioned numerous times and still remains huge topic. virtual vs. physical memory, physical frame allocation, MMUs, page faults, address space layout, and demand paging and swapping are familiar terms for every undergrad in college.\nIn monolithic kernels such as Linux, much of the functionality is handled in kernel. However, there are OSes that push these functionalities to user space such as Barrelfish. Many concept here will thus be borrowed from the Barrelfish OS.","title":"A Little Review on Barrelfish Memory Management "},{"content":"Memory access efï¬ciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program\u0026rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation. Turing architecture might have different results, but the method we used here can be applied as well with slight modification. The method we use in this guide is inspired by the research paper: Dissecting GPU Memory Hierarchy through Microbenchmarking. Here we will explain how P-Chase works and walk through a small example.\n Memory Hierarchy Overview GPU memory hierarchy is different compared to CPU memory hierarchy. Using the terminologies of CUDA, GPU memory space can be categorized in these groups: register, constant memory, shared memory, texture memory, local memory, and global memory. Each different memory space have its own properties. Since we are interested the cache systems, here is a picture demonstrating the memory hierarchy of a NVIDIA GPU:\nImage source\nThe characteristics of each memory space can be found in NVIDIA CUDA C Programming Guide . Here we will focus on some target memory space we are interested in. The paper lists some properties of our target memory space:\n   Memory Type Cached Scope     Global R/W Yes All Threads   Shared R/W N/A Thread Blocks   Texture R Yes All Threads    Even though the paper targets Fermi, Kepler and Maxwell generations of GPU, the properties of the table still holds for Pascal GPU and possibly Turing as well. The cached global/texture memory uses a two-level caching system. The L1 cache is located in each stream multiprocessor (SM), while the L2 cache is off-chip and shared among all SMs. It is unified for instruction, data and page table access. According to CUDA documentation, like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. This function previously was served by the separate L1 cache in Fermi and Kepler. Page table is used by GPU to map virtual addresses to physical addresses, and is usually stored in the global memory. The page table is cached in TLB to reduce memory access latency. Once a thread cannot ï¬nd the page entry in the TLB, it would access the global memory to search in the page table, which introduced significant memory access latency. The GPU-specific shared memory is located in the SMs. On the Fermi and Kepler devices, it shares memory space with the L1 data cache. On Maxwell and Pascal devices, it has a dedicated space, since the functionality of the L1 and texture caches have been merged. One thing to note here is that shared memory is accessed by the thread blocks. Thread-blocks remain limited to 48 KB of shared memory in Pascal. Therefore, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block. This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM.\nHowever, we should be careful that by default, GP100 caches global loads in the L1/Texture cache. In contrast, GP104 follows Kepler and Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler. As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to nvcc at compile time. Even though both GP100 and GP104 belongs to Pascal family, we only focus on GP100 here because that\u0026rsquo;s the GPU we use. Another thing to notice is that unlike Maxwell but similar to Kepler, Pascal caches thread-local memory in the L1 cache. This can mitigate the cost of register spills compared to Maxwell. To illustrate our point, we checked both cudaDevAttrGlobalL1CacheSupported and cudaDevAttrLocalL1CacheSupported on Tesla P100 and GTX 1080 and find both attributes to be 1.\nIn addition to the L2 data cache, global memory data that is read-only for the entire lifetime of a kernel can be cached in the read-only data cache with a compute capability of 3.5 or above. We will also explore the size of this read-only cache using __ldg() intrinsic.\nP-Chase Most existing GPU microbenchmark studies on cache architecture assume a classical set-associative cache model with the least recently used (LRU) replacement policy, the same as the conventional CPU cache. So here we will use this assumption and proceed with our experiments. Here are some notations we will use throughout this post.\n   Notation Description Notation Description     C Cache Size N array size   b cache line size s stride size   a cache associativity k iterations   T number of cache set r cache miss rate    Under our assumptions, data is loaded from main memory to lower cache in the basic unit of a cache line. The number of words in a cache line is referred to as the line size (b). For the LRU set-associative cache, the cache memory is divided into T cache sets, each of which consists of \\(a\\) cache lines. It is essential to have these three assumptions using this kind of cache model:\n  Assumption 1 All cache sets have the same size. The cache parameter should satisfy \\(T \\cdot a \\cdot b = C\\).\n  Assumption 2 In the memory address, the bits representing the cache set are immediately followed by the bits representing the offset.\n  Assumption 3 Cache replacement policy should be LRU.\n  We will later see why these assumptions are essential as we proceed with the experiment. We won\u0026rsquo;t go through how P-Chase work exactly. To find more information, this paper does a good job illustrating how P-Chase work. The takeaway is, we need to brute force an array with one element more than a cache can hold so that cache miss will start to occur periodically whereas such array with less or equal elements to the cache capacity will always result in cache hit and thus no access overhead will be introduced after all data is loaded into the cache. This is the algorithm the paper proposed and we will use it to do the experiment:\n__global__ void KernelFunction ( . . . ) { //declare shared memory space  __shared__ unsigned int s tvalue [ ] ; __shared__ unsigned int s index [ ] ; preheat the data ; // implementation varies  for (it = 0; it \u0026lt; iter ; it++) { start_time=clock(); j = my_array[j]; //store the array index  // This following line is essential because due to  // instruction-level parallelism (ILP), function clock() may  // overlap with its previous instruction and even return before  // the previous instruction finishes. For example,  // end_time=clock() can return before j = my_array[j] returns.  // adding s_index [it]= j since it have data dependency on the  // previous line. Thus the memory access will be over before  // end_time=clock() started.  s_index [it]= j; end_time=clock(); //store the access latency  s_tvalue[it]= end_timeâˆ’start_time ; } } The steps is the same as the paper proposes, so here we show the paper\u0026rsquo;s method:\n  Determine cache size C . We set s to 1. We then initialize N with a small value and increase it gradually until the ï¬rst cache miss appears. C equals the maximum N where all memory accesses are cache hits.\n  Determine cache line size b. We set s to 1. We begin with N = C + 1 and increase N gradually again. When N \u0026lt; C + b + 1, the numbers of cache misses are close. When N is increased to C + b + 1, there is a sudden increase on the number of cache misses, despite that we only increase N by 1. Accordingly we can ï¬nd b. Based on the memory access patterns, we can also have a general idea on the cache replacement policy.\n  Determine number of cache sets T . We set s to b. We then start with N = C and increase N at the granularity of b. Every increment causes cache misses of a new cache set. When N \u0026gt; C + (T âˆ’ 1)b, all cache sets are missed. We can then deduce T from cache miss patterns accordingly.\n  Determine cache replacement policy. As mentioned before, if the cache replacement policy is LRU, then the memory access process should be periodic and all the cache ways in the cache set are missed. If memory access process is aperiodic, then the replacement policy cannot be LRU. Under this circumstance, we set N = C + b, s = b with a considerable large k (k \u0026raquo; N/s) so that we can traverse the array multiple times. All cache misses are from one cache set. Every cache miss is caused by its former cache replacement because we overï¬‚ow the cache by only one cache line. We have the accessed data indices thus we can reproduce the full memory access process and ï¬nd how the cache lines are updated.\n  Texture L1 Cache and Read-only Data Cache When use the code with increased our own data preheat implementation because the texture L1 cache can potentially be greater than the shared memory. The original code uses the first iteration of the loop in the algorithm as a way to preheat data:\nconst int it = 6144 // texture L1 may hold more elements,  // So the first iteration may not cold  // hit all elements, some cold hits can  // be moved to the second iteration,  // causing confusion for (int cnt=0; cnt \u0026lt; it; cnt++) { start=clock(); j=tex1Dfetch(tex_ref, j); s_value[cnt] = j; end=clock();\ts_tvalue[cnt] = (end -start); } However, if texture L1 cache is greater than the shared memory allowed for each thread block, then some reads in the second loop will trigger cache misses. But such misses are in fact cold misses, not misses caused after the texture L1 cache is completely filled up. One solution is increase iteration to a much larger number so that the first iteration will always fill up the texture L1 cache. Note that if you move the data preheat out such as\nfor (int cnt=0; cnt \u0026lt; it; cnt++) { tmp=tex1Dfetch(tex_ref, tmp); } The compiler can optimize this whole step out and thus nothing actually gets executed.\nAfter we run the modified code, the result shows that the we the cache missed starts when we set our array size to 6145, indicating the texture L1 cache can hold 6144 ints, which is equivalent to 24 kb. We also notice that each miss is followed by 7 consecutive hits. This means the cache line size is 8 words(b = 32 bytes). The structure of the L1 TLB is shown below, notice there are 192 lines in each set:\n   Set1 Set2 Set3 Set4     1-8 33-40 65-72 97-104   9-16 41-48 \u0026hellip; \u0026hellip;   17-24 46-56 \u0026hellip; \u0026hellip;   25-32 57-64 89-96 121-128   129-136      \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;   2969-2976 3001-3008 3033-3040 3065-3072    According to CUDA documentation, GK110 adds the ability for read-only data in global memory to be loaded through the same cache used by the texture pipeline via a standard pointer without the need to bind a texture beforehand and without the sizing limitations of standard textures. The read-only data cache is loaded by calling __ldg(const restricted * address). We modified the code used to test texture L1 cache. The basic logic remains the same. When the arrays size is set to 6144 integers no cache misses occur with stride set as 32 (s=32 bytes). As soon as we increased one more element in the array cache misses start occurring. This shows the read-only cache is 24kb. We then noticed that the misses occur in a group of either 4 or 8. We infer the cache line to be 32 bytes and the replacement policy is LRU, same as Maxwell. We we increase the array to include 6248 elements(6144+323+8, 6144 is the max capacity of the cache, 32 consecutive number in a set, 323 to cause cache miss in set1, set2, and set3, only need to include 8 more to cause cache miss in set4 since s=32bytes), no caches hits occur. Therefore, we infer the caches set number to be 4, each cache line is 32 bytes, and each set contains 192 cache lines, the same as the texture L1 cache. The memory mapping seems arbitrary because the hit and miss patterns didn\u0026rsquo;t follow that of the texture L1 cache.\n","permalink":"https://www.bodunhu.com/blog/posts/gpumemoryhierarchy/","summary":"Memory access efï¬ciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program\u0026rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation.","title":"Pascal GPU memory and cache hierarchy"},{"content":"I was always interested by the name \u0026ldquo;map reduce\u0026rdquo; since two years ago when I first heard this term. But I\u0026rsquo;ve never put any effort to know the concept until Chris mentioned it in class because it will be on the next exam so I figured I\u0026rsquo;d better figure out what is going on before it was too late. Just kidding:) But map reduce does borrows a lot of characteristics from traditional relational databases even though many useful and important features in RDBMS are eliminated from the map reduce system. You can check this long list of roasts on map reduce here.\n But the intention of this post is not about roasting map reduce so if you absolutely resent how map reduce is such a disgrace to RDBMS you are in the wrong place. Essentially, MapReduce is a programming model. Users need to define a map function that processes a key/value pair, producing a set of key/value pairs, then a reduce function will read these intermediate pair, merging pairs with the same intermediate key. It is important to realize the MapReduce is a programming model because it allows the programmers to follow this model without having to worry about the technical details needed to ensure the operations between clusters. In fact, the programming model is very easy to understand. Everything you need is already summarized in the name MapReduce.\nBasically, the computation takes a set of pair/key values are input and output a set of pair/key values. The users write the map function which take an input pair and produce a set of intermediate key/value pairs(we will know why the output in intermediate). The MapReduce library takes all intermediate pairs and group the ones with the same key and pass them to the reduce function. The reduce function is also written by the user. It takes an intermediate key with a set of values corresponding to that key, merging those values in hope to form a smaller set of values. What it means is that the reduce function usually produces zero or just one output value. The intermediate values are supplied to reduce function via an iterator. There might be occasions where the memory doesn\u0026rsquo;t have enough space for all intermediate value and thus some values needs to be pushed to permanent storage.\n","permalink":"https://www.bodunhu.com/blog/posts/mapreduce/","summary":"I was always interested by the name \u0026ldquo;map reduce\u0026rdquo; since two years ago when I first heard this term. But I\u0026rsquo;ve never put any effort to know the concept until Chris mentioned it in class because it will be on the next exam so I figured I\u0026rsquo;d better figure out what is going on before it was too late. Just kidding:) But map reduce does borrows a lot of characteristics from traditional relational databases even though many useful and important features in RDBMS are eliminated from the map reduce system.","title":"Map Reduce"},{"content":"The concept of a worldwide of networks of information was introduced long before the technology used to build the internet. The first workable prototype came in the late 1960s with the creation of ARPANET(The Advanced Research Projects Agency Network). The famous TCP/IP, or Transmission Control Protocol and Internet Protocol, was developed by Robert Kahn and Vinton Cerf in the 1970s. In the 1980s, research by Tim Berners-Lee gave birth to the World Wide Web, linking hypertext documents into an information system, making them accessible from any node on the network (History of Internet). The implementation and the evolution of the internet has improved ever since. Today, for most users, the internet feels like smoke and mirrors since requiring everyone to understand the technical implementation will be way too harsh. However, as software developers, they are much more likely to deal with networks sometime in life. This article is meant to unveil technical details of networks mainly from a programmer\u0026rsquo;s perspective so the focus will be put on the software side.\n The OS View of Networks For the operating system, the network is perceived as an extra device. The Network Interface Controller(NIC), which is a hardward device used to connect the computer to a computer network, is added to the bus.The data can be transferred to/from memory to NIC through two methods: DMA or memory-mapped I/O. DMA refers to direct memory access. The name suggests that the hardware is able to read or write memory without the involvement of CPU. On the other hand, memory-mapped I/O means the CPU can control the hardware to read or write specific memory addresses, which means the CPU is doing the job of writing/reading to/from memory. DMA is usually used for high-bandwidth operations such as disk I/O while memory-mapped I/O is used in low-bandwidth operations like change control bits.\nLayers of Network The OSI (Open Source Interconnection) 7 Layer Model devides network communication into seven layers. Here I will discuss each layer and its corresponding function. Starting from the lowest one:\nLayer 1: This layer is the physical layer which is concerned with the transmission of unconstructed raw bit stream over the physical medium. Thus the protocal data unit(PDU) is bit for this layer.\nLayer 2: This is the data link layer. Its function is in charge of reliably transfer the data frames between two nodes connected by a physical layer. The PDU of this layer is frame.\nLayer 3: This is the network layer. This layer is in charge of structuring and managing a multi-node network. Examples includes addressing, routing, and load control. The PDU for this layer is packet.\nLayer 4: This is the transport layer. It is used to deliver messages error-free, in sequence, and without duplications or losses. The PDU of this layer is segment/datagram(segment for TCP, datagram for UDP).\nLayer 5: This is the session layer. It allows session establishment between process running on different stations. Layer 5 is often OS/Library. From here, the PDU will generalized to data.\nLayer 6: This is the presentation layer. As the name suggests, it formats the data and present it to the application layer. It can be viewed as the translator for the network. It is usually OS/Library.\nLayer 7: The final layer is called the application layer. It serves as the window for the users and appliaction processes to access the network.\nNote the Department of Defense Four-Layer Model has only four catagories: the Network Access Layer(layer 1-2), the Internet Layer(layer 3), the Host-to-Host Layer(layer 4), and the Process Layer(layer 5-7).\nMore on Layer 2 Network There are three types of layer 2 networks: System Area Network(SAN), Local Area Network(LAN), and Wide Area Network(WAN).\n","permalink":"https://www.bodunhu.com/blog/posts/networks/","summary":"The concept of a worldwide of networks of information was introduced long before the technology used to build the internet. The first workable prototype came in the late 1960s with the creation of ARPANET(The Advanced Research Projects Agency Network). The famous TCP/IP, or Transmission Control Protocol and Internet Protocol, was developed by Robert Kahn and Vinton Cerf in the 1970s. In the 1980s, research by Tim Berners-Lee gave birth to the World Wide Web, linking hypertext documents into an information system, making them accessible from any node on the network (History of Internet).","title":"Networks"},{"content":"What exactly is a file system? The general concept is that the file system provides naming organization. It manages the physical disk layout such as picking a block constituting a file, balancing locality with expandability, and managing free space. It can translate from file name and offset to the actual data block. In a nutshell, it is a servant that manages all the dirty details of communicating the data between system and the hardware in an optimal way which you aren\u0026rsquo;t required to understand so you can go on and do other things with your life. So what are the functionalities of file systems? In general, it providses file name organizations such as directories. It can manage disk layout by picking blocks that constitute a file, balancing locality with expandability, and manage free space. It can translate from file name and offset to the actual data block.\n File Let\u0026rsquo;s start from and bottom-to-top pattern to describe file system by first introducing the most fundamental unit: the file itself. So a file is composed of two parts: the metadata and the actual data. The metadata is a file header that holds information about where the data is stored and attributes of the file such as a permission, access time, owner id, size, and so on. One thing to note is that meta data blocks are stored on a location that is known by the OS and thus it can be accessed without having to check another data structure. Then the actual data is the part users actually care about. There are two kinds of blocks (there can be more than these two data blocks but we will only discuss two here), The directory data block which maps file names to file headers, and file data block that contains information we care about.\nDesign File Layout There are several factors we need to take into consideration when designing file layout:\n Support for sequential and random access. Almost every file operation is either sequential or random. Lay out the files on the physical disk. Maintain file location information. This makes sense since we need an agent to keep track all files because we users are too lazy to do that. In Unix most files are small in size so we need to support small files, which means block size can\u0026rsquo;t be too large due to internal fagmentation. Most disk space is consumed by large files so we also need to support large files and accessing them should be effcient as well. I/O operations target both types of files.  Block VS Sector Before we dig deeper into file system design, it\u0026rsquo;s important to note the the block size of file system is different from disk blocks size. According to Practical File System Design, block is the smallest unit writable by a disk or ï¬le system. Everything a ï¬le system does is composed of operations done on blocks. A ï¬le system block is always the same size as or larger (in integer multiples) than the disk block size. Also each blocks consists of consecutive sectors so that sequential access becomes possible. A larger block size increases transfer efficiency also because of sequential access since you don\u0026rsquo;t have to move the head too many times, it may be convenient if the block size matches the machine\u0026rsquo;s page size, this is because we don\u0026rsquo;t have to switch pages assuming the block is bigger than the page size. Many systems allows transfer of many sectors between interrupts.\nAllocation methods Contiguous Allocation  OS maintains an ordered list of free disk blocks. OS allocates a contiguous chunk of free blocks when it creates a file. Placement/allocation policy can be first-fit, best-fit, or worst-fit. File header specifies starting block and length. Pros:  All file data stored contiguously on disk. Simple to implement as bump pointer is a common way of implementation. Best performance for initial write of a file due to locality resulted from contiguous allocaiton.   Cons:  External fragmentation because some allocation for large files are simply impossible, resulting in wasted unallocated space, and hard to grow file in size. Later writes may cause the file to grow which would require it to be copied and moved.    Linked Allocation  Files are stored as a linked list of blocks, in each sector, there\u0026rsquo;s a pointer pointing to the next sector. (This is a hardware implementation, we still use blocks fot later discussion.) The file header keeps a pointer to the first and last sector/block allocated to that file. There are two types of implementations for Linked allocation:  Linked list of disk blocks, data blocks point to other blocks Linked list in a table (FAT file system)   Pros:  Reduce or eliminate external fragmentation since blocks can fit in if there are free blocks available. Easy to grow file just like adding elements into a linked list. Linear access is somewhat efficient(It\u0026rsquo;s linked list, what do you expect? O(1)?).   Cons:  linear random access time in linked list.    FAT File System (File Allocation Table) FAT32 file system is very important file system created by Microsoft. It was introduced the solve the volume problem posed by FAT16. Although named FAT32, only 28 of the 32 bits are actually used and the remaining 4 bits are \u0026ldquo;reserved for future use\u0026rdquo;. As a result, a FAT32 partition has a maximum cluster count of (268,435,455)2^28-1. I found this discription about FAT32 on StackExchange that is useful:\n Although VFAT was a clever system, it did not address the limitations of With the appearance of the FAT32 file system, the maximum number of clusters per partition went increased from 65535 to 268,435,455 (228-1). FAT32 thus allows much bigger partitions (up to 8 terabytes). Although the maximum theoretical size of a FAT32 partition is 8 TB, Microsoft has voluntarily limited it to 32 GB on Windows 9x systems to promote NTFS\n FAT32 is implemented in a completely different way . Unlike FFS in UNIX, each entry in in the MTF merely represents a block of data. Each block is able to point to another block, with multiple entries in the table to represent a file represented multiple blocks. Each file\u0026rsquo;s file number is indicated using the index of the first entry in the MTF. Thus, in order to locate a specific block of a file, we need to search the MTF sequentially.\n","permalink":"https://www.bodunhu.com/blog/posts/filesystem/","summary":"What exactly is a file system? The general concept is that the file system provides naming organization. It manages the physical disk layout such as picking a block constituting a file, balancing locality with expandability, and managing free space. It can translate from file name and offset to the actual data block. In a nutshell, it is a servant that manages all the dirty details of communicating the data between system and the hardware in an optimal way which you aren\u0026rsquo;t required to understand so you can go on and do other things with your life.","title":"File System Design"},{"content":"This chapter is all about disk. Before we start. We won\u0026rsquo;t go deep into the mechanical part of disk operation; rather we will be focusing on general concept related to disk and algorithms to improve disk performance.\n  The Evaluation Criteria Here we are introduing the basic components used to evaluate the performance of disk operation.\nSeek Time This is the time to position the head over the track. Maximum can be going from innermost track to outer most track. It usually ranges from 10ms to over 20 ms. However, the average seek time is usually to seek 1/3 of the way across the disk.\nHead Swtitch Time This is time spent to move from one track on one surface to another track on a different surface. The range is similar to seek time.\nRotation Delay This is the time spend for the sector to spin underneath the head. It varies depends on how fast the disk rotates.\nTransfer Time The time spend to read or write sector as it spins by.\n Transfer time: time to move the bytes from disk to memory Surface transfer time: time to transfer one or more sequential sectors to/from surface after head reads/writes first sector Host transfer time: time to transfer data between host memory and disk buffer  Disk Head Scheduling (Mainly focusing on HDD) Now we\u0026rsquo;ve looked at the basic performance evaluation critiria for HDD, it\u0026rsquo;s reasonable to discuss how to reduce head movement so that the amount of time spent on moving the head from on track to the other will decrease because the disk I/O request for can be stored in a queue. (Note the seek time takes the most amount of time so it\u0026rsquo;s reasonable to reduce it.)\nFIFO This technique is easy to understand, the head will move to the corresponding track based the order of the queue of requests. Since the requested data can be read/writen on random tracks, the performance can be heavily affected.\nSSTF (Shortest Seek Time first) The queue of requests is reordered such that the head will only look for the closest track it can move to and thus ignore the global state of locations of all requests. This is a greedy algorithm and thus can be trapped in local optimal value.\nSCAN/Elevator/LOOK Simply move the head to one direction until the request that is closest to that end of the disk is reached, then reverse the direction of the head and find the rest of the requests.\n Optimization: the head is reset when no more requests exist between the current head position and the approaching edge of the disk (LOOK scheduling)  C-SCAN/C-LOOK (\u0026ldquo;Circular Scan\u0026rdquo; scheduling) Move the head in one direction until an edge of the disk is reached and then reset to the opposite edge. optimization: the head is reset when no more requests exist between the current head position and the approaching edge of the disk (called C-LOOK scheduling).\n Note the only difference between SCAN and C-SCAN is that in C-SCAN, after the head reaches one edge, an optimized jump implemented by the hardware is used to directly move the head to the opposite edge instead of reversing the movement direction.)  Partitioning Disks are partitioned in order to minize the largest seek possible time since each partition is a logically seperate disk. (It\u0026rsquo;s just merely a collection of cylinders.) More information covering partitioning will be covered in file system.\nOther Techiniques to Reduce Overhead To minimize rotational latency and seek time, we can also:\n Make disks smaller (less movement distance) Spin disks faster Schedule disk operations to minimize head movement(we\u0026rsquo;ve just discussed) Lay out data on disk so that related data is on nearby tracks(locality and also less movement) Place commonly used files on disk Block size: (note disk is presented with sector address using logical block address converted by the controller)  Too small: low transfer rate because we need to perform more seeks for same amount of data. Too big: internal fragmentation    SSD The basic advantage of SSD is that it doesn\u0026rsquo;t have moving parts and thus random access is blazingly fast. It\u0026rsquo;s implemented using NAND and is non-volatile.\nBasic NAND Flash Units  The fundemental unit is a page which is 4KB. 128 pages are organized together forming a block of size 512KB. Each block is the unit forming a plane. There are 1024 blocks on one plane and the size of the plane is 512MB.\nOperations:  Read page: fast in terms of nano seconds compared to micro seconds for spinning disk. Write page: can only write to empty page, same as above. Erace block: (ms) Before a page can be written, all bits in the page need to be set to 1. Note the only way to set bits in a page to 1 is to erase the whole block. Read and Write occur in page unit.  ","permalink":"https://www.bodunhu.com/blog/posts/disks/","summary":"This chapter is all about disk. Before we start. We won\u0026rsquo;t go deep into the mechanical part of disk operation; rather we will be focusing on general concept related to disk and algorithms to improve disk performance.\n  The Evaluation Criteria Here we are introduing the basic components used to evaluate the performance of disk operation.\nSeek Time This is the time to position the head over the track. Maximum can be going from innermost track to outer most track.","title":"Disk Introduction"},{"content":"As we can see in the previous post, all allocation algorithms we discussed lead to external fragmentation. As time goes by, external fragmentation is going to get worse and we need solutions for the problem. We can use swap areas to swap out memory onto the disk, or move allocated memory together(a process named memory compaction), leaving empty spaces together. Even these approaches can reduce external fragmentation and allow a higher degree of multiprogramming, they are not perfect. In addition, it is possible to have a single process that is just too big to fit into memory. We are going to discuss methods used to completely eliminate external fragmentation. More than that, we will discuss how to make memory sharing possible and how to allow more processes to execute at once.\n Too big to fit It\u0026rsquo;s easy for us to assume the amount of available memory that can be allocated is not a big problem. It\u0026rsquo;s easy for programmers to assume the available memory resource is almost infinite and thus we rarely care about the situation in which the code we wrote is going to occupy all memory resource. But let\u0026rsquo;s just consider the scenario where we create a program which later create a process that is just too big to fit into memory, what should we do?\nThe natural response would be: just cut them into pieces! This is a technique called overlay: programmers manually cut the program into pieces, or overlays. When the program executes, a overlay manager is created to swap pieces in and out, allowing only necessary pieces in memory at a given time. But tell me, what is the last time you see an user-level application manually cut into \u0026ldquo;pieces\u0026rdquo; by the programmer? Doing things manually is not desired trait of a programmer. Programmers should always be lazy and automate things, or just leave it to someone else!\nPaging I\u0026rsquo;m pretty sure you don\u0026rsquo;t like the idea of overlaying as it requires you to do things manually. That\u0026rsquo;s where paging comes into play. Instead of dividing the program by the programmer, why don\u0026rsquo;t we let the system do the dirty job? Before we start, I\u0026rsquo;m going to throw two questions to you: why can a virtual address space be bigger than the physical memory? How are each piece of a process brought into the memory?\nThe technique to divide a address space into fixed-size pages is called paging. A copy of the address space is stored on the disk. The physical memory is viewed as a series of equal-sized page frames. We will discuss later about how the system choose to load a page into a frame and how to manage pages that are currently in memory.\nSo how do we use virtual addresses with the our recently introduced pages to find a location in memory? As we can see, a virtual address space is divided into pages, each with a fixed number of entries. In order to represent the number of pages and number of entries, we need two variables:\np - page number(pMAX pages)\no - page offset (difference between the byte address to search and the start of the page, MAX indicates the total number of entries in a table)\nVirtual Address calculation: oMAX x p + o (here o is the offset in the last page)\nThe frame size is equal to that of a page. It\u0026rsquo;s easy to understand since we need to put everything stored in a page into the frame, we need them both to be equally sized. Note that since virtual address space can be bigger than physical memory, the number of frames can be smaller than the number of pages, which means the number of bits reserved for frame number can be smaller than the number of bits used to indicate the number of pages.\nsource\nFrom Virtual to Physical: Allocation Policy We\u0026rsquo;ve discussed how that a process\u0026rsquo;s virtual address space can be divided into pages and mapped to frames in physical memory. Here we are going to discuss some policies used to implement the mapping process. I\u0026rsquo;m going to leave three questions to think here as well: why pages are arbitrarily located in physical memory? How do we find them if they are arbitrarily located in physical memory? Why aren\u0026rsquo;t all pages mapped to frames? These questions will become more clear as we progress into further discussion.\nHere\u0026rsquo;s the solution: a page table. Each process has one page table that contains all mapping information for each possible virtual address belonged to that process. Even though we call it table, it\u0026rsquo;s merely a data structure used by a virtual memory system in a computer operating system to store the mapping between virtual addresses and physical addresses. However, the mapping is invisible to the process. The protection mechanism is the same as dynamic relocation we\u0026rsquo;ve discussed before.\nVirtual Address Translation Now we are going through a step-by-step description of address how to translate virtual address to physical address.\n  First, the process will give the CPU a virtual address to translate.\n  Then MMU will split the address into two parts, the page number and the offset.\n  Since the size of a page and a frame are the same, the offset of the virtual address is sent along without no modification to the physical memory.\n  Use page number to find the corresponding entry in the page table.\n  Check if the page exists in physical memory.\n  If the page does exist in physical memory, the frame number is sent along. If the requested page is on the disk, then the corresponding page is moved to memory and frame number is recorded now.\n  Offset is appended to the end of the frame number to get the physical address.\n  So, we\u0026rsquo;ve achieved several goals now by using paging technique:\n  Reduce or even eliminate external fragmentation.\n  Easy to grow processes.\n  Allow more process that is too big to fit into memory to be able to execute.\n  Easy to allocate and deallocate.\n  Share memory is between processes is easy since memory used by difference processes no longer has to be contiguous. Even pages may exist in different position, they can be mapped to the same physical address in memory.\n  More about Page Table One thing to notice is that there\u0026rsquo;s only one page table for each process. The page table is part of the process\u0026rsquo;s state and serves as protection mechanism to prevent processes accessing each other\u0026rsquo;s memory. There\u0026rsquo;re several elements in each page table entry as well:\n  Flags: dirty bit, resident bit, and dirty bit (we will talk about them later). Flag is stored at the beginning of each entry.\n  Frame number: stored in the remaining bits. It tells where the page lives in physical memory.\n  However, page table still has its disadvantages, the most important thing to notice is that we need two memory accesses to implement virtual memory reference, first access is to get the page table entry, the second access is used to get the actual data from memory, if it\u0026rsquo;s present. As we know, memory access is extremly slow and expensive, thus we need something faster.\nTranslation Lookaside Buffer (TLB) Since it\u0026rsquo;s hard to improve the speed from the algorithm side, let\u0026rsquo;s just drop the algorithm for a minute and switch our focus onto the hardware. Here we will discuss how to improve the speed of memory reference by adding a hardware component called TLB. Here are several basic characteristics of TLB:\n  The TLB holds recently used frame/page pairings.\n  It has high hit ratios due to locality.\n  For a TLB hit, translation can be finished in one cycle.\n  So how does TLB help with efficiency? It\u0026rsquo;s actually really simple. The system simultaneously sends the page number to both page table and TLB. If there\u0026rsquo;s TLB hit, then the TLB cache sends the frame number to the memory without having to look into the page table, which avoids the first reference into the memory to find the page table. If there\u0026rsquo;s missing TLB, everything stays the same: look for the page table in memory and update the TLB.\nsource\nProblems with Page Table Now we solved the problems of external fragmentation. It seems paging works like a charm and makes things a lot easier. However, we notice it\u0026rsquo;s still not perfect in terms of space usage:\n  Data structure overhead (The page table can be huge!)\n  Inappropriate page size can lead to internal fragmentation, and less processes to exist in memory in the same(page too big)!\n  Thus we need more complex methods to solve the above issues.\nMulti-level Page Tables The basic concept of multi-level page table is to subdivide page number into n parts(n stands for number levels of pages tables). n is decided by the architecture of the system. Each entry in each page table which is exists each entry points to the corresponding page table in the next level. The last level page table entries hold the bit/frame information associated to the page table entry.\nSO how does it work exactly? First, we have only one first-level page table. We extract the first subdivision of the virtual address, added to the PTBR to get the corresponding entry in the first-level page table. Then we extract the second subdivision of the virtual address, add it to the address of the beginning of the second-level page table which we got from the corresponding first-level page table entry. This process continues until we reach the last-level page table. From the corresponding entry we can get the frame number. The offset is preserved so we just need to append the offset to the frame number and we\u0026rsquo;re done! One reminder is that multi-level page table requires several lookups to eventually find the frame number, so TLB becomes extremely important here in terms of improving performance.\nHow does multi-level page table save space? You\u0026rsquo;re probably still confused why multi-level page table saves space by adding more tables. Don\u0026rsquo;t worry, I will walk you through an example to illustrate the magic behind the scene:)\nAssume a process has a ((2^{10}\\) pages, each PTE occupying 4 bytes (32-bit system). Without multilevel page table, we need \\(2^{20} \\times 4 = 4MB\\) for one page table stored in memory. Even we just need a portion of all pages, we need the whole page table present in memory to find the corresponding frames. Now, if we divide the virtual address into 3 sections with last one being the offset, we have a two-level page table. The first 10 bits are used to index the page table in the first level and the next 10 bits are used to index the page table in the second level. If we only need virtual addresses that have the second 10 bits modified and leave the first 10 bits untouched, then we only need to find one entry in the first-level page table. Since the first-level page table has to be always present in memory, it will consume \\(2^{10} \\times 4=4KB\\) memory space. Now, since we need every entry in a second-level page table pointed by the entry we just found in an entry in the first-level page table, it requires \\(2^{10} \\times 4bytes = 4KB\\) memory. So we only need to use 4 + 4 = 8KB for all memory we need instead of 4MB without multi-level page tables.\nAnother interesting fact is that, even if we need to use all pages of a process, multi-level page table will potentially increase the space needed. Let\u0026rsquo;s take the above example and assume we need every single pages from a process. Then we need to store the first-level page table, which takes $2^{10} \\times 4bytes = 4KB$. Then, for each entry in the first-level page table, there\u0026rsquo;s a corresponding second-level page table, each with the size of \\(2^{10} \\times 4 = 4KB\\). Since the first-level table has \\(2^{10}\\) entries, the total number of second-level page tables is \\(2^{10}\\), each with the size of 4KB, so the total amount of spaces is \\(2^{10} \\times 4kb + 4kb = 4MB + 4KB\\). Then bottom line is: if we need to map every pages to its frames, then the total amount of entries in the last level will be the number of pages regardless of how many levels we use since each page has to have a mapping. Under such case, the total amount of memory used by the last-level page tables will be equivalent to the amount used when we use only one huge page table. The additional space comes from the upper levels, but the previous level will only save the corresponding number of entries. (number of table in the next level).\nsource\n","permalink":"https://www.bodunhu.com/blog/posts/virtualaddressmechanism/","summary":"As we can see in the previous post, all allocation algorithms we discussed lead to external fragmentation. As time goes by, external fragmentation is going to get worse and we need solutions for the problem. We can use swap areas to swap out memory onto the disk, or move allocated memory together(a process named memory compaction), leaving empty spaces together. Even these approaches can reduce external fragmentation and allow a higher degree of multiprogramming, they are not perfect.","title":"Virtual Memory Mechanisms"},{"content":"I love pointers. Pointer is very a useful feature in programming languages like C/C++. I can pass weird hexidecimal numbers to a function and then it will magically locate where the program is in memory. However, all those values we see are merely virtual addresses, a running program\u0026rsquo;s view of memory in system. Any address we can see while programming user-level programs is a virtual address. It is no more than an illusion of where the data is actually laid out in memory. Only the almighty OS knows where the data actually locates in physical memory.\n Three Goals of VM When the OS provides this layer of abstraction to translate virtual address to physical address, we say that the OS is virtualizing memory. In order to achieve virtualization, there\u0026rsquo;re three goals to keep in mind.\nTransparency: The OS will implement transparency in a way that is invisible to the running program. Usually, transparency would suggest a clear understanding of how things work. However, when we are talking about VM, it means the running program is unaware of the fact its memory is translated by the OS(and hardware) behind the scene.\nEfficiency: The OS should do its best to ensure the efficiency of virtualization, in both time and space. Some of the methods used to improved efficiency, including hardware components like translation lookaside buffer(TLB), will be discussed in the following chapter.\nProtection: Being able to protect processes from interfering each other or even the OS is important. When one process performs actions like read and write, the process should be isolated so that it\u0026rsquo;s unable to modify the data of other processes or behave maliciously.\nBasic Concept: Address Space Before we start, there are several terms I will use throughout the discussion so it\u0026rsquo;s better to get familiar with them.\nPhysical address space: it is merely a collection of physical addresses used by the hardware. The value of the address can range from 0 to the MAXsys. The address is actually utilized by the physical memory to fetch the contents inside.\nLogical/Virtual address: It a collection of address a process is able to access (the process is not aware of the actual physical location). It can be bigger than physical address due a technique called paging which will be discussed later.\nSegment: A chunk of memory assigned to the process to use.\nImage source\nHow does an address get generated? Uniprogramming: This is a very simple scenario, there\u0026rsquo;s only one process executing at the moment, its address always starts at 0 and the OS is loaded in a fixed part of the memory. The process executes in contiguous section of memory. The process is able to use the all the available portions of the memory as long as it doesn\u0026rsquo;t touch the OS\u0026rsquo;s property.\nRelocation Same as uniprogramming, the OS locates at the highest location in the memory. The OS allocates a contiguous segment of memory for processes to use. If it doesn\u0026rsquo;t have enough space, it simply wait the process to terminate. The relocation address (also known as base address) is the first physical address a process can use. Limit address is the largest physical address the process can use.\nAlso note there\u0026rsquo;re two types of relocation:\n  static: The address is produced during load time and the process is load into the given location to execute. The OS can\u0026rsquo;t move it as soon as the process is loaded in. Note the static address can be changed in both linking and loading stage. Linking stage might involving library routines and loading stage might increment the base address by the amount of memory used by previous process already present in memory. Note the printed value here is the actual physical address, not a virtual address.\n  dynamic: Physical address is obtained by adding the base register to virtual address. The result has to be less than bound register or an exception will be raised by the processor.\n  Problem with relocation:\nEven though the concept of relocation is easy to understand, it can easily lead to problems with unused memory space. The biggest problem is external fragmentation. When a process finished executing and the memory it occupied is deallocated, it will leave a \u0026ldquo;hole\u0026rdquo; behind and become available for other process to use. The problem is, as the size of the emory of the previously running process can be any random number, the \u0026ldquo;hole\u0026rdquo; it leaves behind may be too small for other processes to fit in. Even if it is big enough for other processes to fit in, they may eventually lead to smaller fragments that are too small for a process to use, which leads to external fragmentation. There\u0026rsquo;s also another problem called internal fragmentation, but it won\u0026rsquo;t be discussed here.\nHow to minimize external fragmentation? As we can see, relocation leaves rooms for external fragmentation, which can be a problem since unused spaces will be a waste. Are there any methods we can use to reduce the external fragmentation to minimize external fragmentation and better utilize the free blocks? There\u0026rsquo;re three policies we will discuss that can be used to achieve our goals. A reminder is that they can\u0026rsquo;t completely eliminate external fragmentation but merely minimize it to a certain level.\nPolicies First-Fit policy Finding the first free block that can hold the requested amount of memory.\nRequirements:\n  All free blocks are tracked in a list and sorted by address.\n  Allocation requires a search throughout the list to find the first free block to fit in the memory.\n  After deallocation, the freed block might need to be merged with other free ones.\n  Pros:\n  very easy to implement.\n  Tend to produces large free block towards the end of the address space.\n  Cons:\n  Allocation is slow.\n  It will eventually lead to fragmentation.\n  Best-Fit policy Finding the smallest free block to allocate the request memory.\nGoal:\n  Avoid fragmenting huge free block.\n  Minimize the size of external fragmentation.\n  Requirements:\n  All free blocks are tracked in a list and sorted by address.\n  Allocation still needs a search in the list to find a suitable block to fit in.\n  After deallocation, the freed block may need to be merged with other free blocks.\n  Pros:\n  Kinda simple to implement\n  Reduce external fragmentation, works well when allocations size gets smaller.\n  Cons:\n  Still leaves room for external fragmentation.\n  When it is deallocated and merged back, the new free block may require to be resorted.\n  Worst-Fit policy Finding the largest free block to allocate request amount of bytes.\nGoal:\n To avoid having too many small free segments(reduce external fragmentation).  Requirement:\n  Free blocks sorted by size\n  Allocation is fast since the first one is always the largest one.\n  After dealloaction, it needs to check if the new free block needs to be merged back and resort the list.\n  Pros:\n Works great if all allocations are of medium size.  cons:\n  Still external fragmentation\n  Deallocation is slow(need to resort and merge)\n  Tends to break down large free blocks, which can lead to failure to allocate large blocks.\n  Technique to further reduce and eliminate external fragmentation will be discussed later.\nDynamic Relocation Advantages   Processes can move during execution.\n  Processes can grow over time.\n  It\u0026rsquo;s easy to provide protection since we only need two registers.\n  Fast and simple\n  Disadvantages   Allocation is contiguous.\n  Sharing memory is hard since there\u0026rsquo;re no way to set base and bound register to be the same for more than one processes.\n  Multiprogramming is limited since all active processes have to be loaded into the memory, which creates another problem that physical memory becomes the limit for how many processes can be loaded. (Swapping might help but the number of active processes still needs to be in the memory.)\n  Need to add memory references every allocation.\n  Memory management is a mess.\n  Everyone has the same permission throughout the address space, which can potential create problems.\n  ","permalink":"https://www.bodunhu.com/blog/posts/virtualaddress/","summary":"I love pointers. Pointer is very a useful feature in programming languages like C/C++. I can pass weird hexidecimal numbers to a function and then it will magically locate where the program is in memory. However, all those values we see are merely virtual addresses, a running program\u0026rsquo;s view of memory in system. Any address we can see while programming user-level programs is a virtual address. It is no more than an illusion of where the data is actually laid out in memory.","title":"Virtual Memory Overview"}]