<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>paper-review on std::bodun::blog</title><link>https://www.bodunhu.com/blog/categories/paper-review/</link><description>Recent content in paper-review on std::bodun::blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 09 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://www.bodunhu.com/blog/categories/paper-review/index.xml" rel="self" type="application/rss+xml"/><item><title>Paper Review - Dynamic Tensor Rematerialization</title><link>https://www.bodunhu.com/blog/posts/paper-review-dynamic-tensor-rematerialization/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/paper-review-dynamic-tensor-rematerialization/</guid><description>Dynamic Tensor Rematerialization (DTR) treats GPU memory as a large cache, where tensors can be evicted to save memory, and recomputed if needed later.
DTR&amp;rsquo;s eviction policy relies on the heuristic \(h\). The heuristic assigns a value \(h(t)\) to each resident tensor \(t\), approximating the cost of evicting the tensor. DTR evicts the tensor with the lowest cost based on the value of \(h\). \(h\) can factor in arbitrary metadata.</description></item><item><title>Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning</title><link>https://www.bodunhu.com/blog/posts/paper-review-capuchin-tensor-based-gpu-memory-management-for-deep-learning/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/paper-review-capuchin-tensor-based-gpu-memory-management-for-deep-learning/</guid><description>This paper aims to reduce GPU memory usage during DNN training. Capuchin achieves this goal though swapping and recomputation, using tensor as unit of operation. The major question is how to balance between swapping and recomputation to achieve max resource utilization.
Swap and Recomputation Benefit The ultimate goal of swapping and recomputation is to hide the overhead as much as possible to minimize the wait time of back-access (a tensor evicted earlier being accessed again).</description></item></channel></rss>