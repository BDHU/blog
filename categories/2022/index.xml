<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2022 on std::bodun::blog</title><link>https://www.bodunhu.com/blog/categories/2022/</link><description>Recent content in 2022 on std::bodun::blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 30 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.bodunhu.com/blog/categories/2022/index.xml" rel="self" type="application/rss+xml"/><item><title>TensorIR Transformation</title><link>https://www.bodunhu.com/blog/posts/tensorir-transformation/</link><pubDate>Tue, 30 Aug 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/tensorir-transformation/</guid><description>In the previous post, we&amp;rsquo;ve explored how to write primitive functions in TensorIR. Here, we will see how to transform TensorIR into other (potentially more performant) variants. The content is drived from the mlc course taught by Tianqi Chen.
Batched BMM ReLu A batched matrix multiplication followed by a ReLu operation can be expressed using numpy as:
def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray): Y = np.empty((16, 128, 128), dtype=&amp;#34;float32&amp;#34;) for n in range(16): for i in range(128): for j in range(128): for k in range(128): if k == 0: Y[n, i, j] = 0 Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j] for n in range(16): for i in range(128): for j in range(128): C[n, i, j] = max(Y[n, i, j], 0) Translating the numpy code into TensorIR we get:</description></item><item><title>Dive into TensorIR</title><link>https://www.bodunhu.com/blog/posts/dive-into-tensorir/</link><pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/dive-into-tensorir/</guid><description>TensorIR is a compiler abstraction for optimizing programs with tensor computation primitives in TVM. Imagine a DNN task as a graph, where each node represents a tensor computation. TensorIR explains how each node/tensor computation primitive in the graph is carried out. This post explains my attempt to implement 2D convolution using TensorIR. It is derived from the Machine Learning Compilation course offered by Tianqi Chen.
Implement 2D Convolution 2D convolution is a common operation in image processing.</description></item><item><title>Pathways: Google's New ML System</title><link>https://www.bodunhu.com/blog/posts/pathways-googles-new-ml-system/</link><pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/pathways-googles-new-ml-system/</guid><description>Google recently released the paper about its new ML system called &lt;a href="https://arxiv.org/pdf/2203.12533.pdf">Pathways&lt;/a>. I&amp;rsquo;m a bit surprised since I expect it to introduce a brand new model architecture. In fact, this paper is not easy to digest at all. I feel like it&amp;rsquo;s written for people who spent many years developing ML frameworks. Anyway, we will try to understand why it is developed and how it works. Also, you should check this &lt;a href="https://zhuanlan.zhihu.com/p/495592456">post&lt;/a> (in Chinese). This post explains many concepts in Pathways much more clearly. Many contents here are credited to this post.</description></item><item><title>FlexFlow</title><link>https://www.bodunhu.com/blog/posts/flexflow/</link><pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/flexflow/</guid><description>FlexFlow is a deep learning framework that discovers a fast parallelization strategy for distributed DNN training. It uses &lt;em>SOAP&lt;/em> (Sample-Operation-Attribute-Parameter) search space of parallelization strategies. in short, FlexFlow automates the parallelization of model training.</description></item><item><title>Add Mermaid to Hugo with Dark Mode</title><link>https://www.bodunhu.com/blog/posts/add-mermaid-to-hugo-with-dark-mode/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/add-mermaid-to-hugo-with-dark-mode/</guid><description>Recently, I was revisiting materials in Deep Learning. I need tools that generate diagrams easily. Drawing the graphs from scratch and upload them individually to the image hosting platform is a daunting process. This is when &lt;a href="https://github.com/mermaid-js/mermaid">Mermaid&lt;/a> comes into rescue. Now I can generate diagrams directly using Markdown. Here&amp;rsquo;s how to do it inside a &lt;a href="https://gohugo.io/">Hugo&lt;/a> site.</description></item><item><title>Cross Entropy Loss</title><link>https://www.bodunhu.com/blog/posts/cross-entropy-loss/</link><pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/cross-entropy-loss/</guid><description>Many deep learning tasks involve classification, where a model outputs a series of probabilities for their corresponding labels. The goal is to correctly predict a given input&amp;rsquo;s label. Mathematically, it means generating max probabilities for the correct label. The probabilities are generated through a process called softmax.
The softmax function outputs a vector \(\hat{y}\), which represents estimated conditional probabilities of each class given an input \(x\), For example, \(\hat{y}_1 = P(y=\textrm{car}\ |\ x)\).</description></item><item><title>Maximum Likelihood for Classification</title><link>https://www.bodunhu.com/blog/posts/maximum-likelihood-for-classification/</link><pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/maximum-likelihood-for-classification/</guid><description>Let&amp;rsquo;s say we want to classify an input text \(y\) and give it a label \(x\). Formally, we want to find:
\[ \textrm{argmax} P(x | y) \]
By Bayes&amp;rsquo; rule this is the same as
\[ \textrm{argmax} \frac{P(y|x)P(y)}{P(x)} \]
Suppose we have five documents as training data and one document as the input as testing data. Our objective is to give a label to the test sentence.
Credit: Eunsol Choi Let&amp;rsquo;s define the probability of class as (\(N\) is the total number of classes)</description></item><item><title>Machine Learning System Resources</title><link>https://www.bodunhu.com/blog/posts/machine-learning-system-resources/</link><pubDate>Sat, 08 Jan 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/machine-learning-system-resources/</guid><description>This is my personal list of resources related to machine learning systems. Feel free to drop me an email if you think there&amp;rsquo;s something worth mentioning. I will try to update this page frequently to include the most recent stuffs in mlsys. Resources Facebook&amp;rsquo;s external large-scale work NGC Container Doc Awesome-System-for-Machine-Learning: A curated list of research in machine learning systems (MLSys). Paper notes are also provided. Courses Deep Learning Systems:</description></item></channel></rss>