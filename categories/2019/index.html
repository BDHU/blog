<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2019 | std::bodun::blog</title><meta name=keywords content><meta name=description content="Personal Blog for Bodun Hu. Longhorn at UT Austin"><meta name=author content><link rel=canonical href=https://www.bodunhu.com/blog/categories/2019/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.e5f3c4ceaebb5646fc1ba414004c06d9e00115f24843fefe9f9cd9552d00dad7.css integrity="sha256-5fPEzq67Vkb8G6QUAEwG2eABFfJIQ/7+n5zZVS0A2tc=" rel="preload stylesheet" as=style><link rel=icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.utexas.edu/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.utexas.edu/favicon-32x32.png><link rel=apple-touch-icon href=https://www.utexas.edu/apple-touch-icon.png><link rel=mask-icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><link rel=alternate type=application/rss+xml href=https://www.bodunhu.com/blog/categories/2019/index.xml><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-108144808-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="2019"><meta property="og:description" content="Personal Blog for Bodun Hu. Longhorn at UT Austin"><meta property="og:type" content="website"><meta property="og:url" content="https://www.bodunhu.com/blog/categories/2019/"><meta property="og:image" content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Bodun's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="2019"><meta name=twitter:description content="Personal Blog for Bodun Hu. Longhorn at UT Austin"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://www.bodunhu.com/blog/ accesskey=h title="std::bodun::blog (Alt + H)">std::bodun::blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.bodunhu.com/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://www.bodunhu.com/blog/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://www.bodunhu.com/blog/about/ title=About><span>About</span></a></li><li><a href=https://www.bodunhu.com/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://www.bodunhu.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://www.bodunhu.com/blog/categories/>Categories</a></div><h1>2019</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/probabilistic_clock_synchronization/>Understanding Probabilistic Clock Synchronization</a></h2></header><footer class=entry-footer>September 17, 2019&nbsp;·&nbsp;5 min</footer><section class=entry-content><p><p>This post is meant to discuss the probabilistic clock synchronization technique. The main goal of this technique is to bound the difference between systems by setting up an upper bound. In short,
\(|P(t)-Q(t)|\leq \varepsilon\). We will discuss what these symbols represent later.</p><h2 id=perfect-synchronization>Perfect Synchronization</h2><p>The motivation behind this technique is that synchronization always involves overheads. In a perfect environment where network delay and request processing time are both 0, the clocks can be synchronized with ease. A slave P will send &ldquo;Time = ?&rdquo; at global time
t
to master Q and master Q replies &ldquo;Time = Q(t)&rdquo; instantaneously at global time
t
. Then P will adjust its clock P(t) according to Q(t). However, such case only exists in imagination.</p><h2 id=amortization>Amortization</h2><p>Suppose the difference between the clock of P and Q is \(\Delta\) at synchronization, our goal is to adjust P&rsquo;s logical clock C(t) to mitigate the difference. The adjustment is simple:</p><p>\[C(t)=H(t)+A(t)\]</p><p>Here C(t) is P&rsquo;s logical clock, H(t) is P&rsquo;s hardware clock, and A(t) is the adjustment function(can also be A(H(t))).</p><p align=center><img src="https://raw.githubusercontent.com/BDHU/Page_pics/master/clock.png?token=ACKPLVNGE4DFY4GQF55PU7C5QFOGW" width=600></p><p>A naive method will be simply subtract or add
\(\Delta\) to C(t) to mitigate the difference. However, it will create a discontinuity in P&rsquo;s clock, which may disrupt systems services. For example, if
\(\Delta = 2\) seconds,the logical clock will instantly jump ahead 2 seconds and a stopwatch will skip one second.</p><p>So the adjustment function is as follows:</p><p>\[A(t)=m\cdot H(t)+N\]</p><p>Now the logical clock can be derived as follows:</p><p>\[C(t)=(1+m)\cdot H(t)+N\]</p><p>This process is called amortization.</p><p>However, how do we know the value for m and N? Let&rsquo;s take a look at the time when amortization process starts, the logical time of P at this moment is:</p><p>\[L=(1+m)\cdot H+N \qquad (1)\]</p><p>At the end of the amortization (lasts for time period \(\alpha\)) we have reached
\(M=H+\alpha\)
. Here M is the master logical clock sent by master Q. So at the end of the amortization, the slave P should be able to catch up with its master&rsquo;s logical clock after
\(\alpha\)
period of time. Therefore, we have:</p><p>\[M+\alpha = (1+m)(H+\alpha)+N \qquad (2)\]</p><p>Solving (1) and (2) together, we now get:</p><p>\[m = \frac{M-L}{\alpha}\]</p><p>\[N = L - (1+m)H\]</p><p>Thus, at the end of amortization at time t where
\(t > H+\alpha\)
, we would want the following to be true:</p><p>\[C(t)=C(H+\alpha)+(H(t)-H(H+\alpha))=H(t)+M-H\]</p><p>Here is a question, why is N required in this case. Couldn&rsquo;t we simply use m to amortize the time difference? Here&rsquo;s my interpretation(feel free to pin me if you have something else in mind): if N is set to be 0, then at the beginning of amortization, we would have:</p><p>\[L=(1+m)H\]</p><p>Therefore,
\(m = \frac{L-H}{H}\)
. Now, m is settled by L and H. Compared to
\(m=\frac{M-L}{\alpha}\)
, we can see that now m is a constant and not determined by the value of
\(\alpha\)
. We lost control of the amortization rate m, which is not desirable.</p><h2 id=general-case>General Case</h2><p>We now return to the general case where network delay and processing time are both present. The sitution is represented below:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/general_case.png width=500></p><p>Looking at this graph, we can see slave P takes 2d real time to for a round-trip. Let&rsquo;s also assume that 2D is the round-trip delay measured by P&rsquo;s clock between sending and receiving. Then we can bound the clock time 2D based on the drift rate \(\rho\) of the clock:</p><p>\[2d(1-\rho)\leq 2D \leq2d(1+\rho)\]</p><p>Ignoring higher order terms of \(\rho\), we now have \(2d\leq(1+\rho)2D\).</p><p>When looking at the graph above, one thing to notice is we are not sure of the time \(\alpha\) and \(\beta\). However, if we are going to pick one, \(\beta\) will be more important than \(\alpha\). This is because if we know the value of \(\beta\), then we know the lower bound of the round-trip delay. Here we assume min is the minimum amount of time required for network transfer, \(\beta\) will be the time master Q spends between processing the request and responds the result back to P.</p><p>Now we&rsquo;ve narrowed down our focus to \(min+\beta\). The time interval between \(Q(t)=T\) and the arrival of &lsquo;&lsquo;Time=T&rsquo;&rsquo; at P will be at least \(min(1-\rho)\). This is based on \(\beta=0\) and clock drift rate.</p><p>The upper bound of the interval will be \((min+\beta)(1+\rho)\), assuming no time is wasted for \(Q\) to wait until it starts processing the request from P. The time required will be \(min+\beta\) and we need to take Q&rsquo;s drift rate \(\rho\) into account. We can also see that the total round-trip real time is \(2d=2min+\alpha+\beta\). Thus we get:</p><p>\[\beta=2d-2min-\alpha \leq 2d-2min\]</p><p>With this equation, we can see that the upper bound measured from Q(t)=T is also bounded. Thus, we have:</p><p>\[
\begin{eqnarray}
(min+\beta)(1+\rho) &\leq& (min+2d-2min)(1+\rho) \nonumber \newline
&=& (2d-min)(1+\rho) \nonumber \newline
&=&(1+\rho)2d-min(1+\rho) \nonumber \newline
&\leq&(1+\rho)2D(1+\rho)-min(1+\rho) \nonumber \newline
&=&(1+2\rho +\rho^2)2D-min(1+\rho) \nonumber \newline
&\approx&(1+2\rho )2D-min(1+\rho) \nonumber
\end{eqnarray}
\]</p><p>Now we can see that master Q&rsquo;s clock time when P receives the response is bounded in the interval \([T+min(1-\rho), T+2D(1+2\rho )-min(1+\rho)]\). The take away here is that we can&rsquo;t use real time t in a distributed system because it&rsquo;s merely an abstract concept since all systems in a network essentially rely on their own clock time. We need to find the relationship between T and master&rsquo;s clock cycle because P will rely on T, not real time t.</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/arxivsubmition/>How to Put Papers on ArXiv</a></h2></header><footer class=entry-footer>June 25, 2019&nbsp;·&nbsp;3 min</footer><section class=entry-content><p><p>I was recently trying to put my research paper draft on ArXiv. I thought it would be as simple as submitting the pdf file, which should take approximately less than ten minutes. I was wrong. It took several hours to figure what was going on. I included some tips here to prevent mistakes I made from happening again.</p><p>The first mistake I made was assuming a single submission of pdf file would be sufficient. ArXiv apparently has mechanisms detecting whether the submitted pdf file is generated using Tex/Latex. According to ArXiv:</p><blockquote><p>a PDF file created from a TeX/LaTeX file will be rejected. There are <a href=https://arxiv.org/help/faq/whytex>good reasons</a> why arXiv insists on TeX/LaTeX source if it is available. arXiv produces PDF automatically from all TeX submitted source. For information on viewing the PDF provided by arXiv, see our PDF browsing help.</p></blockquote><p>So, the first thing I came up with was to somehow make the pdf appearing &ldquo;anonymous&rdquo; to ArXiv. The were several methods but none of them appear to be practical. If you are interested there is a <a href=https://tex.stackexchange.com/questions/95080/making-an-anonymous-pdf-file-using-pdflatex/95109>link</a> to some methods that might be useful. <a href=https://ctan.org/pkg/pdfprivacy>pdfprivacy</a> is package used to remove or suppress pdf meta-data and it sounds promising but I haven&rsquo;t tried yet.</p><p>So the only option left was to follow the restriction described. It was confusing at the beginning because the everything worked like a charm on Overleaf but it completely fell apart when I tried to compile the sources locally. I was under the impression that if it worked on Overleaf i should work everywhere else, which cause many hours of searching for potential problems related to local environment.</p><p>After hours of frustration, it started to appear that there was nothing wrong with my local environment. The pdf produced by Overleaf was only &ldquo;appearing&rdquo; correct. There were several syntax issue in my .bib file, mostly caused by careless copy-and-paste and duplicate records. Overleaf simple suppressed some of those errors, which leads me to think everything was fine.</p><p>There were also error messages popping up during compilation. Most of them are related to undefined references. Something like:</p><p><code>Warning--empty journal in article</code></p><p>The problem was that bibliographic information obtained from Google Scholar night include serious mistakes. The warning message was telling that entries of type @article require a non-empty journal field. For example, the entry could look like:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>@article{article,  
  title={Something Cool},  
  author={Somebody},
  year={2019},
  publisher={IET}
}
</code></pre></div><p>The four required fields for entries of type @article are author, title, journal, and year. This is why the warning message showed up. But it doesn&rsquo;t really affect the compilation on ArXiv.</p><p>When I finally compiled all sources locally with success, I immediately moved all source on ArXiv hoping it would finally work. It didn&rsquo;t.</p><p><code>! LaTeX Error: File 'shoc.pdf' not found.</code></p><p>I had no idea why this occurred. All sources I used to compile were uploaded to ArXiv so there were no reasons for it to fail. More surprisingly, the references only failed for my .eps files but not .png files. According to ArXiv there are several <a href=https://arxiv.org/help/faq/psbad>reasons</a> why PostScript (PS/EPS) figures might fail on ArXiv. Due to the error message, it appears the system is trying to find a file called <em>shoc.pdf</em> to insert into the main pdf but somehow couldn&rsquo;t locate the file.</p><p>The solution was to upload the pdf files produced locally to ArXiv. However, the locally generated files have slightly different name. All files names are modified to &ldquo;name-eps-convert-to.pdf&rdquo;. What a hassle!</p><p>Overall, uploading to ArXiv was not the most pleasant experience. Latex&rsquo;s compilation system is the one to blame.</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/operatingsystemmemorymanagement/>A Little Review on Barrelfish Memory Management</a></h2></header><footer class=entry-footer>February 18, 2019&nbsp;·&nbsp;9 min</footer><section class=entry-content><p><p>The memory management has been mentioned numerous times and still remains huge topic. virtual vs. physical memory, physical frame allocation, MMUs, page faults, address space layout, and
demand paging and swapping are familiar terms for every undergrad in college.</p><p>In monolithic kernels such as Linux, much of the functionality is handled in kernel. However, there are OSes that push these functionalities to user space such as Barrelfish. Many concept here will thus be borrowed from the <a href=http://www.barrelfish.org/>Barrelfish OS</a>. I will also borrow some materials from the main pdf from Barrelfish course materials provided by Professor Simon Peter.</p><h2 id=memory-management-in-general>Memory Management in General</h2><p>Microkernels like L4, Mach, Chorus, and Spring trapped page faults in the kernel but then reflected them up to other processes which carried out the
actual page fault handling. This was done on a per-region basis, so each area of
virtual memory was associated with some paging server. Memory objects could be shared between different processes and mapped differently in different address spaces.</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/Barrelfish/os.png width=400></p><p>Such abstraction means that what happens when a page fault happens is entirely dependent on the code in the user-level pager. This design is highly extensible since it&rsquo;s all user code and thus isolated, which means that if a user-level pager crashes, there&rsquo;s a good chance the rest of the OS can continue quite happily since much of the functionality is moved away from the kernel.</p><p>However, moving functionality out of the kernel an important question: if user-space processes can manipulate virtual address spaces, how can
we make sure that one user&rsquo;s program can&rsquo;t manipulate another address space and memory? Here we will introduce the concept of capabilities.</p><h2 id=capabilities>Capabilities</h2><p>Capabilities are introduced to solve the access control problem in operating systems. Access control is the problem of specifying, and enforcing, which subjects (or principals) can perform particular actions on particular objects in an operating system.</p><p>The Barrelfish documentation does a good job illustrating capabilities: abstractly, access control can be thought of as a matrix, which represents all possible combinations of operations in the system. Each row of the matrix represents a
different subject, and each column represents a different object. Each entry in the
matrix contains a list of permissible actions.</p><p>Thus, we have two targets to emphasis: the subject and the object. The ACL(access control list) focuses on the object being operated on.</p><p>A good example will be whenever you enter <em>ls -a</em> in a Linux terminal, you will get list of entries specifies the attributes of a file. Here the attributes represent how a object (in this case, a file) may be accessed.</p><p>On the other hand, a capability can be thought of as a &ldquo;key&rdquo; or &ldquo;licence&rdquo;. It is an unforgettable token which grants authority. Possession of a capability for an object gives the holder the right to perform certain operations on the object.</p><p>A good example will be the file descriptor in Linux. A file is accessed through its file descriptor. Here the file descriptor serves as the &ldquo;key&rdquo; to gain access to the file itself. Capabilities provide fine-grained access control: it is easy to provide access to specific subjects, and it is easy to delegate permissions to others in a controlled manner.</p><p>Note that to be correct, any capability representation must protect capabilities
against forgery. Capabilities can be implemented in various ways such as tagged capabilities, sparse capabilities, or partitioned capabilities. In Barrelfish we used the partitioned capabilities.</p><p>In partitioned capabilities, the kernel ensures that memory used to store capabilities is always separated from that used by user processes to store data and code, for example by using the MMU or ensuring that capability memory is only accessible in kernel mode. The OS maintains the list of capabilities each user principal holds (the clist), and explicitly validates access when performing any privileged operation. Thus, whenever the user accesses memory, the operation can only be done through the resources' corresponding capability. For example, one can map a page frame in the page table page through functions calls with only capabilities.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>Caprefa<span style=color:#f92672>-&gt;</span>install(Caprefb, slot, flags)
</code></pre></div><h2 id=capabilities-in-barrelfish>Capabilities in Barrelfish</h2><p>According to Barrelfish documentation, all memory in Barrelfish (and some other system resources which do not occupy memory) is described using capabilities. Capabilities are typed, and capabilities can be retyped by users holding them according to certain rules and restrictions. The official documentation has very good explanation on the capability management in Barrelfish. Here is the permissible types for the retype invocation capability retyping:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/Barrelfish/cap.png width=400></p><p align=center><a href=http://www.barrelfish.org/publications/TN-013-CapabilityManagement.pdf>Image source</a></p><p>Capabilities referring to memory regions. Capabilities can also be split, resulting 2 new capabilities of the same type, one for each half of the region. Some of the more important capability types in Barrelfish are shown in figure below. The picture is from the Barrelfish manual provided in CS378 Mutlicore class by Simon Peter:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/Barrelfish/cap_aos.png width=400></p><p>Allocation and management of physical memory is achieved by retyping and splitting operations on capabilities. For most kernels, the implementation is to constantly allocate and deallocate memory for a wide variety of purposes, much as any large C program relies heavily on malloc and free.</p><p>The problem is what the kernel should do when this runs out. The current solution in Linux is little more than &ldquo;kill a random process and reclaim its memory&rdquo;, which can be a problem for system stability. In Barrelfish, all kernel objects are actually allocated by user programs. If a user process wants to create another process (or dispatcher in Barrelfish parlance), it has to get a capability to a DRAM area of the right size, retype this capability to type Dispatcher, and hand this to the kernel. This will be covered in later posts. To access different types of memory resources, the corresponding capability has to be retyped to the right type.</p><h2 id=more-on-implementation>More On Implementation</h2><p>In Barrelfish, every capability resides in a slot in a CNode, so a pair (CNode, slot) would identify a capability. It is important to point out that the CNode is another capability itself. Each process in Barrelfish has a CSpace which is structured as a two-level table. So there are actually two different CNode capability types - one for the first level of the table, and one for the second. Every process has, within its &ldquo;dispatcher control block&rdquo;, a pointer to the top-level or root CNode which the kernel can traverse.</p><p>A capability reference in Barrelfish is very similar to VA: the first few bits can represent an index into the first level L1CNode, while the next few bits refer to a slot in a CNode referred to by the capability in the L1CNode slot. Here is a picture from the main pdf showing how the the CSpace is represented in Barrelfish:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/Barrelfish/Cspace.png width=400></p><h2 id=thoughts-on-design-decisions>Thoughts on Design Decisions</h2><p>Even though it is pretty straight forward to understand the CSpace structure, the actual implementation is a lot more complicated than that. Since the CSpace is not directly accessible by user space program, there are additional data structures used to keep track of available memory resources.</p><p>In our implementation, the user process keeps a doubly linked list of <em>struct mmnode</em> to indicate the memory available for allocation. Each element in the free list tracks the information corresponding to one capability. However, there is a big problem with this seemingly simple implementation. Every time we allocate a practical memory space from the memory region, a new capability is created while the old capability still remain in the physical memory pointing to a memory range before the allocation happens. Therefore, the old capability would cover extra memory spaces that are already allocated and managed by other capabilities.</p><p>To solve this problem, we maintain the allocation information in the <em>struct mmnode</em> each time an allocation occurs. If a capability covering physical address space from 0 to 100 is requested for 20 units of memory space, then the memory available for the next allocation would be from 20 to 100 even though the capability itself still manages 0 to 100. By restricting subsequent accesses only to the new memory range, the old capability can still be kept around and used later for retyping.</p><p>Another Problem emerges when we try to free a memory. Since everything is managed by capabilities, freeing a piece of memory also involves managing the capability responsible for the memory. So an intuitive thought could be whenever a memory space is freed, the corresponding capability is merged back to a piece of memory adjacent to it, managed by a different capability.</p><p>However, since capabilities can not be merged, an alternative choice would be to simple destroy it during free. However, this is even a bigger problem in Barrelfish.</p><p>Imagine the scenario where capability A is partially allocate from memory space 30 to 100. Later on another memory is freed and that piece of memory is managed by capability with base 100 and size 20, so the memory range covers 100 to 120, which indicates these the two capability could be &ldquo;merged&rdquo;.</p><p>In this case, if the first capability is destroyed, all children of the first capability will also be destroyed, thus the already allocated memory from 0 to 20 will be thrown away, which is not desired. If the second capability is destroyed, the first one will also be destroyed to create a new capability covering 20 to 120, which will still results in the destruction of capability A.</p><p>Our assumption here is that the parent or root capability is never destroyed when added to the free list. Whenever a capability needs to be freed, the memory manager is responsible to make sure the capability is only merged with another capability from the same parent capability.</p><p>This is done by creating another list of nodes that tracks all parent capabilities. It is only added when the memory manager adds new capabilities to the free list. After the user initializes free, the memory manager actually creates a new free struct mmnode first, then it find the node&rsquo;s parent node, copying the parent&rsquo;s capability and attributes to the newly created node with updated offset to indicate that the memory hasn&rsquo;t been freed yet.</p><p>After that, the memory manager insert the node into the free list. If the memory manager finds out that there are capabilities adjacent to the just-added node, then we simply need to update the attributes of the corresponding mmnode to indicate that merging succeeds. The old mmnode is simply thrown away.</p><p>The advantage of this implementation is that root or parent capabilities are kept around and the next retype will be fairly simple. The implementation is also very straightforward.</p><p>There is of course more efficient solution than a linked list. For example, Linux uses both linked list and red-black tree to store thread information. The redundant data structures can be used in different scenarios when appropriate. However, we only use this simplified version to prove our concepts. Optimizations vary but the general concept still works pretty well.</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/gpumemoryhierarchy/>Pascal GPU memory and cache hierarchy</a></h2></header><footer class=entry-footer>January 15, 2019&nbsp;·&nbsp;10 min</footer><section class=entry-content><p><p>Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program&rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation. Turing architecture might have different results, but the method we used here can be applied as well with slight modification. The method we use in this guide is inspired by the research paper: <a href=https://ieeexplore.ieee.org/document/7445236>Dissecting GPU Memory Hierarchy through Microbenchmarking</a>. Here we will explain how P-Chase works and walk through a small example.</p><h2 id=memory-hierarchy-overview>Memory Hierarchy Overview</h2><p>GPU memory hierarchy is different compared to CPU memory hierarchy. Using the terminologies of CUDA, GPU memory space can be categorized in these groups: register, constant memory, shared memory, texture memory, local memory, and global memory. Each different memory space have its own properties. Since we are interested the
cache systems, here is a picture demonstrating the memory hierarchy of a NVIDIA GPU:</p><p align=center><img src=https://gistbok.ucgis.org/sites/default/files/1000px-Memory.svg_.png width=450></p><p align=center><a href=https://gistbok.ucgis.org/bok-topics/graphics-processing-units-gpus>Image source</a></p><p>The characteristics of each memory space can be found in <a href=https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA_C_Programming_Guide.pdf>NVIDIA CUDA C Programming Guide
</a>. Here we will focus on some target memory space we are interested in. The paper lists some properties of our target memory space:</p><table><thead><tr><th>Memory</th><th style=text-align:center>Type</th><th style=text-align:right>Cached</th><th style=text-align:right>Scope</th></tr></thead><tbody><tr><td>Global</td><td style=text-align:center>R/W</td><td style=text-align:right>Yes</td><td style=text-align:right>All Threads</td></tr><tr><td>Shared</td><td style=text-align:center>R/W</td><td style=text-align:right>N/A</td><td style=text-align:right>Thread Blocks</td></tr><tr><td>Texture</td><td style=text-align:center>R</td><td style=text-align:right>Yes</td><td style=text-align:right>All Threads</td></tr></tbody></table><p>Even though the paper targets Fermi, Kepler and Maxwell generations of GPU, the properties of the table still holds for Pascal GPU and possibly Turing as well. The cached global/texture memory uses a two-level caching system. The L1 cache is located in each stream multiprocessor (SM), while the L2 cache is off-chip and shared among all SMs. It is unified for instruction, data and page table access. According to CUDA documentation, like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. This function previously was served by the separate L1 cache in Fermi and Kepler. Page table is used by GPU to map virtual addresses to physical addresses, and is usually stored in the global memory. The page table is cached in TLB to reduce memory access latency. Once a thread cannot ﬁnd the page entry in the TLB, it would access the global memory to search in the page table, which introduced significant memory access latency. The GPU-specific shared memory is located in the SMs. On the Fermi and Kepler devices, it shares memory space with the L1 data cache. On Maxwell and Pascal devices, it has a dedicated space, since the functionality of the L1 and texture caches have been merged. One thing to note here is that shared memory is accessed by the thread blocks. Thread-blocks remain limited to 48 KB of shared memory in Pascal. Therefore, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block. This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM.</p><p>However, we should be careful that by default, GP100 caches global loads in the L1/Texture cache. In contrast, GP104 follows Kepler and Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler. As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to nvcc at compile time. Even though both GP100 and GP104 belongs to Pascal family, we only focus on GP100 here because that&rsquo;s the GPU we use. Another thing to notice is that unlike Maxwell but similar to Kepler, Pascal caches thread-local memory in the L1 cache. This can mitigate the cost of register spills compared to Maxwell. To illustrate our point, we checked both cudaDevAttrGlobalL1CacheSupported and cudaDevAttrLocalL1CacheSupported on Tesla P100 and GTX 1080 and find both attributes to be 1.</p><p>In addition to the L2 data cache, global memory data that is read-only for the entire lifetime of a kernel can be cached in the read-only data cache with a compute capability of 3.5 or above. We will also explore the size of this read-only cache using __ldg() intrinsic.</p><h2 id=p-chase>P-Chase</h2><p>Most existing GPU microbenchmark studies on cache architecture assume a classical set-associative cache model with the least recently used (LRU) replacement policy, the same as the conventional CPU cache. So here we will use this assumption and proceed with our experiments. Here are some notations we will use throughout this post.</p><table><thead><tr><th>Notation</th><th style=text-align:center>Description</th><th style=text-align:right>Notation</th><th style=text-align:right>Description</th></tr></thead><tbody><tr><td>C</td><td style=text-align:center>Cache Size</td><td style=text-align:right>N</td><td style=text-align:right>array size</td></tr><tr><td>b</td><td style=text-align:center>cache line size</td><td style=text-align:right>s</td><td style=text-align:right>stride size</td></tr><tr><td>a</td><td style=text-align:center>cache associativity</td><td style=text-align:right>k</td><td style=text-align:right>iterations</td></tr><tr><td>T</td><td style=text-align:center>number of cache set</td><td style=text-align:right>r</td><td style=text-align:right>cache miss rate</td></tr></tbody></table><p>Under our assumptions, data is loaded from main memory to lower cache in the basic unit of a cache line. The number of words in a cache line is referred to as the line size (b). For the LRU set-associative cache, the cache memory is divided into T cache sets, each of which consists of \(a\) cache lines. It is essential to have these three assumptions using this kind of cache model:</p><ul><li><p><strong>Assumption 1</strong> All cache sets have the same size. The cache parameter should satisfy \(T \cdot a \cdot b = C\).</p></li><li><p><strong>Assumption 2</strong> In the memory address, the bits representing the cache set are immediately followed by the bits representing the offset.</p></li><li><p><strong>Assumption 3</strong> Cache replacement policy should be LRU.</p></li></ul><p>We will later see why these assumptions are essential as we proceed with the experiment. We won&rsquo;t go through how P-Chase work exactly. To find more information, this <a href=https://arxiv.org/pdf/1509.02308.pdf>paper</a> does a good job illustrating how P-Chase work. The takeaway is, we need to brute force an array with one element more than a cache can hold so that cache miss will start to occur periodically whereas such array with less or equal elements to the cache capacity will always result in cache hit and thus no access overhead will be introduced after all data is loaded into the cache. This is the algorithm the paper proposed and we will use it to do the experiment:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>__global__ <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>KernelFunction</span> ( . . . ) {
    <span style=color:#75715e>//declare shared memory space
</span><span style=color:#75715e></span>    __shared__ <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> s tvalue [ ] ;
    __shared__ <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> s index [ ] ;
    preheat the data ; <span style=color:#75715e>// implementation varies
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (it <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; it <span style=color:#f92672>&lt;</span> iter ; it<span style=color:#f92672>++</span>) {
        start_time<span style=color:#f92672>=</span>clock();
        j <span style=color:#f92672>=</span> my_array[j];
        <span style=color:#75715e>//store the array index
</span><span style=color:#75715e></span>
        <span style=color:#75715e>// This following line is essential because due to
</span><span style=color:#75715e></span>        <span style=color:#75715e>// instruction-level parallelism (ILP), function clock() may
</span><span style=color:#75715e></span>        <span style=color:#75715e>// overlap with its previous instruction and even return before
</span><span style=color:#75715e></span>        <span style=color:#75715e>// the previous instruction finishes. For example,
</span><span style=color:#75715e></span>        <span style=color:#75715e>// end_time=clock() can return before j = my_array[j] returns.
</span><span style=color:#75715e></span>        <span style=color:#75715e>// adding s_index [it]= j since it have data dependency on the
</span><span style=color:#75715e></span>        <span style=color:#75715e>// previous line. Thus the memory access will be over before
</span><span style=color:#75715e></span>        <span style=color:#75715e>// end_time=clock() started.
</span><span style=color:#75715e></span>        s_index [it]<span style=color:#f92672>=</span> j;
        end_time<span style=color:#f92672>=</span>clock();
        <span style=color:#75715e>//store the access latency
</span><span style=color:#75715e></span>        s_tvalue[it]<span style=color:#f92672>=</span> end_time<span style=color:#960050;background-color:#1e0010>−</span>start_time ;
    }
}
</code></pre></div><p>The steps is the same as the paper proposes, so here we show the paper&rsquo;s method:</p><ol><li><p>Determine cache size C . We set s to 1. We then initialize N with a small value and increase it gradually until the ﬁrst cache miss appears. C equals the maximum N where all memory accesses are cache hits.</p></li><li><p>Determine cache line size b. We set s to 1. We begin with N = C + 1 and increase N gradually again. When N &lt; C + b + 1, the numbers of cache misses are close. When N is increased to C + b + 1, there is a sudden increase on the number of cache misses, despite that we only increase N by 1. Accordingly we can ﬁnd b. Based on the memory access patterns, we can also have a general idea on the cache replacement policy.</p></li><li><p>Determine number of cache sets T . We set s to b. We then start with N = C and increase N at the granularity of b. Every increment causes cache misses of a new cache set. When N > C + (T − 1)b, all cache sets are missed. We can then deduce T from cache miss patterns accordingly.</p></li><li><p>Determine cache replacement policy. As mentioned before, if the cache replacement policy is LRU, then the memory access process should be periodic and all the cache ways in the cache set are missed. If memory access process is aperiodic, then the replacement policy cannot be LRU. Under this circumstance, we set N = C + b, s = b with a considerable large k (k &#187; N/s) so that we can traverse the array multiple times. All cache misses are from one cache set. Every cache miss is caused by its former cache replacement because we overﬂow the cache by only one cache line. We have the accessed data indices thus we can reproduce the full memory access process and ﬁnd how the cache lines are updated.</p></li></ol><h2 id=texture-l1-cache-and-read-only-data-cache>Texture L1 Cache and Read-only Data Cache</h2><p>When use the <a href=http://www.comp.hkbu.edu.hk/~chxw/Code/fine_grain_Maxwell_texture_L1.cu>code</a> with increased our own data preheat implementation because the texture L1 cache can potentially be greater than the shared memory. The original code uses the first iteration of the loop in the algorithm as a way to preheat data:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>const</span> <span style=color:#66d9ef>int</span> it <span style=color:#f92672>=</span>  <span style=color:#ae81ff>6144</span> <span style=color:#75715e>// texture L1 may hold more elements,
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// So the first iteration may not cold
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// hit all elements, some cold hits can
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// be moved to the second iteration,
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// causing confusion
</span><span style=color:#75715e></span><span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> cnt<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>; cnt <span style=color:#f92672>&lt;</span> it; cnt<span style=color:#f92672>++</span>) {
	start<span style=color:#f92672>=</span>clock();
        j<span style=color:#f92672>=</span>tex1Dfetch(tex_ref, j);
	s_value[cnt] <span style=color:#f92672>=</span> j;
			
	end<span style=color:#f92672>=</span>clock();			
	s_tvalue[cnt] <span style=color:#f92672>=</span> (end <span style=color:#f92672>-</span>start);
}
</code></pre></div><p>However, if texture L1 cache is greater than the shared memory allowed for each thread block, then some reads in the second loop will trigger cache misses. But such misses are in fact cold misses, not misses caused after the texture L1 cache is completely filled up. One solution is increase iteration to a much larger number so that the first iteration will always fill up the texture L1 cache. Note that if you move the data
preheat out such as</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> cnt<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>; cnt <span style=color:#f92672>&lt;</span> it; cnt<span style=color:#f92672>++</span>) {
        tmp<span style=color:#f92672>=</span>tex1Dfetch(tex_ref, tmp);
}
</code></pre></div><p>The compiler can optimize this whole step out and thus nothing actually gets executed.</p><p>After we run the modified code, the result shows that the we the cache missed starts when we set our array size to 6145, indicating the texture L1 cache can hold 6144 ints, which is equivalent to 24 kb. We also notice that each miss is followed by 7 consecutive hits. This means the cache line size is 8 words(b = 32 bytes). The structure of the L1 TLB is shown below, notice there are 192 lines in each set:</p><table><thead><tr><th>Set1</th><th style=text-align:center>Set2</th><th style=text-align:right>Set3</th><th style=text-align:right>Set4</th></tr></thead><tbody><tr><td>1-8</td><td style=text-align:center>33-40</td><td style=text-align:right>65-72</td><td style=text-align:right>97-104</td></tr><tr><td>9-16</td><td style=text-align:center>41-48</td><td style=text-align:right>&mldr;</td><td style=text-align:right>&mldr;</td></tr><tr><td>17-24</td><td style=text-align:center>46-56</td><td style=text-align:right>&mldr;</td><td style=text-align:right>&mldr;</td></tr><tr><td>25-32</td><td style=text-align:center>57-64</td><td style=text-align:right>89-96</td><td style=text-align:right>121-128</td></tr><tr><td>129-136</td><td></td><td></td><td></td></tr><tr><td>&mldr;</td><td style=text-align:center>&mldr;</td><td style=text-align:right>&mldr;</td><td style=text-align:right>&mldr;</td></tr><tr><td>2969-2976</td><td style=text-align:center>3001-3008</td><td style=text-align:right>3033-3040</td><td style=text-align:right>3065-3072</td></tr></tbody></table><p>According to CUDA documentation, GK110 adds the ability for read-only data in global memory to be loaded through the same cache used by the texture pipeline via a standard pointer without the need to bind a texture beforehand and without the sizing limitations of standard textures. The read-only data cache is loaded by calling __ldg(const restricted * address). We modified the code used to test texture L1 cache. The basic logic remains the same. When the arrays size is set to 6144 integers no cache misses occur with stride set as 32 (s=32 bytes). As soon as we increased one more element in the array cache misses start occurring. This shows the read-only cache is 24kb. We then noticed that the misses occur in a group of either 4 or 8. We infer the cache line to be 32 bytes and the replacement policy is LRU, same as Maxwell. We we increase the array to include 6248 elements(6144+32<em>3+8, 6144 is the max capacity of the cache, 32 consecutive number in a set, 32</em>3 to cause cache miss in set1, set2, and set3, only need to include 8 more to cause cache miss in set4 since s=32bytes), no caches hits occur. Therefore, we infer the caches set number to be 4, each cache line is 32 bytes, and each set contains 192 cache lines, the same as the texture L1 cache. The memory mapping seems arbitrary because the hit and miss patterns didn&rsquo;t follow that of the texture L1 cache.</p></p></section></article></main><footer class=footer><span>&copy; 2021 <a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>