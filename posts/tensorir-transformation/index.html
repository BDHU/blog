<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin 🤘. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZLK2GHB055"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZLK2GHB055",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-ZLK2GHB055","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<link rel=canonical href=https://www.bodunhu.com/blog/posts/tensorir-transformation/><title>TensorIR Transformation</title></head><body><header id=banner><h2><a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></h2><nav><ul><li><a href=/blog/posts/ title=posts>Archive</a></li><li><a href=https://www.bodunhu.com/ title=about>About</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>TensorIR Transformation</h1><div><time>September 2, 2022</time></div></header><p>In the previous <a href=/blog/posts/dive-into-tensorir/>post</a>, we&rsquo;ve explored how to write primitive functions in TensorIR. Here, we will see how to transform TensorIR into other (potentially more performant) variants. The content is drived from the <a href=https://mlc.ai/summer22/>mlc</a> course taught by <a href=https://tqchen.com/>Tianqi Chen</a>.</p><h2 id=batched-bmm-relu>Batched BMM ReLu</h2><p>A batched matrix multiplication followed by a ReLu operation can be expressed using numpy as:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lnumpy_mm_relu_v2</span>(A: np<span style=color:#f92672>.</span>ndarray, B: np<span style=color:#f92672>.</span>ndarray, C: np<span style=color:#f92672>.</span>ndarray):
</span></span><span style=display:flex><span>    Y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>empty((<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> k <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                        Y[n, i, j] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>                    Y[n, i, j] <span style=color:#f92672>=</span> Y[n, i, j] <span style=color:#f92672>+</span> A[n, i, k] <span style=color:#f92672>*</span> B[n, k, j]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>                C[n, i, j] <span style=color:#f92672>=</span> max(Y[n, i, j], <span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p>Translating the numpy code into TensorIR we get:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyBmmRule</span>:
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bmm_relu</span>(A: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>],
</span></span><span style=display:flex><span>               W: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>],
</span></span><span style=display:flex><span>               Y: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>]):
</span></span><span style=display:flex><span>    T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;bmm_relu&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>    <span style=color:#75715e># we must to allocate the buffer here!</span>
</span></span><span style=display:flex><span>    Y_ <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>alloc_buffer([<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n, i, j, k <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M&#34;</span>):
</span></span><span style=display:flex><span>        vn <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>16</span>, n)
</span></span><span style=display:flex><span>        vi <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i)
</span></span><span style=display:flex><span>        vj <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j)
</span></span><span style=display:flex><span>        vk <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, k)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>init():
</span></span><span style=display:flex><span>          Y_[vn, vi, vj] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        Y_[vn, vi, vj] <span style=color:#f92672>+=</span> A[vn, vi, vk] <span style=color:#f92672>*</span> W[vn, vk, vj]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n, i, j <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;R&#34;</span>):
</span></span><span style=display:flex><span>        vn <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>16</span>, n)
</span></span><span style=display:flex><span>        vi <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i)
</span></span><span style=display:flex><span>        vj <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j)
</span></span><span style=display:flex><span>        Y[vn, vi, vj] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>max(Y_[vn, vi, vj], T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>))
</span></span></code></pre></div><p>Our ultimate goal is to transform the TensorIR above to the following form:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TargetModule</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bmm_relu</span>(A: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], B: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], C: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>]) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;bmm_relu&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        Y <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>alloc_buffer([<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>parallel(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i1, i2_0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> ax0_init <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>vectorized(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_init&#34;</span>):
</span></span><span style=display:flex><span>                        n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>                        j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0_init)
</span></span><span style=display:flex><span>                        Y[n, i, j] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> ax1_0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>32</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>for</span> ax1_1 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>unroll(<span style=color:#ae81ff>4</span>):
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>for</span> ax0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                            <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_update&#34;</span>):
</span></span><span style=display:flex><span>                                n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>                                j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0)
</span></span><span style=display:flex><span>                                k <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, ax1_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>+</span> ax1_1)
</span></span><span style=display:flex><span>                                Y[n, i, j] <span style=color:#f92672>=</span> Y[n, i, j] <span style=color:#f92672>+</span> A[n, i, k] <span style=color:#f92672>*</span> B[n, k, j]
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> i2_1 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>vectorized(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;R&#34;</span>):
</span></span><span style=display:flex><span>                        n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>                        j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> i2_1)
</span></span><span style=display:flex><span>                        C[n, i, j] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>max(Y[n, i, j], T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>))
</span></span></code></pre></div><p>Before we perform the transformation, let&rsquo;s understand what the transformed TensorIR is doing by looking at several loops here.</p><p>First, taking a look at</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> i1, i2_0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ax0_init <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>vectorized(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_init&#34;</span>):
</span></span><span style=display:flex><span>            n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>            j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0_init)
</span></span><span style=display:flex><span>            Y[n, i, j] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p>The code block is initializing the <code>Y</code> matrix to be 0. But it does so by initializing every 8 consecutive elements in each row of <code>Y</code> using a <em>vectorized</em> operation (which might be faster).</p><p>The next loop is bit tricky:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> ax1_0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>32</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ax1_1 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>unroll(<span style=color:#ae81ff>4</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> ax0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_update&#34;</span>):
</span></span><span style=display:flex><span>                n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>                j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0)
</span></span><span style=display:flex><span>                k <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, ax1_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>+</span> ax1_1)
</span></span><span style=display:flex><span>                Y[n, i, j] <span style=color:#f92672>=</span> Y[n, i, j] <span style=color:#f92672>+</span> A[n, i, k] <span style=color:#f92672>*</span> B[n, k, j]
</span></span></code></pre></div><p>This loop is actually performing the matrix multiplication of <code>A</code> and <code>B</code>. We mutiply a row in <code>A</code> with a column in <code>B</code> and sum up the result into a number.</p><p>Here, <code>i</code> is mapped to <code>i1</code>, which means we access <code>A</code> one row at a time.i <code>k = T.axis.reduce(128, ax1_0 * 4 + ax1_1)</code> means we access one row in matrix <code>A</code> and one column in matrix <code>B</code> sequentially duing mutiplying, while applying unrolling in hope for better access efficency (\(128 = 32\times 4))). <code>j = T.axis.spatial(128, i2_0 * 8 + ax0)</code> really just means accessing each column sequentially, nothing special.</p><h2 id=perform-transformation>Perform Transformation</h2><p>To perform tranformation on any TensorIP, it&rsquo;s very <strong>important to follow the steps listed below</strong>:</p><ol><li>Get block</li><li>Get loops</li><li>Organize loops by split, reorder, compute_at/reverse_compute_at</li><li>Decompose reduction</li><li>vectorize/unroll/parallel</li></ol><p>Applying step 1, 2, and 3, we first get the block from the original TensorIR:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sch <span style=color:#f92672>=</span> tvm<span style=color:#f92672>.</span>tir<span style=color:#f92672>.</span>Schedule(MyBmmRule)
</span></span><span style=display:flex><span><span style=color:#75715e># Step 1. Get blocks</span>
</span></span><span style=display:flex><span>block_M <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_block(<span style=color:#e6db74>&#34;M&#34;</span>, func_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bmm_relu&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 2. Get loops</span>
</span></span><span style=display:flex><span>n, i, j, k <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_loops(block_M)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 3. Organize loops</span>
</span></span><span style=display:flex><span>k0, k1 <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>split(k, factors<span style=color:#f92672>=</span>[<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>4</span>])
</span></span><span style=display:flex><span>j0, j1 <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>split(j, factors<span style=color:#f92672>=</span>[<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>8</span>])
</span></span></code></pre></div><p>The reason we split <code>k</code> and <code>j</code> in such a way is: we already mentioned <code>k</code> dimension is accessed sequentially but with unrolling (4) applied; when matrix <code>Y</code> is initialized, a vectorized operation (applied on 8 elements) is applied to dimension <code>j</code>, or every 8 elements in one row(TVM is row-major, therefore might be faster).</p><p>But the next question is: how do we reorder the spitted loop? I spent a lot of time trying to figure that out. Turns out the simplest way is to write out the implementation in numpy and proceed from there. Remember, we&rsquo;ve already splitted <code>k</code> and <code>j</code>, which are used during matrix multiplication, so our new matrix multipliation in numy would be:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> j0 <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> k0 <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>32</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> k1 <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> j1 <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                Y[i, <span style=color:#ae81ff>8</span><span style=color:#f92672>*</span>j0<span style=color:#f92672>+</span>j1] <span style=color:#f92672>+=</span> A[i, <span style=color:#ae81ff>4</span><span style=color:#f92672>*</span>k0 <span style=color:#f92672>+</span> k1] <span style=color:#f92672>*</span> B[<span style=color:#ae81ff>4</span><span style=color:#f92672>*</span>k0<span style=color:#f92672>+</span>k1, <span style=color:#ae81ff>8</span><span style=color:#f92672>*</span>j0<span style=color:#f92672>+</span>j1]
</span></span></code></pre></div><p>Because we move the the next column in <code>B</code> after traversing the previous column, we will put <code>j1</code> at the innermost loop. Therefore, the transformation for TensorIR would be:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sch<span style=color:#f92672>.</span>reorder(j0, k0, k1, j1)
</span></span></code></pre></div><p>We can print out the transformed TensorIR with <code>print(sch.mod.script())</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Module</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@tir.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bmm_relu</span>(A: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], B: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], C: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>]) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        tir<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;bmm_relu&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        Y <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>alloc_buffer([<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>parallel(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i, j_0, k_0, k_1, j_1 <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M&#34;</span>):
</span></span><span style=display:flex><span>                    vn, vi <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [n, i])
</span></span><span style=display:flex><span>                    vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> j_1)
</span></span><span style=display:flex><span>                    vk <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, k_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>+</span> k_1)
</span></span><span style=display:flex><span>                    tir<span style=color:#f92672>.</span>reads(A[vn, vi, vk], B[vn, vk, vj])
</span></span><span style=display:flex><span>                    tir<span style=color:#f92672>.</span>writes(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>init():
</span></span><span style=display:flex><span>                        Y[vn, vi, vj] <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                    Y[vn, vi, vj] <span style=color:#f92672>=</span> Y[vn, vi, vj] <span style=color:#f92672>+</span> A[vn, vi, vk] <span style=color:#f92672>*</span> B[vn, vk, vj]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> n, i, j <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;R&#34;</span>):
</span></span><span style=display:flex><span>                vn, vi, vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SSS&#34;</span>, [n, i, j])
</span></span><span style=display:flex><span>                tir<span style=color:#f92672>.</span>reads(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                tir<span style=color:#f92672>.</span>writes(C[vn, vi, vj])
</span></span><span style=display:flex><span>                C[vn, vi, vj] <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>max(Y[vn, vi, vj], tir<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>))
</span></span></code></pre></div><p>Now, we just need to move the ReLu operation (<code>for n, i, j in tir.grid(16, 128, 128):</code>) into the loop above:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>block_M <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_block(<span style=color:#e6db74>&#34;M&#34;</span>, func_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bmm_relu&#34;</span>)
</span></span><span style=display:flex><span>sch<span style=color:#f92672>.</span>reverse_compute_at(block_M, j0)
</span></span></code></pre></div><p>Step 4 involves seperating initialization and matrix multiplication, therefore we use <code>M_init = sch.decompose_reduction(block_M, k0)</code>, which results in:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Module</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@tir.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bmm_relu</span>(A: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], B: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], C: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>]) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># function attr dict</span>
</span></span><span style=display:flex><span>        tir<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;bmm_relu&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        <span style=color:#75715e># body</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># with tir.block(&#34;root&#34;)</span>
</span></span><span style=display:flex><span>        Y <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>alloc_buffer([<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>parallel(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i, j_0 <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> j_1_init <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_init&#34;</span>):
</span></span><span style=display:flex><span>                        vn, vi <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [n, i])
</span></span><span style=display:flex><span>                        vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> j_1_init)
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>reads()
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>writes(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                        Y[vn, vi, vj] <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> k_0, k_1, j_1 <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_update&#34;</span>):
</span></span><span style=display:flex><span>                        vn, vi <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [n, i])
</span></span><span style=display:flex><span>                        vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> j_1)
</span></span><span style=display:flex><span>                        vk <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, k_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>+</span> k_1)
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>reads(Y[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>writes(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                        Y[vn, vi, vj] <span style=color:#f92672>=</span> Y[vn, vi, vj] <span style=color:#f92672>+</span> A[vn, vi, vk] <span style=color:#f92672>*</span> B[vn, vk, vj]
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> ax0 <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;R&#34;</span>):
</span></span><span style=display:flex><span>                        vn, vi <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [n, i])
</span></span><span style=display:flex><span>                        vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0)
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>reads(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>writes(C[vn, vi, vj])
</span></span><span style=display:flex><span>                        C[vn, vi, vj] <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>max(Y[vn, vi, vj], tir<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>))
</span></span></code></pre></div><p>The final step is easy, just apply vectorize/parallel/unroll onto corresponding loop:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n, i, j_0, j_1_init <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_loops(M_init)
</span></span><span style=display:flex><span>sch<span style=color:#f92672>.</span>vectorize(j_1_init)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>n, i, j_0, i2_1 <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_loops(block_R)
</span></span><span style=display:flex><span>sch<span style=color:#f92672>.</span>vectorize(i2_1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>block_M_update <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_block(<span style=color:#e6db74>&#34;M_update&#34;</span>, func_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bmm_relu&#34;</span>)
</span></span><span style=display:flex><span>n, i, j_0, k_0, k_1, j_1 <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_loops(block_M_update)
</span></span></code></pre></div><p>Print out the final TensorIR to find out its final form ( ͡❛ ͜ʖ ͡❛).</p><div style=width:100% id=comment><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></article></main><footer id=footer><p>© 2023 Bodun Hu. All rights reserved.</p></footer></body></html>