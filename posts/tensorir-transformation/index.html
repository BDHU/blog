<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>TensorIR Transformation | std::bodun::blog</title><meta name=title content="TensorIR Transformation"><meta name=description content="In the previous post, we&rsquo;ve explored how to write primitive functions in TensorIR. Here, we will see how to transform TensorIR into other (potentially more performant) variants. The content is drived from the mlc course taught by Tianqi Chen.
Batched BMM ReLu A batched matrix multiplication followed by a ReLu operation can be expressed using numpy as:
def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray): Y = np.empty((16, 128, 128), dtype=&#34;float32&#34;) for n in range(16): for i in range(128): for j in range(128): for k in range(128): if k == 0: Y[n, i, j] = 0 Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j] for n in range(16): for i in range(128): for j in range(128): C[n, i, j] = max(Y[n, i, j], 0) Translating the numpy code into TensorIR we get:"><meta name=keywords content="mlsys,tvm,"><meta property="og:title" content="TensorIR Transformation"><meta property="og:description" content="In the previous post, we&rsquo;ve explored how to write primitive functions in TensorIR. Here, we will see how to transform TensorIR into other (potentially more performant) variants. The content is drived from the mlc course taught by Tianqi Chen.
Batched BMM ReLu A batched matrix multiplication followed by a ReLu operation can be expressed using numpy as:
def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray): Y = np.empty((16, 128, 128), dtype=&#34;float32&#34;) for n in range(16): for i in range(128): for j in range(128): for k in range(128): if k == 0: Y[n, i, j] = 0 Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j] for n in range(16): for i in range(128): for j in range(128): C[n, i, j] = max(Y[n, i, j], 0) Translating the numpy code into TensorIR we get:"><meta property="og:type" content="article"><meta property="og:url" content="https://www.bodunhu.com/blog/posts/tensorir-transformation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-30T00:00:00+00:00"><meta property="article:modified_time" content="2022-09-02T19:53:16-05:00"><meta property="og:site_name" content="std::bodun::blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="TensorIR Transformation"><meta name=twitter:description content="In the previous post, we&rsquo;ve explored how to write primitive functions in TensorIR. Here, we will see how to transform TensorIR into other (potentially more performant) variants. The content is drived from the mlc course taught by Tianqi Chen.
Batched BMM ReLu A batched matrix multiplication followed by a ReLu operation can be expressed using numpy as:
def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray): Y = np.empty((16, 128, 128), dtype=&#34;float32&#34;) for n in range(16): for i in range(128): for j in range(128): for k in range(128): if k == 0: Y[n, i, j] = 0 Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j] for n in range(16): for i in range(128): for j in range(128): C[n, i, j] = max(Y[n, i, j], 0) Translating the numpy code into TensorIR we get:"><meta name=twitter:site content="@https://twitter.com/BodunHu"><meta itemprop=name content="TensorIR Transformation"><meta itemprop=description content="In the previous post, we&rsquo;ve explored how to write primitive functions in TensorIR. Here, we will see how to transform TensorIR into other (potentially more performant) variants. The content is drived from the mlc course taught by Tianqi Chen.
Batched BMM ReLu A batched matrix multiplication followed by a ReLu operation can be expressed using numpy as:
def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray): Y = np.empty((16, 128, 128), dtype=&#34;float32&#34;) for n in range(16): for i in range(128): for j in range(128): for k in range(128): if k == 0: Y[n, i, j] = 0 Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j] for n in range(16): for i in range(128): for j in range(128): C[n, i, j] = max(Y[n, i, j], 0) Translating the numpy code into TensorIR we get:"><meta itemprop=datePublished content="2022-08-30T00:00:00+00:00"><meta itemprop=dateModified content="2022-09-02T19:53:16-05:00"><meta itemprop=wordCount content="1334"><meta itemprop=keywords content="mlsys,tvm,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style></head><body><header><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><a href=/blog/ class=title><h2>std::bodun::blog</h2></a><nav><a href=/blog/>Home</a>
<a href=/posts/>Archive</a>
<a href=https://www.bodunhu.com/>About</a></nav></header><main><h1>TensorIR Transformation</h1><p><i><time datetime=2022-08-30 pubdate>30 Aug, 2022</time></i></p><content><p>In the previous <a href=/blog/posts/dive-into-tensorir/>post</a>, we&rsquo;ve explored how to write primitive functions in TensorIR. Here, we will see how to transform TensorIR into other (potentially more performant) variants. The content is drived from the <a href=https://mlc.ai/summer22/>mlc</a> course taught by <a href=https://tqchen.com/>Tianqi Chen</a>.</p><h2 id=batched-bmm-relu>Batched BMM ReLu</h2><p>A batched matrix multiplication followed by a ReLu operation can be expressed using numpy as:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lnumpy_mm_relu_v2</span>(A: np<span style=color:#f92672>.</span>ndarray, B: np<span style=color:#f92672>.</span>ndarray, C: np<span style=color:#f92672>.</span>ndarray):
</span></span><span style=display:flex><span>    Y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>empty((<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> k <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                        Y[n, i, j] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>                    Y[n, i, j] <span style=color:#f92672>=</span> Y[n, i, j] <span style=color:#f92672>+</span> A[n, i, k] <span style=color:#f92672>*</span> B[n, k, j]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>                C[n, i, j] <span style=color:#f92672>=</span> max(Y[n, i, j], <span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p>Translating the numpy code into TensorIR we get:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyBmmRule</span>:
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bmm_relu</span>(A: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>],
</span></span><span style=display:flex><span>               W: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>],
</span></span><span style=display:flex><span>               Y: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>]):
</span></span><span style=display:flex><span>    T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;bmm_relu&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>    <span style=color:#75715e># we must to allocate the buffer here!</span>
</span></span><span style=display:flex><span>    Y_ <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>alloc_buffer([<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n, i, j, k <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M&#34;</span>):
</span></span><span style=display:flex><span>        vn <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>16</span>, n)
</span></span><span style=display:flex><span>        vi <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i)
</span></span><span style=display:flex><span>        vj <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j)
</span></span><span style=display:flex><span>        vk <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, k)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>init():
</span></span><span style=display:flex><span>          Y_[vn, vi, vj] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        Y_[vn, vi, vj] <span style=color:#f92672>+=</span> A[vn, vi, vk] <span style=color:#f92672>*</span> W[vn, vk, vj]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n, i, j <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;R&#34;</span>):
</span></span><span style=display:flex><span>        vn <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>16</span>, n)
</span></span><span style=display:flex><span>        vi <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i)
</span></span><span style=display:flex><span>        vj <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j)
</span></span><span style=display:flex><span>        Y[vn, vi, vj] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>max(Y_[vn, vi, vj], T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>))
</span></span></code></pre></div><p>Our ultimate goal is to transform the TensorIR above to the following form:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TargetModule</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bmm_relu</span>(A: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], B: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], C: T<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>]) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;bmm_relu&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        Y <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>alloc_buffer([<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>parallel(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i1, i2_0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> ax0_init <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>vectorized(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_init&#34;</span>):
</span></span><span style=display:flex><span>                        n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>                        j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0_init)
</span></span><span style=display:flex><span>                        Y[n, i, j] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> ax1_0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>32</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>for</span> ax1_1 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>unroll(<span style=color:#ae81ff>4</span>):
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>for</span> ax0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                            <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_update&#34;</span>):
</span></span><span style=display:flex><span>                                n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>                                j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0)
</span></span><span style=display:flex><span>                                k <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, ax1_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>+</span> ax1_1)
</span></span><span style=display:flex><span>                                Y[n, i, j] <span style=color:#f92672>=</span> Y[n, i, j] <span style=color:#f92672>+</span> A[n, i, k] <span style=color:#f92672>*</span> B[n, k, j]
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> i2_1 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>vectorized(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;R&#34;</span>):
</span></span><span style=display:flex><span>                        n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>                        j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> i2_1)
</span></span><span style=display:flex><span>                        C[n, i, j] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>max(Y[n, i, j], T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>))
</span></span></code></pre></div><p>Before we perform the transformation, let&rsquo;s understand what the transformed TensorIR is doing by looking at several loops here.</p><p>First, taking a look at</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> i1, i2_0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ax0_init <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>vectorized(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_init&#34;</span>):
</span></span><span style=display:flex><span>            n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>            j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0_init)
</span></span><span style=display:flex><span>            Y[n, i, j] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p>The code block is initializing the <code>Y</code> matrix to be 0. But it does so by initializing every 8 consecutive elements in each row of <code>Y</code> using a <em>vectorized</em> operation (which might be faster).</p><p>The next loop is bit tricky:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> ax1_0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>32</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ax1_1 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>unroll(<span style=color:#ae81ff>4</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> ax0 <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_update&#34;</span>):
</span></span><span style=display:flex><span>                n, i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [i0, i1])
</span></span><span style=display:flex><span>                j <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, i2_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0)
</span></span><span style=display:flex><span>                k <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, ax1_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>+</span> ax1_1)
</span></span><span style=display:flex><span>                Y[n, i, j] <span style=color:#f92672>=</span> Y[n, i, j] <span style=color:#f92672>+</span> A[n, i, k] <span style=color:#f92672>*</span> B[n, k, j]
</span></span></code></pre></div><p>This loop is actually performing the matrix multiplication of <code>A</code> and <code>B</code>. We mutiply a row in <code>A</code> with a column in <code>B</code> and sum up the result into a number.</p><p>Here, <code>i</code> is mapped to <code>i1</code>, which means we access <code>A</code> one row at a time.i <code>k = T.axis.reduce(128, ax1_0 * 4 + ax1_1)</code> means we access one row in matrix <code>A</code> and one column in matrix <code>B</code> sequentially duing mutiplying, while applying unrolling in hope for better access efficency (\(128 = 32\times 4))). <code>j = T.axis.spatial(128, i2_0 * 8 + ax0)</code> really just means accessing each column sequentially, nothing special.</p><h2 id=perform-transformation>Perform Transformation</h2><p>To perform tranformation on any TensorIP, it&rsquo;s very <strong>important to follow the steps listed below</strong>:</p><ol><li>Get block</li><li>Get loops</li><li>Organize loops by split, reorder, compute_at/reverse_compute_at</li><li>Decompose reduction</li><li>vectorize/unroll/parallel</li></ol><p>Applying step 1, 2, and 3, we first get the block from the original TensorIR:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sch <span style=color:#f92672>=</span> tvm<span style=color:#f92672>.</span>tir<span style=color:#f92672>.</span>Schedule(MyBmmRule)
</span></span><span style=display:flex><span><span style=color:#75715e># Step 1. Get blocks</span>
</span></span><span style=display:flex><span>block_M <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_block(<span style=color:#e6db74>&#34;M&#34;</span>, func_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bmm_relu&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 2. Get loops</span>
</span></span><span style=display:flex><span>n, i, j, k <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_loops(block_M)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 3. Organize loops</span>
</span></span><span style=display:flex><span>k0, k1 <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>split(k, factors<span style=color:#f92672>=</span>[<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>4</span>])
</span></span><span style=display:flex><span>j0, j1 <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>split(j, factors<span style=color:#f92672>=</span>[<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>8</span>])
</span></span></code></pre></div><p>The reason we split <code>k</code> and <code>j</code> in such a way is: we already mentioned <code>k</code> dimension is accessed sequentially but with unrolling (4) applied; when matrix <code>Y</code> is initialized, a vectorized operation (applied on 8 elements) is applied to dimension <code>j</code>, or every 8 elements in one row(TVM is row-major, therefore might be faster).</p><p>But the next question is: how do we reorder the spitted loop? I spent a lot of time trying to figure that out. Turns out the simplest way is to write out the implementation in numpy and proceed from there. Remember, we&rsquo;ve already splitted <code>k</code> and <code>j</code>, which are used during matrix multiplication, so our new matrix multipliation in numy would be:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> j0 <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> k0 <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>32</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> k1 <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> j1 <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                Y[i, <span style=color:#ae81ff>8</span><span style=color:#f92672>*</span>j0<span style=color:#f92672>+</span>j1] <span style=color:#f92672>+=</span> A[i, <span style=color:#ae81ff>4</span><span style=color:#f92672>*</span>k0 <span style=color:#f92672>+</span> k1] <span style=color:#f92672>*</span> B[<span style=color:#ae81ff>4</span><span style=color:#f92672>*</span>k0<span style=color:#f92672>+</span>k1, <span style=color:#ae81ff>8</span><span style=color:#f92672>*</span>j0<span style=color:#f92672>+</span>j1]
</span></span></code></pre></div><p>Because we move the the next column in <code>B</code> after traversing the previous column, we will put <code>j1</code> at the innermost loop. Therefore, the transformation for TensorIR would be:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sch<span style=color:#f92672>.</span>reorder(j0, k0, k1, j1)
</span></span></code></pre></div><p>We can print out the transformed TensorIR with <code>print(sch.mod.script())</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Module</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@tir.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bmm_relu</span>(A: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], B: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], C: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>]) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        tir<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;bmm_relu&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        Y <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>alloc_buffer([<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>parallel(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i, j_0, k_0, k_1, j_1 <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M&#34;</span>):
</span></span><span style=display:flex><span>                    vn, vi <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [n, i])
</span></span><span style=display:flex><span>                    vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> j_1)
</span></span><span style=display:flex><span>                    vk <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, k_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>+</span> k_1)
</span></span><span style=display:flex><span>                    tir<span style=color:#f92672>.</span>reads(A[vn, vi, vk], B[vn, vk, vj])
</span></span><span style=display:flex><span>                    tir<span style=color:#f92672>.</span>writes(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>init():
</span></span><span style=display:flex><span>                        Y[vn, vi, vj] <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                    Y[vn, vi, vj] <span style=color:#f92672>=</span> Y[vn, vi, vj] <span style=color:#f92672>+</span> A[vn, vi, vk] <span style=color:#f92672>*</span> B[vn, vk, vj]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> n, i, j <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;R&#34;</span>):
</span></span><span style=display:flex><span>                vn, vi, vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SSS&#34;</span>, [n, i, j])
</span></span><span style=display:flex><span>                tir<span style=color:#f92672>.</span>reads(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                tir<span style=color:#f92672>.</span>writes(C[vn, vi, vj])
</span></span><span style=display:flex><span>                C[vn, vi, vj] <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>max(Y[vn, vi, vj], tir<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>))
</span></span></code></pre></div><p>Now, we just need to move the ReLu operation (<code>for n, i, j in tir.grid(16, 128, 128):</code>) into the loop above:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>block_M <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_block(<span style=color:#e6db74>&#34;M&#34;</span>, func_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bmm_relu&#34;</span>)
</span></span><span style=display:flex><span>sch<span style=color:#f92672>.</span>reverse_compute_at(block_M, j0)
</span></span></code></pre></div><p>Step 4 involves seperating initialization and matrix multiplication, therefore we use <code>M_init = sch.decompose_reduction(block_M, k0)</code>, which results in:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Module</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@tir.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bmm_relu</span>(A: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], B: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>], C: tir<span style=color:#f92672>.</span>Buffer[(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>), <span style=color:#e6db74>&#34;float32&#34;</span>]) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># function attr dict</span>
</span></span><span style=display:flex><span>        tir<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;bmm_relu&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        <span style=color:#75715e># body</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># with tir.block(&#34;root&#34;)</span>
</span></span><span style=display:flex><span>        Y <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>alloc_buffer([<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>parallel(<span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i, j_0 <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> j_1_init <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_init&#34;</span>):
</span></span><span style=display:flex><span>                        vn, vi <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [n, i])
</span></span><span style=display:flex><span>                        vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> j_1_init)
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>reads()
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>writes(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                        Y[vn, vi, vj] <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> k_0, k_1, j_1 <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>grid(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;M_update&#34;</span>):
</span></span><span style=display:flex><span>                        vn, vi <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [n, i])
</span></span><span style=display:flex><span>                        vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> j_1)
</span></span><span style=display:flex><span>                        vk <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(<span style=color:#ae81ff>128</span>, k_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>+</span> k_1)
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>reads(Y[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>writes(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                        Y[vn, vi, vj] <span style=color:#f92672>=</span> Y[vn, vi, vj] <span style=color:#f92672>+</span> A[vn, vi, vk] <span style=color:#f92672>*</span> B[vn, vk, vj]
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> ax0 <span style=color:#f92672>in</span> tir<span style=color:#f92672>.</span>serial(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> tir<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;R&#34;</span>):
</span></span><span style=display:flex><span>                        vn, vi <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>remap(<span style=color:#e6db74>&#34;SS&#34;</span>, [n, i])
</span></span><span style=display:flex><span>                        vj <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(<span style=color:#ae81ff>128</span>, j_0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>+</span> ax0)
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>reads(Y[vn, vi, vj])
</span></span><span style=display:flex><span>                        tir<span style=color:#f92672>.</span>writes(C[vn, vi, vj])
</span></span><span style=display:flex><span>                        C[vn, vi, vj] <span style=color:#f92672>=</span> tir<span style=color:#f92672>.</span>max(Y[vn, vi, vj], tir<span style=color:#f92672>.</span>float32(<span style=color:#ae81ff>0</span>))
</span></span></code></pre></div><p>The final step is easy, just apply vectorize/parallel/unroll onto corresponding loop:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n, i, j_0, j_1_init <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_loops(M_init)
</span></span><span style=display:flex><span>sch<span style=color:#f92672>.</span>vectorize(j_1_init)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>n, i, j_0, i2_1 <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_loops(block_R)
</span></span><span style=display:flex><span>sch<span style=color:#f92672>.</span>vectorize(i2_1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>block_M_update <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_block(<span style=color:#e6db74>&#34;M_update&#34;</span>, func_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bmm_relu&#34;</span>)
</span></span><span style=display:flex><span>n, i, j_0, k_0, k_1, j_1 <span style=color:#f92672>=</span> sch<span style=color:#f92672>.</span>get_loops(block_M_update)
</span></span></code></pre></div><p>Print out the final TensorIR to find out its final form ( ͡❛ ͜ʖ ͡❛).</p></content><p><a href=https://www.bodunhu.com/blog/tags/mlsys/>#mlsys</a>
<a href=https://www.bodunhu.com/blog/tags/tvm/>#tvm</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>