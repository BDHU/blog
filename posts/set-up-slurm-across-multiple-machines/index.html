<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><title>Set up Slurm across Multiple Machines</title></head><body><header id=return><h2></h2><a href=https://www.bodunhu.com/blog/><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" style="vertical-align:-.125em" width="1em" height="1em" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m11 5-7 7 7 7m-7-7h16"/></svg>&nbspHome
       </a></header><main id=content><article><header id=post-header><h1>Set up Slurm across Multiple Machines</h1><div><time>Updated May 1, 2022</time></div></header><p>To install <a href=https://slurm.schedmd.com/documentation.html>Slurm</a>, we need to have admin access to the machine. This post explains how I got Slurm running in multiple Linux servers. All servers are running on Ubuntu 18.04 LTS.</p><h2 id=setup-munge>Setup Munge</h2><p>First, we need to make sure the clocks, users and groups (UIDs and GIDs) are synchronized across the cluster. We need to create two users: <code>slurm</code> and <code>munge</code> across all servers.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ <span style=color:#111>export</span> <span style=color:#111>MUNGEUSER</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3456</span>
</span></span><span style=display:flex><span>$ groupadd -g <span style=color:#111>$MUNGEUSER</span> munge
</span></span><span style=display:flex><span>$ useradd  -m -c <span style=color:#d88200>&#34;MUNGE Uid &#39;N&#39; Gid Emporium&#34;</span> -d /var/lib/munge -u <span style=color:#111>$MUNGEUSER</span> -g munge  -s /sbin/nologin munge
</span></span><span style=display:flex><span>$ <span style=color:#111>export</span> <span style=color:#111>SLURMUSER</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3457</span>
</span></span><span style=display:flex><span>$ groupadd -g <span style=color:#111>$SLURMUSER</span> slurm
</span></span><span style=display:flex><span>$ useradd  -m -c <span style=color:#d88200>&#34;SLURM workload manager&#34;</span> -d /var/lib/slurm -u <span style=color:#111>$SLURMUSER</span> -g slurm  -s /bin/bash slurm
</span></span></code></pre></div><p>Then, we install <a href=https://linux.die.net/man/7/munge>Munge</a> for authentication:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ apt install munge libmunge2 libmunge-dev
</span></span></code></pre></div><p>To test if munge is installed successfully:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ munge -n <span style=color:#111>|</span> unmunge <span style=color:#111>|</span> grep STATUS
</span></span><span style=display:flex><span>STATUS:           Success <span style=color:#f92672>(</span>0<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Next, we create a munge authentication key on one of the servers:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ /usr/sbin/create-munge-key
</span></span></code></pre></div><p>After we generate munge authentication key, we copy the key <code>/etc/munge/munge.key</code> on that server to all other servers (overwrite the <code>/etc/munge/munge.key</code> on all other servers).</p><p>We need to setup the rights for munge accordingly on every server:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ chown -R munge: /etc/munge/ /var/log/munge/ /var/lib/munge/ /run/munge/
</span></span><span style=display:flex><span>$ chmod <span style=color:#ae81ff>0700</span> /etc/munge/ /var/log/munge/ /var/lib/munge/
</span></span><span style=display:flex><span>$ chmod <span style=color:#ae81ff>0755</span> /run/munge/
</span></span></code></pre></div><p>Then, we enable and start the munge service with (remember to not use sudo when running munge):</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ systemctl <span style=color:#111>enable</span> munge
</span></span><span style=display:flex><span>$ systemctl start munge
</span></span></code></pre></div><p>You can then test whether munge works properly by executing:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>munge -n                    <span style=color:#75715e># Generate a credential on stdout</span>
</span></span><span style=display:flex><span>munge -n <span style=color:#111>|</span> unmunge          <span style=color:#75715e># Displays information about the MUNGE key  </span>
</span></span><span style=display:flex><span>munge -n <span style=color:#111>|</span> ssh somehost unmunge
</span></span></code></pre></div><p>If everything is setup properly, you shouldn&rsquo;t see any error messages.</p><h2 id=setup-slurm>Setup Slurm</h2><p>Use <code>apt</code> to install slurm in Ubuntu systems (make sure all nodes have the same slurm versions):</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ apt install slurm-wlm
</span></span></code></pre></div><p>Next, we need to configure slurm. Since we used package manager to install slurm, the version is lower than the latest release. Thus, it&rsquo;s preferably to not use the official <a href=https://slurm.schedmd.com/configurator.html>Slurm Configuration Tool</a>. Instead, we can find the corresponding version&rsquo;s configuration tool at <code>/usr/share/doc/slurmctld/slurm-wlm-configurator.html</code> (we just To install Slurm, we need to have admin access to the machine. This post explains how I got Slurm running in multiple Linux servers. All servers are running on Ubuntu 18.04.5 LTS.
Setup Munge</p><p>First, we need to make sure the clocks, users and groups (UIDs and GIDs) are synchronized across the cluster. We need to create two users: slurm and munge across all servers.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ <span style=color:#111>export</span> <span style=color:#111>MUNGEUSER</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3456</span>
</span></span><span style=display:flex><span>$ groupadd -g <span style=color:#111>$MUNGEUSER</span> munge
</span></span><span style=display:flex><span>$ useradd  -m -c <span style=color:#d88200>&#34;MUNGE Uid &#39;N&#39; Gid Emporium&#34;</span> -d /var/lib/munge -u <span style=color:#111>$MUNGEUSER</span> -g munge  -s /sbin/nologin munge
</span></span><span style=display:flex><span>$ <span style=color:#111>export</span> <span style=color:#111>SLURMUSER</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3457</span>
</span></span><span style=display:flex><span>$ groupadd -g <span style=color:#111>$SLURMUSER</span> slurm
</span></span><span style=display:flex><span>$ useradd  -m -c <span style=color:#d88200>&#34;SLURM workload manager&#34;</span> -d /var/lib/slurm -u <span style=color:#111>$SLURMUSER</span> -g slurm  -s /bin/bash slurm
</span></span></code></pre></div><p>Then, we install Munge for authentication:</p><p>$ apt install munge libmunge2 libmunge-dev</p><p>To test if munge is installed successfully:</p><p>$ munge -n | unmunge | grep STATUS
STATUS: Success (0)</p><p>Next, we create a munge authentication key on one of the servers:</p><p>$ /usr/sbin/create-munge-key</p><p>After we generate munge authentication key, we copy the key /etc/munge/munge.key on that server to all other servers (overwrite the /etc/munge/ can be found <a href=https://wiki.archlinux.org/title/Slurm>here</a>.</p><p>After filling up the required fields in the form, we copy the generated file into <code>/etc/slurm-llnl/slurm.conf</code> all all nodes. Then, you can execute <code>sinfo</code> to check all nodes status. You can also launch jobs to see if it actually works, for example:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>srun -N2 -l /bin/hostname
</span></span></code></pre></div><p>This should print out the hostname for all the nodes in the cluster.</p><h2 id=add-gpu-support>Add GPU support</h2><p>To add GPU support, we first create a file <code>gres.conf</code> in <code>/etc/slurm-llnl/</code>. Here is an example on one node:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Name=gpu File=/dev/nvidia0
</span></span><span style=display:flex><span>Name=gpu File=/dev/nvidia1
</span></span><span style=display:flex><span>Name=gpu File=/dev/nvidia2
</span></span></code></pre></div><p>Then, we add <code>GresTypes=gpu</code> into <code>/etc/slurm-llnl/slurm.conf</code>. Next, we add the GPU information to <code>slurm.conf</code>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NodeName=node1 Gres=gpu:3 State=UNKNOWN
</span></span></code></pre></div><div style=width:100% id=comment><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></article></main><footer id=footer><p>Â© 2022 Bodun Hu. All rights reserved.</p></footer></body></html>