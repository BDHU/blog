<!doctype html><html><head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="std::bodun::blog. Personal Blog of Bodun (Edward) Hu. CS PhD student at University of Texas at Austin. Operating systems, network, heterogeneity, MLSys, anything system. UTCS">
<link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico>
<link rel=stylesheet href=/blog/css/style.min.css>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script>
<script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-108144808-1','auto'),ga('send','pageview'))</script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-108144808-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<title>Set up Slurm across Multiple Machines</title>
</head>
<body><header id=banner>
<h2><a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></h2>
<nav>
<ul>
<li>
<a href=/blog/posts/ title=posts>archive</a>
</li><li>
<a href=https://www.bodunhu.com/ title=about>about</a>
</li>
</ul>
</nav>
</header>
<main id=content>
<article>
<header id=post-header>
<h1>Set up Slurm across Multiple Machines</h1>
<div>
<time>Nov 16, 2021</time>
</div>
</header><p>To install <a href=https://slurm.schedmd.com/documentation.html>Slurm</a>, we need to have admin access to the machine. This post explains how I got Slurm running in multiple Linux servers. All servers are running on Ubuntu 18.04 LTS.</p>
<h2 id=setup-munge>Setup Munge</h2>
<p>First, we need to make sure the clocks, users and groups (UIDs and GIDs) are synchronized across the cluster. We need to create two users: <code>slurm</code> and <code>munge</code> across all servers.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ <span class=nb>export</span> <span class=nv>MUNGEUSER</span><span class=o>=</span><span class=m>3456</span>
$ groupadd -g <span class=nv>$MUNGEUSER</span> munge
$ useradd  -m -c <span class=s2>&#34;MUNGE Uid &#39;N&#39; Gid Emporium&#34;</span> -d /var/lib/munge -u <span class=nv>$MUNGEUSER</span> -g munge  -s /sbin/nologin munge
$ <span class=nb>export</span> <span class=nv>SLURMUSER</span><span class=o>=</span><span class=m>3457</span>
$ groupadd -g <span class=nv>$SLURMUSER</span> slurm
$ useradd  -m -c <span class=s2>&#34;SLURM workload manager&#34;</span> -d /var/lib/slurm -u <span class=nv>$SLURMUSER</span> -g slurm  -s /bin/bash slurm
</code></pre></div><p>Then, we install <a href=https://linux.die.net/man/7/munge>Munge</a> for authentication:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ apt install munge libmunge2 libmunge-dev
</code></pre></div><p>To test if munge is installed successfully:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ munge -n <span class=p>|</span> unmunge <span class=p>|</span> grep STATUS
STATUS:           Success <span class=o>(</span>0<span class=o>)</span>
</code></pre></div><p>Next, we create a munge authentication key on one of the servers:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ /usr/sbin/create-munge-key
</code></pre></div><p>After we generate munge authentication key, we copy the key <code>/etc/munge/munge.key</code> on that server to all other servers (overwrite the <code>/etc/munge/munge.key</code> on all other servers).</p>
<p>We need to setup the rights for munge accordingly on every server:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ chown -R munge: /etc/munge/ /var/log/munge/ /var/lib/munge/ /run/munge/
$ chmod <span class=m>0700</span> /etc/munge/ /var/log/munge/ /var/lib/munge/
$ chmod <span class=m>0755</span> /run/munge/
</code></pre></div><p>Then, we enable and start the munge service with (remember to not use sudo when running munge):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ systemctl <span class=nb>enable</span> munge
$ systemctl start munge
</code></pre></div><p>You can then test whether munge works properly by executing:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>munge -n                    <span class=c1># Generate a credential on stdout</span>
munge -n <span class=p>|</span> unmunge          <span class=c1># Displays information about the MUNGE key  </span>
munge -n <span class=p>|</span> ssh somehost unmunge
</code></pre></div><p>If everything is setup properly, you shouldn&rsquo;t see any error messages.</p>
<h2 id=setup-slurm>Setup Slurm</h2>
<p>Use <code>apt</code> to install slurm in Ubuntu systems (make sure all nodes have the same slurm versions):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ apt install slurm-wlm
</code></pre></div><p>Next, we need to configure slurm. Since we used package manager to install slurm, the version is lower than the latest release. Thus, it&rsquo;s preferably to not use the official <a href=https://slurm.schedmd.com/configurator.html>Slurm Configuration Tool</a>. Instead, we can find the corresponding version&rsquo;s configuration tool at <code>/usr/share/doc/slurmctld/slurm-wlm-configurator.html</code> (we just To install Slurm, we need to have admin access to the machine. This post explains how I got Slurm running in multiple Linux servers. All servers are running on Ubuntu 18.04.5 LTS.
Setup Munge</p>
<p>First, we need to make sure the clocks, users and groups (UIDs and GIDs) are synchronized across the cluster. We need to create two users: slurm and munge across all servers.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>$ <span class=nb>export</span> <span class=nv>MUNGEUSER</span><span class=o>=</span><span class=m>3456</span>
$ groupadd -g <span class=nv>$MUNGEUSER</span> munge
$ useradd  -m -c <span class=s2>&#34;MUNGE Uid &#39;N&#39; Gid Emporium&#34;</span> -d /var/lib/munge -u <span class=nv>$MUNGEUSER</span> -g munge  -s /sbin/nologin munge
$ <span class=nb>export</span> <span class=nv>SLURMUSER</span><span class=o>=</span><span class=m>3457</span>
$ groupadd -g <span class=nv>$SLURMUSER</span> slurm
$ useradd  -m -c <span class=s2>&#34;SLURM workload manager&#34;</span> -d /var/lib/slurm -u <span class=nv>$SLURMUSER</span> -g slurm  -s /bin/bash slurm
</code></pre></div><p>Then, we install Munge for authentication:</p>
<p>$ apt install munge libmunge2 libmunge-dev</p>
<p>To test if munge is installed successfully:</p>
<p>$ munge -n | unmunge | grep STATUS
STATUS: Success (0)</p>
<p>Next, we create a munge authentication key on one of the servers:</p>
<p>$ /usr/sbin/create-munge-key</p>
<p>After we generate munge authentication key, we copy the key /etc/munge/munge.key on that server to all other servers (overwrite the /etc/munge/ can be found <a href=https://wiki.archlinux.org/title/Slurm>here</a>.</p>
<p>After filling up the required fields in the form, we copy the generated file into <code>/etc/slurm-llnl/slurm.conf</code> all all nodes. Then, you can execute <code>sinfo</code> to check all nodes status. You can also launch jobs to see if it actually works, for example:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>srun -N2 -l /bin/hostname
</code></pre></div><p>This should print out the hostname for all the nodes in the cluster.</p>
<h2 id=add-gpu-support>Add GPU support</h2>
<p>To add GPU support, we first create a file <code>gres.conf</code> in <code>/etc/slurm-llnl/</code>. Here is an example on one node:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text>Name=gpu File=/dev/nvidia0
Name=gpu File=/dev/nvidia1
Name=gpu File=/dev/nvidia2
</code></pre></div><p>Then, we add <code>GresTypes=gpu</code> into <code>/etc/slurm-llnl/slurm.conf</code>. Next, we add the GPU information to <code>slurm.conf</code>:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text>NodeName=node1 Gres=gpu:3 State=UNKNOWN
</code></pre></div>
<div style=width:100% id=comment>
<script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=preferred-color-scheme crossorigin=anonymous async></script>
</div>
</article>
</main><footer id=footer>
<p>Â© 2022 Bodun Hu. All rights reserved.</p>
</footer>
</body>
</html>