<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Set up Slurm across Multiple Machines</title><link rel=stylesheet href=https://www.bodunhu.com/blog/css/colors-preference.min.79f305c614b4ceb62b5bba50782f7c04f68b58d85622e7a8bf1c71f396386716.css><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><header id=header><h1><a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></h1><p>PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML.</p></header><div id=page><div id=sidebar><nav><ul class=nav><li><a href=/blog/posts/><span>Archive</span>&nbsp;</a></li><li><a href=https://www.bodunhu.com/><span>About</span>&nbsp;</a></li><li><a href=/blog/index.xml><span>Feed</span>&nbsp;</a></li></ul></nav></div><div id=content><article class=post><h1><a href=https://www.bodunhu.com/blog/posts/set-up-slurm-across-multiple-machines/>Set up Slurm across Multiple Machines</a></h1><div class=post-content><p>To install <a href=https://slurm.schedmd.com/documentation.html>Slurm</a>, we need to have admin access to the machine. This post explains how I got Slurm running in multiple Linux servers. All servers are running on Ubuntu 18.04 LTS.</p><h2 id=setup-munge>Setup Munge</h2><p>First, we need to make sure the clocks, users and groups (UIDs and GIDs) are synchronized across the cluster. We need to create two users: <code>slurm</code> and <code>munge</code> across all servers.</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ <span style=color:#0087ff>export</span> <span style=color:#0087ff>MUNGEUSER</span>=<span style=color:#00afaf>3456</span>
</span></span><span style=display:flex><span>$ groupadd -g <span style=color:#0087ff>$MUNGEUSER</span> munge
</span></span><span style=display:flex><span>$ useradd  -m -c <span style=color:#00afaf>&#34;MUNGE Uid &#39;N&#39; Gid Emporium&#34;</span> -d /var/lib/munge -u <span style=color:#0087ff>$MUNGEUSER</span> -g munge  -s /sbin/nologin munge
</span></span><span style=display:flex><span>$ <span style=color:#0087ff>export</span> <span style=color:#0087ff>SLURMUSER</span>=<span style=color:#00afaf>3457</span>
</span></span><span style=display:flex><span>$ groupadd -g <span style=color:#0087ff>$SLURMUSER</span> slurm
</span></span><span style=display:flex><span>$ useradd  -m -c <span style=color:#00afaf>&#34;SLURM workload manager&#34;</span> -d /var/lib/slurm -u <span style=color:#0087ff>$SLURMUSER</span> -g slurm  -s /bin/bash slurm
</span></span></code></pre></div><p>Then, we install <a href=https://linux.die.net/man/7/munge>Munge</a> for authentication:</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ apt install munge libmunge2 libmunge-dev
</span></span></code></pre></div><p>To test if munge is installed successfully:</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ munge -n | unmunge | grep STATUS
</span></span><span style=display:flex><span>STATUS:           Success (0)
</span></span></code></pre></div><p>Next, we create a munge authentication key on one of the servers:</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ /usr/sbin/create-munge-key
</span></span></code></pre></div><p>After we generate munge authentication key, we copy the key <code>/etc/munge/munge.key</code> on that server to all other servers (overwrite the <code>/etc/munge/munge.key</code> on all other servers).</p><p>We need to setup the rights for munge accordingly on every server:</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ chown -R munge: /etc/munge/ /var/log/munge/ /var/lib/munge/ /run/munge/
</span></span><span style=display:flex><span>$ chmod <span style=color:#00afaf>0700</span> /etc/munge/ /var/log/munge/ /var/lib/munge/
</span></span><span style=display:flex><span>$ chmod <span style=color:#00afaf>0755</span> /run/munge/
</span></span></code></pre></div><p>Then, we enable and start the munge service with (remember to not use sudo when running munge):</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ systemctl <span style=color:#0087ff>enable</span> munge
</span></span><span style=display:flex><span>$ systemctl start munge
</span></span></code></pre></div><p>You can then test whether munge works properly by executing:</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>munge -n                    <span style=color:#4e4e4e># Generate a credential on stdout</span>
</span></span><span style=display:flex><span>munge -n | unmunge          <span style=color:#4e4e4e># Displays information about the MUNGE key  </span>
</span></span><span style=display:flex><span>munge -n | ssh somehost unmunge
</span></span></code></pre></div><p>If everything is setup properly, you shouldn&rsquo;t see any error messages.</p><h2 id=setup-slurm>Setup Slurm</h2><p>Use <code>apt</code> to install slurm in Ubuntu systems (make sure all nodes have the same slurm versions):</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ apt install slurm-wlm
</span></span></code></pre></div><p>Next, we need to configure slurm. Since we used package manager to install slurm, the version is lower than the latest release. Thus, it&rsquo;s preferably to not use the official <a href=https://slurm.schedmd.com/configurator.html>Slurm Configuration Tool</a>. Instead, we can find the corresponding version&rsquo;s configuration tool at <code>/usr/share/doc/slurmctld/slurm-wlm-configurator.html</code> (we just To install Slurm, we need to have admin access to the machine. This post explains how I got Slurm running in multiple Linux servers. All servers are running on Ubuntu 18.04.5 LTS.
Setup Munge</p><p>First, we need to make sure the clocks, users and groups (UIDs and GIDs) are synchronized across the cluster. We need to create two users: slurm and munge across all servers.</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ <span style=color:#0087ff>export</span> <span style=color:#0087ff>MUNGEUSER</span>=<span style=color:#00afaf>3456</span>
</span></span><span style=display:flex><span>$ groupadd -g <span style=color:#0087ff>$MUNGEUSER</span> munge
</span></span><span style=display:flex><span>$ useradd  -m -c <span style=color:#00afaf>&#34;MUNGE Uid &#39;N&#39; Gid Emporium&#34;</span> -d /var/lib/munge -u <span style=color:#0087ff>$MUNGEUSER</span> -g munge  -s /sbin/nologin munge
</span></span><span style=display:flex><span>$ <span style=color:#0087ff>export</span> <span style=color:#0087ff>SLURMUSER</span>=<span style=color:#00afaf>3457</span>
</span></span><span style=display:flex><span>$ groupadd -g <span style=color:#0087ff>$SLURMUSER</span> slurm
</span></span><span style=display:flex><span>$ useradd  -m -c <span style=color:#00afaf>&#34;SLURM workload manager&#34;</span> -d /var/lib/slurm -u <span style=color:#0087ff>$SLURMUSER</span> -g slurm  -s /bin/bash slurm
</span></span></code></pre></div><p>Then, we install Munge for authentication:</p><p>$ apt install munge libmunge2 libmunge-dev</p><p>To test if munge is installed successfully:</p><p>$ munge -n | unmunge | grep STATUS
STATUS: Success (0)</p><p>Next, we create a munge authentication key on one of the servers:</p><p>$ /usr/sbin/create-munge-key</p><p>After we generate munge authentication key, we copy the key /etc/munge/munge.key on that server to all other servers (overwrite the /etc/munge/ can be found <a href=https://wiki.archlinux.org/title/Slurm>here</a>.</p><p>After filling up the required fields in the form, we copy the generated file into <code>/etc/slurm-llnl/slurm.conf</code> all all nodes. Then, you can execute <code>sinfo</code> to check all nodes status. You can also launch jobs to see if it actually works, for example:</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>srun -N2 -l /bin/hostname
</span></span></code></pre></div><p>This should print out the hostname for all the nodes in the cluster.</p><h2 id=add-gpu-support>Add GPU support</h2><p>To add GPU support, we first create a file <code>gres.conf</code> in <code>/etc/slurm-llnl/</code>. Here is an example on one node:</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Name=gpu File=/dev/nvidia0
</span></span><span style=display:flex><span>Name=gpu File=/dev/nvidia1
</span></span><span style=display:flex><span>Name=gpu File=/dev/nvidia2
</span></span></code></pre></div><p>Then, we add <code>GresTypes=gpu</code> into <code>/etc/slurm-llnl/slurm.conf</code>. Next, we add the GPU information to <code>slurm.conf</code>:</p><div class=highlight><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NodeName=node1 Gres=gpu:3 State=UNKNOWN
</span></span></code></pre></div></div><p class=meta>Posted on <span class=postdate>16. November 2021</span></p></article></div><footer id=footer><p class=copyright><p>Â© 2022 <a href=https://www.bodunhu.com>Bodun Hu</a>. All rights reserved.</p></p></footer></div></body></html>