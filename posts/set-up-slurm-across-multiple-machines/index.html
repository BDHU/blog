<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><title>Set up Slurm across Multiple Machines</title></head><body><header id=return><h2></h2><a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></header><main id=content><article><header id=post-header><h1>Set up Slurm across Multiple Machines</h1><div><time>Nov 16, 2021</time></div></header><p>To install <a href=https://slurm.schedmd.com/documentation.html>Slurm</a>, we need to have admin access to the machine. This post explains how I got Slurm running in multiple Linux servers. All servers are running on Ubuntu 18.04 LTS.</p><h2 id=setup-munge>Setup Munge</h2><p>First, we need to make sure the clocks, users and groups (UIDs and GIDs) are synchronized across the cluster. We need to create two users: <code>slurm</code> and <code>munge</code> across all servers.</p><p>To test if munge is installed successfully:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ munge -n <span class=p>|</span> unmunge <span class=p>|</span> grep STATUS
</span></span><span class=line><span class=cl>STATUS:           Success <span class=o>(</span>0<span class=o>)</span>
</span></span></code></pre></div><p>Next, we create a munge authentication key on one of the servers:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ /usr/sbin/create-munge-key
</span></span></code></pre></div><p>After we generate munge authentication key, we copy the key <code>/etc/munge/munge.key</code> on that server to all other servers (overwrite the <code>/etc/munge/munge.key</code> on all other servers).</p><p>We need to setup the rights for munge accordingly on every server:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ chown -R munge: /etc/munge/ /var/log/munge/ /var/lib/munge/ /run/munge/
</span></span><span class=line><span class=cl>$ chmod <span class=m>0700</span> /etc/munge/ /var/log/munge/ /var/lib/munge/
</span></span><span class=line><span class=cl>$ chmod <span class=m>0755</span> /run/munge/
</span></span></code></pre></div><p>Then, we enable and start the munge service with (remember to not use sudo when running munge):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ systemctl <span class=nb>enable</span> munge
</span></span><span class=line><span class=cl>$ systemctl start munge
</span></span></code></pre></div><p>You can then test whether munge works properly by executing:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>munge -n                    <span class=c1># Generate a credential on stdout</span>
</span></span><span class=line><span class=cl>munge -n <span class=p>|</span> unmunge          <span class=c1># Displays information about the MUNGE key  </span>
</span></span><span class=line><span class=cl>munge -n <span class=p>|</span> ssh somehost unmunge
</span></span></code></pre></div><p>If everything is setup properly, you shouldn&rsquo;t see any error messages.</p><h2 id=setup-slurm>Setup Slurm</h2><p>Use <code>apt</code> to install slurm in Ubuntu systems (make sure all nodes have the same slurm versions):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ apt install slurm-wlm
</span></span></code></pre></div><p>Next, we need to configure slurm. Since we used package manager to install slurm, the version is lower than the latest release. Thus, it&rsquo;s preferably to not use the official <a href=https://slurm.schedmd.com/configurator.html>Slurm Configuration Tool</a>. Instead, we can find the corresponding version&rsquo;s configuration tool at <code>/usr/share/doc/slurmctld/slurm-wlm-configurator.html</code>. More information regarding setting the slurm config can be found <a href=https://wiki.archlinux.org/title/Slurm>here</a>.</p><p>After filling up the required fields in the form, we copy the generated file into <code>/etc/slurm-llnl/slurm.conf</code> all all nodes. Then, you can execute <code>sinfo</code> to check all nodes status. You can also launch jobs to see if it actually works, for example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>srun -N2 -l /bin/hostname
</span></span></code></pre></div><p>This should print out the hostname for all the nodes in the cluster.</p><h2 id=add-gpu-support>Add GPU support</h2><p>To add GPU support, we first create a file <code>gres.conf</code> in <code>/etc/slurm-llnl/</code>. Here is an example on one node:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Name=gpu File=/dev/nvidia0
</span></span><span class=line><span class=cl>Name=gpu File=/dev/nvidia1
</span></span><span class=line><span class=cl>Name=gpu File=/dev/nvidia2
</span></span></code></pre></div><p>Then, we add <code>GresTypes=gpu</code> into <code>/etc/slurm-llnl/slurm.conf</code>. Next, we add the GPU information to <code>slurm.conf</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>NodeName=node1 Gres=gpu:3 State=UNKNOWN
</span></span></code></pre></div><div style=width:100% id=comment><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=preferred-color-scheme crossorigin=anonymous async></script></div></article></main><footer id=footer><p>Â© 2022 Bodun Hu. All rights reserved.</p></footer></body></html>