<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZLK2GHB055"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZLK2GHB055")}</script><link rel=canonical href=https://www.bodunhu.com/blog/posts/set-up-slurm-across-multiple-machines/><title>Set up Slurm across Multiple Machines</title></head><body><header id=banner><h2><a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></h2><nav><ul><li><a href=/blog/posts/ title=posts>Archive</a></li><li><a href=https://www.bodunhu.com/ title=about>About</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>Set up Slurm across Multiple Machines</h1><div><time>April 19, 2023</time></div></header><p>To install <a href=https://slurm.schedmd.com/documentation.html>Slurm</a>, we need to have admin access to the machine. This post explains how I got Slurm running in multiple Linux servers. All servers are running on Ubuntu 18.04 LTS.</p><h2 id=setup-munge>Setup Munge</h2><p>First, we need to make sure the clocks, users and groups (UIDs and GIDs) are synchronized across the cluster. We need to create two users: <code>slurm</code> and <code>munge</code> across all servers.
z</p><p>Then, we install <a href=https://linux.die.net/man/7/munge>Munge</a> for authentication:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ apt install munge libmunge2 libmunge-dev
</span></span></code></pre></div><p>To test if munge is installed successfully:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ munge -n <span class=p>|</span> unmunge <span class=p>|</span> grep STATUS
</span></span><span class=line><span class=cl>STATUS:           Success <span class=o>(</span>0<span class=o>)</span>
</span></span></code></pre></div><p>Next, we create a munge authentication key on one of the servers:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ /usr/sbin/create-munge-key
</span></span></code></pre></div><p>After we generate munge authentication key, we copy the key <code>/etc/munge/munge.key</code> on that server to all other servers (overwrite the <code>/etc/munge/munge.key</code> on all other servers).</p><p>We need to setup the rights for munge accordingly on every server:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ chown -R munge: /etc/munge/ /var/log/munge/ /var/lib/munge/ /run/munge/
</span></span><span class=line><span class=cl>$ chmod <span class=m>0700</span> /etc/munge/ /var/log/munge/ /var/lib/munge/
</span></span><span class=line><span class=cl>$ chmod <span class=m>0755</span> /run/munge/
</span></span></code></pre></div><p>Then, we enable and start the munge service with (remember to not use sudo when running munge):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ systemctl <span class=nb>enable</span> munge
</span></span><span class=line><span class=cl>$ systemctl start munge
</span></span></code></pre></div><p>You can then test whether munge works properly by executing:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>munge -n                    <span class=c1># Generate a credential on stdout</span>
</span></span><span class=line><span class=cl>munge -n <span class=p>|</span> unmunge          <span class=c1># Displays information about the MUNGE key  </span>
</span></span><span class=line><span class=cl>munge -n <span class=p>|</span> ssh somehost unmunge
</span></span></code></pre></div><p>If everything is setup properly, you shouldn&rsquo;t see any error messages.</p><h2 id=setup-slurm>Setup Slurm</h2><p>Use <code>apt</code> to install slurm in Ubuntu systems (make sure all nodes have the same slurm versions):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ apt install slurm-wlm
</span></span></code></pre></div><p>Next, we need to configure slurm. Since we used package manager to install slurm, the version is lower than the latest release. Thus, it&rsquo;s preferably to not use the official <a href=https://slurm.schedmd.com/configurator.html>Slurm Configuration Tool</a>. Instead, we can find the corresponding version&rsquo;s configuration tool at <code>/usr/share/doc/slurmctld/slurm-wlm-configurator.html</code>.</p><p>After filling up the required fields in the form, we copy the generated file into <code>/etc/slurm-llnl/slurm.conf</code> on all nodes. Then, you can execute <code>sinfo</code> to check all nodes status. You can also launch jobs to see if it actually works, for example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>srun -N2 -l /bin/hostname
</span></span></code></pre></div><p>This should print out the hostname for all the nodes in the cluster.</p><h2 id=add-gpu-support>Add GPU support</h2><p>To add GPU support, we first create a file <code>gres.conf</code> in <code>/etc/slurm-llnl/</code>. Here is an example on one node:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Name=gpu File=/dev/nvidia0
</span></span><span class=line><span class=cl>Name=gpu File=/dev/nvidia1
</span></span><span class=line><span class=cl>Name=gpu File=/dev/nvidia2
</span></span></code></pre></div><p>Then, we add <code>GresTypes=gpu</code> into <code>/etc/slurm-llnl/slurm.conf</code>. Next, we add the GPU information to <code>slurm.conf</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>NodeName=node1 Gres=gpu:3 State=UNKNOWN
</span></span></code></pre></div><script src=https://giscus.app/client.js data-repo=BDHU/blog-comments data-repo-id=R_kgDOKZLDLA data-category=Announcements data-category-id=DIC_kwDOKZLDLM4CZrU- data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=noborder_light data-lang=en crossorigin=anonymous async></script></article></main><footer id=footer><p>Â© 2024 Bodun Hu. All rights reserved.
<a href=/blog/index.xml>Subscribe</a></p></footer></body></html>