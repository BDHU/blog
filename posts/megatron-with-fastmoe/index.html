<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Megatron with FastMoE | std::bodun::blog</title><meta name=title content="Megatron with FastMoE"><meta name=description content="This is a guide on setting up Megatron-LM with FastMoE. Megatron is a transformer developed by the Applied Deep Learning Research team at NVIDIA. FastMoE enables PyTorch support for the Mixture of Experts (MoE) models. We use the FastMoE layer to replace the MLP layers in the transformer language model.
Prerequisites Docker We recommend using one of NGC&rsquo;s recent PyTorch containers. The Megatron-LM repo uses pytorch:20.12-py3. We pull the image with:"><meta name=keywords content="ml,moe,megatron,dnn,how-to,"><meta property="og:title" content="Megatron with FastMoE"><meta property="og:description" content="This is a guide on setting up Megatron-LM with FastMoE. Megatron is a transformer developed by the Applied Deep Learning Research team at NVIDIA. FastMoE enables PyTorch support for the Mixture of Experts (MoE) models. We use the FastMoE layer to replace the MLP layers in the transformer language model.
Prerequisites Docker We recommend using one of NGC&rsquo;s recent PyTorch containers. The Megatron-LM repo uses pytorch:20.12-py3. We pull the image with:"><meta property="og:type" content="article"><meta property="og:url" content="https://www.bodunhu.com/blog/posts/megatron-with-fastmoe/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-12-01T00:00:00+00:00"><meta property="article:modified_time" content="2021-12-10T22:47:11-06:00"><meta property="og:site_name" content="std::bodun::blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Megatron with FastMoE"><meta name=twitter:description content="This is a guide on setting up Megatron-LM with FastMoE. Megatron is a transformer developed by the Applied Deep Learning Research team at NVIDIA. FastMoE enables PyTorch support for the Mixture of Experts (MoE) models. We use the FastMoE layer to replace the MLP layers in the transformer language model.
Prerequisites Docker We recommend using one of NGC&rsquo;s recent PyTorch containers. The Megatron-LM repo uses pytorch:20.12-py3. We pull the image with:"><meta name=twitter:site content="@https://twitter.com/BodunHu"><meta itemprop=name content="Megatron with FastMoE"><meta itemprop=description content="This is a guide on setting up Megatron-LM with FastMoE. Megatron is a transformer developed by the Applied Deep Learning Research team at NVIDIA. FastMoE enables PyTorch support for the Mixture of Experts (MoE) models. We use the FastMoE layer to replace the MLP layers in the transformer language model.
Prerequisites Docker We recommend using one of NGC&rsquo;s recent PyTorch containers. The Megatron-LM repo uses pytorch:20.12-py3. We pull the image with:"><meta itemprop=datePublished content="2021-12-01T00:00:00+00:00"><meta itemprop=dateModified content="2021-12-10T22:47:11-06:00"><meta itemprop=wordCount content="628"><meta itemprop=keywords content="ml,moe,megatron,dnn,how-to,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style></head><body><header><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><a href=/blog/ class=title><h2>std::bodun::blog</h2></a><nav><a href=/blog/>Home</a>
<a href=/blog/posts/>Archive</a>
<a href=https://www.bodunhu.com/>About</a></nav></header><main><h1>Megatron with FastMoE</h1><p><i><time datetime=2021-12-01 pubdate>01 Dec, 2021</time></i></p><content><p>This is a guide on setting up <a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a> with <a href=https://github.com/laekov/fastmoe>FastMoE</a>. Megatron is a transformer developed by the Applied Deep Learning Research team at NVIDIA. FastMoE enables PyTorch support for the Mixture of Experts (MoE) models. We use the FastMoE layer to replace the MLP layers in the transformer language model.</p><h2 id=prerequisites>Prerequisites</h2><h3 id=docker>Docker</h3><p>We recommend using one of <a href=https://ngc.nvidia.com/catalog/containers/nvidia:pytorch>NGC&rsquo;s recent PyTorch containers</a>. The Megatron-LM repo uses <code>pytorch:20.12-py3</code>. We pull the image with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker pull nvcr.io/nvidia/pytorch:20.12-py3
</span></span></code></pre></div><p>Note: it&rsquo;s possible to use the <a href=https://hub.docker.com/r/pytorch/pytorch>official PyTorch image</a>. However, there are a few dependencies missing, which requires manual installation. Also, PyTorch with versions greater than 1.8 seems to have problem during forward passing so we don&rsquo;t use the official PyTorch image here.</p><p>After the image is pulled successfully, we want to start a container. The <a href=https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch>NGC site</a> contains instructions on how to start a docker image. We use the following script:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --gpus all -it --rm --ipc<span style=color:#f92672>=</span>host -v /home/edwardhu/:/home/edwardhu/ --name pytorch-moe &lt;image_id&gt;
</span></span></code></pre></div><p>Note: we might encounter problems before starting up the docker container. Make sure we set the GPG and remote repo for the <code>nvidia-docker2</code> package on the host and install required packages:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>distribution<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>. /etc/os-release;echo $ID$VERSION_ID<span style=color:#66d9ef>)</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>   <span style=color:#f92672>&amp;&amp;</span> curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>   <span style=color:#f92672>&amp;&amp;</span> curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
</span></span><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install -y nvidia-docker2
</span></span><span style=display:flex><span>sudo systemctl restart docker
</span></span></code></pre></div><h3 id=set-up-fastmoe>Set up FastMoE</h3><p>After we spin up the container, we clone the <a href=https://github.com/laekov/fastmoe>fastmoe repo</a> and enter project. There is a <code>setup.py</code> file in the root of the project. Then we execute:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>USE_NCCL<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> python setup.py install
</span></span></code></pre></div><p>to install FastMoE. For some reason, there is a compilation error saying that <code>broadcastUniqueNCCLID(&amp;ncclID)</code>&rsquo;s definition can not be found. We see there is a condition check right above the error function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>#if defined(TORCH_VERSION_MAJOR) &amp;&amp; (TORCH_VERSION_MAJOR &gt; 1 || \
</span></span></span><span style=display:flex><span><span style=color:#75715e>        (TORCH_VERSION_MAJOR == 1 &amp;&amp; TORCH_VERSION_MINOR &gt;= 8))
</span></span></span></code></pre></div><p>For some reason, the check failed despite the container has PyTorch version <code>1.8.0a0+1606899</code>. <a href=https://github.com/laekov/fastmoe/issues/93>According to the author</a>, the <code>if</code> macro was to deal with PyTorch&rsquo;s API variance between v1.7.x and v1.8.x. For now, we simply comment out the <code>if</code> check and force the <code>broadcastUniqueNCCLID(&amp;ncclID, c10d::OpType::SEND, "fastmoe_nccl_comm", rank);</code> to be used instead of the <code>broadcastUniqueNCCLID(&amp;ncclID)</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>//#if defined(TORCH_VERSION_MAJOR) &amp;&amp; (TORCH_VERSION_MAJOR &gt; 1 || \
</span></span></span><span style=display:flex><span><span style=color:#75715e>//        (TORCH_VERSION_MAJOR == 1 &amp;&amp; TORCH_VERSION_MINOR &gt;= 8))
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        broadcastUniqueNCCLID(<span style=color:#f92672>&amp;</span>ncclID,
</span></span><span style=display:flex><span>                c10d<span style=color:#f92672>::</span>OpType<span style=color:#f92672>::</span>SEND,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;fastmoe_nccl_comm&#34;</span>,
</span></span><span style=display:flex><span>                rank);
</span></span><span style=display:flex><span><span style=color:#75715e>//#else
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#75715e>//broadcastUniqueNCCLID(&amp;ncclID);
</span></span></span><span style=display:flex><span><span style=color:#75715e>//#endif
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        ncclComm_t comm;
</span></span><span style=display:flex><span>        NCCL_SAFE_CALL(ncclCommInitRank(<span style=color:#f92672>&amp;</span>comm, getSize(), ncclID, rank));
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> comm;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><p>Finally, we need to download vocab file for later use since the Megatron repo doesn&rsquo;t have one. Here, we use the vocab file from the <a href=https://github.com/microsoft/SDNet/tree/master/bert_vocab_files>SDNet repo</a>. Feel free to use something else.</p><h3 id=megatron-lm-setup>Megatron-LM Setup</h3><p>After we set up FastMoE, we clone the <a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a> repo into the container. The <a href=https://github.com/laekov/fastmoe/tree/master/examples/megatron>FastMoE&rsquo;s example guide on Megatron</a> uses Megatron <code>v2.2</code> release, so we need to choose the <code>v2.2</code> tag in the Megatron repo.</p><p>Next, we follow the <a href=https://github.com/laekov/fastmoe/tree/master/examples/megatron>FastMoE&rsquo;s guide on Megatron</a> and apply the <code>clip-grad-v2.2.path</code> and <code>fmoefy-v2.2.patch</code> accordingly. Instructions on how to apply patches in Linux is easy to find, for example, here is <a href=https://www.cyberciti.biz/faq/appy-patch-file-using-patch-command/>one</a>.</p><h3 id=race-dataset>RACE Dataset</h3><p>After setting up Megatron-LM, we download the <a href=https://www.cs.cmu.edu/~glai1/data/race/>RACE dataset</a> for fine-tuning downstream tasks (RACE is used with BERT evaluation, the Megatron&rsquo;s repo also has several other examples using GPT, here we stick to BERT). The <a href=https://github.com/NVIDIA/Megatron-LM>Megatron repo</a> also provides instructions on how to acquire these datasets for evaluation. For now, we just want to get the fine-tuning process up and running, without caring so much about the accuracy. Therefore, we don&rsquo;t need to pre-train the BERT model just yet. After the dataset finished downloading, we simply need to decompress it.</p><h3 id=summury>Summury</h3><p>The most important line to change a model to FastMoE style is through:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Initialize FastMoE</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> args<span style=color:#f92672>.</span>fmoefy:
</span></span><span style=display:flex><span>        <span style=color:#f92672>from</span> fmoe.megatron <span style=color:#f92672>import</span> patch_forward_step, patch_model_provider
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        forward_step_func <span style=color:#f92672>=</span> patch_forward_step(forward_step_func)
</span></span><span style=display:flex><span>        model_provider <span style=color:#f92672>=</span> patch_model_provider(model_provider)
</span></span></code></pre></div><p>More information can be found in the fmoefy patch <a href=https://github.com/laekov/fastmoe/blob/master/examples/megatron/fmoefy-v2.2.patch>file</a>.</p></content><p></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>