<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZLK2GHB055"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZLK2GHB055",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-ZLK2GHB055","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<link rel=canonical href=https://www.bodunhu.com/blog/posts/megatron-with-fastmoe/><title>Megatron with FastMoE</title></head><body><header id=banner><h2><a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></h2><nav><ul><li><a href=/blog/posts/ title=posts>Archive</a></li><li><a href=https://www.bodunhu.com/ title=about>About</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>Megatron with FastMoE</h1><div><time>April 19, 2023</time></div></header><p>This is a guide on setting up <a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a> with <a href=https://github.com/laekov/fastmoe>FastMoE</a>. Megatron is a transformer developed by the Applied Deep Learning Research team at NVIDIA. FastMoE enables PyTorch support for the Mixture of Experts (MoE) models. We use the FastMoE layer to replace the MLP layers in the transformer language model.</p><h2 id=prerequisites>Prerequisites</h2><h3 id=docker>Docker</h3><p>We recommend using one of <a href=https://ngc.nvidia.com/catalog/containers/nvidia:pytorch>NGC&rsquo;s recent PyTorch containers</a>. The Megatron-LM repo uses <code>pytorch:20.12-py3</code>. We pull the image with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker pull nvcr.io/nvidia/pytorch:20.12-py3
</span></span></code></pre></div><p>Note: it&rsquo;s possible to use the <a href=https://hub.docker.com/r/pytorch/pytorch>official PyTorch image</a>. However, there are a few dependencies missing, which requires manual installation. Also, PyTorch with versions greater than 1.8 seems to have problem during forward passing so we don&rsquo;t use the official PyTorch image here.</p><p>After the image is pulled successfully, we want to start a container. The <a href=https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch>NGC site</a> contains instructions on how to start a docker image. We use the following script:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --gpus all -it --rm --ipc<span style=color:#f92672>=</span>host -v /home/edwardhu/:/home/edwardhu/ --name pytorch-moe &lt;image_id&gt;
</span></span></code></pre></div><p>Note: we might encounter problems before starting up the docker container. Make sure we set the GPG and remote repo for the <code>nvidia-docker2</code> package on the host and install required packages:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>distribution<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>. /etc/os-release;echo $ID$VERSION_ID<span style=color:#66d9ef>)</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>   <span style=color:#f92672>&amp;&amp;</span> curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>   <span style=color:#f92672>&amp;&amp;</span> curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
</span></span><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install -y nvidia-docker2
</span></span><span style=display:flex><span>sudo systemctl restart docker
</span></span></code></pre></div><h3 id=set-up-fastmoe>Set up FastMoE</h3><p>After we spin up the container, we clone the <a href=https://github.com/laekov/fastmoe>fastmoe repo</a> and enter project. There is a <code>setup.py</code> file in the root of the project. Then we execute:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>USE_NCCL<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> python setup.py install
</span></span></code></pre></div><p>to install FastMoE. For some reason, there is a compilation error saying that <code>broadcastUniqueNCCLID(&amp;ncclID)</code>&rsquo;s definition can not be found. We see there is a condition check right above the error function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>#if defined(TORCH_VERSION_MAJOR) &amp;&amp; (TORCH_VERSION_MAJOR &gt; 1 || \
</span></span></span><span style=display:flex><span><span style=color:#75715e>        (TORCH_VERSION_MAJOR == 1 &amp;&amp; TORCH_VERSION_MINOR &gt;= 8))
</span></span></span></code></pre></div><p>For some reason, the check failed despite the container has PyTorch version <code>1.8.0a0+1606899</code>. <a href=https://github.com/laekov/fastmoe/issues/93>According to the author</a>, the <code>if</code> macro was to deal with PyTorch&rsquo;s API variance between v1.7.x and v1.8.x. For now, we simply comment out the <code>if</code> check and force the <code>broadcastUniqueNCCLID(&amp;ncclID, c10d::OpType::SEND, "fastmoe_nccl_comm", rank);</code> to be used instead of the <code>broadcastUniqueNCCLID(&amp;ncclID)</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>//#if defined(TORCH_VERSION_MAJOR) &amp;&amp; (TORCH_VERSION_MAJOR &gt; 1 || \
</span></span></span><span style=display:flex><span><span style=color:#75715e>//        (TORCH_VERSION_MAJOR == 1 &amp;&amp; TORCH_VERSION_MINOR &gt;= 8))
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        broadcastUniqueNCCLID(<span style=color:#f92672>&amp;</span>ncclID,
</span></span><span style=display:flex><span>                c10d<span style=color:#f92672>::</span>OpType<span style=color:#f92672>::</span>SEND,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;fastmoe_nccl_comm&#34;</span>,
</span></span><span style=display:flex><span>                rank);
</span></span><span style=display:flex><span><span style=color:#75715e>//#else
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#75715e>//broadcastUniqueNCCLID(&amp;ncclID);
</span></span></span><span style=display:flex><span><span style=color:#75715e>//#endif
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        ncclComm_t comm;
</span></span><span style=display:flex><span>        NCCL_SAFE_CALL(ncclCommInitRank(<span style=color:#f92672>&amp;</span>comm, getSize(), ncclID, rank));
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> comm;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><p>Finally, we need to download vocab file for later use since the Megatron repo doesn&rsquo;t have one. Here, we use the vocab file from the <a href=https://github.com/microsoft/SDNet/tree/master/bert_vocab_files>SDNet repo</a>. Feel free to use something else.</p><h3 id=megatron-lm-setup>Megatron-LM Setup</h3><p>After we set up FastMoE, we clone the <a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a> repo into the container. The <a href=https://github.com/laekov/fastmoe/tree/master/examples/megatron>FastMoE&rsquo;s example guide on Megatron</a> uses Megatron <code>v2.2</code> release, so we need to choose the <code>v2.2</code> tag in the Megatron repo.</p><p>Next, we follow the <a href=https://github.com/laekov/fastmoe/tree/master/examples/megatron>FastMoE&rsquo;s guide on Megatron</a> and apply the <code>clip-grad-v2.2.path</code> and <code>fmoefy-v2.2.patch</code> accordingly. Instructions on how to apply patches in Linux is easy to find, for example, here is <a href=https://www.cyberciti.biz/faq/appy-patch-file-using-patch-command/>one</a>.</p><h3 id=race-dataset>RACE Dataset</h3><p>After setting up Megatron-LM, we download the <a href=https://www.cs.cmu.edu/~glai1/data/race/>RACE dataset</a> for fine-tuning downstream tasks (RACE is used with BERT evaluation, the Megatron&rsquo;s repo also has several other examples using GPT, here we stick to BERT). The <a href=https://github.com/NVIDIA/Megatron-LM>Megatron repo</a> also provides instructions on how to acquire these datasets for evaluation. For now, we just want to get the fine-tuning process up and running, without caring so much about the accuracy. Therefore, we don&rsquo;t need to pre-train the BERT model just yet. After the dataset finished downloading, we simply need to decompress it.</p><h3 id=summury>Summury</h3><p>The most important line to change a model to FastMoE style is through:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Initialize FastMoE</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> args<span style=color:#f92672>.</span>fmoefy:
</span></span><span style=display:flex><span>        <span style=color:#f92672>from</span> fmoe.megatron <span style=color:#f92672>import</span> patch_forward_step, patch_model_provider
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        forward_step_func <span style=color:#f92672>=</span> patch_forward_step(forward_step_func)
</span></span><span style=display:flex><span>        model_provider <span style=color:#f92672>=</span> patch_model_provider(model_provider)
</span></span></code></pre></div><p>More information can be found in the fmoefy patch <a href=https://github.com/laekov/fastmoe/blob/master/examples/megatron/fmoefy-v2.2.patch>file</a>.</p><script src=https://giscus.app/client.js data-repo=BDHU/blog-comments data-repo-id=R_kgDOKZLDLA data-category=Announcements data-category-id=DIC_kwDOKZLDLM4CZrU- data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=noborder_light data-lang=en crossorigin=anonymous async></script></article></main><footer id=footer><p>Â© 2023 Bodun Hu. All rights reserved.
<a href=/blog/index.xml>Subscribe</a></p></footer></body></html>