<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<link rel=canonical href=https://www.bodunhu.com/blog/posts/paper-review-dynamic-tensor-rematerialization/><title>Paper Review - Dynamic Tensor Rematerialization</title></head><body><header id=return><h2></h2><a href=https://www.bodunhu.com/blog/>std::bodun::blog::</a></header><main id=content><article><header id=post-header><h1>Paper Review - Dynamic Tensor Rematerialization</h1><div><time>February 14, 2022</time></div></header><p>Dynamic Tensor Rematerialization (<a href=https://arxiv.org/pdf/2006.09616.pdf>DTR</a>) treats GPU memory as a large cache, where tensors can be evicted to save memory, and recomputed if needed later.</p><p>DTR&rsquo;s eviction policy relies on the heuristic \(h\). The heuristic assigns a value \(h(t)\) to each resident tensor \(t\), approximating the cost of evicting the tensor. DTR evicts the tensor with the lowest cost based on the value of \(h\). \(h\) can factor in arbitrary metadata.</p><p>During every operator call in <a href=https://pytorch.org/>PyTorch</a>, DTR intercepts the call and performs the following tasks:</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/DTR/DTR-operator-intercept.png#center alt=DTR-operator-intercept></p><p>In short, whenever we perform an operation, we first recursively re-calculate all the non-resident tensors the current operation depends on, while evicting tensors we don&rsquo;t need until there are enough GPU space left. To decide which tensors to evict, DTR uses the tensor with the lowest value \(h\):</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/DTR/tensor-evict.png#center alt=tensor-evict></p><p>The heuristic \(h\) evicts tensors based on three properties: staleness, size, and compute cost. It evicts tensors that are: least recently used, takes large GPU memory space, and easy to recompute. \(H _{DTR}\) is computed as:</p><p>\[
h _{DTR}(s, m, c) (t) := \frac{c(t)}{m(t) \cdot s(t)&rsquo;}
\]</p><p>Recomputing an evicted tensor \(t\) may result in recomputing many more tensors that \(t\) recursively depends on. Thus, the paper proposes an improved heuristic to take the recursive recomputations into account (with more maintenance cost). These tensors are called <em>evicted neighborhood \(e ^{*} (t)\)</em>.</p><p>\[
h_ {DTR-improved}(s, m, c) (t) := \frac{c(t) + \sum _{u \in e ^{*} (t)} c(u)}{m(t) \cdot s(t)&rsquo;}
\]</p><p>This heuristic captures the recomputation costs for all tensors that \(t\) recursively depend on.</p><div style=width:100% id=comment><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=preferred-color-scheme crossorigin=anonymous async></script></div></article></main><footer id=footer><p>Â© 2022 Bodun Hu. All rights reserved.</p></footer></body></html>