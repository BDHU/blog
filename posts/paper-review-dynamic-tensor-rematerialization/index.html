<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZLK2GHB055"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZLK2GHB055")}</script><link rel=canonical href=https://www.bodunhu.com/blog/posts/paper-review-dynamic-tensor-rematerialization/><title>std::bodun::blog</title></head><body><header id=banner><span><a href=https://www.bodunhu.com/blog/>blog</a> | <a href=https://www.bodunhu.com/blog//posts>posts</a> | <a href=/about/>about</a></span></header><main id=content><article><header id=post-header><h2>Paper Review - Dynamic Tensor Rematerialization</h2><div><time>April 19, 2023</time></div></header><p>Dynamic Tensor Rematerialization (<a href=https://arxiv.org/pdf/2006.09616.pdf>DTR</a>) treats GPU memory as a large cache, where tensors can be evicted to save memory, and recomputed if needed later.</p><p>DTR&rsquo;s eviction policy relies on the heuristic \(h\). The heuristic assigns a value \(h(t)\) to each resident tensor \(t\), approximating the cost of evicting the tensor. DTR evicts the tensor with the lowest cost based on the value of \(h\). \(h\) can factor in arbitrary metadata.</p><p>During every operator call in <a href=https://pytorch.org/>PyTorch</a>, DTR intercepts the call and performs the following tasks:</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/DTR/DTR-operator-intercept.png#center alt=DTR-operator-intercept></p><p>In short, whenever we perform an operation, we first recursively re-calculate all the non-resident tensors the current operation depends on, while evicting tensors we don&rsquo;t need until there are enough GPU space left. To decide which tensors to evict, DTR uses the tensor with the lowest value \(h\):</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/DTR/tensor-evict.png#center alt=tensor-evict></p><p>The heuristic \(h\) evicts tensors based on three properties: staleness, size, and compute cost. It evicts tensors that are: least recently used, takes large GPU memory space, and easy to recompute. \(H _{DTR}\) is computed as:</p><p>\[
h _{DTR}(s, m, c) (t) := \frac{c(t)}{m(t) \cdot s(t)&rsquo;}
\]</p><p>Recomputing an evicted tensor \(t\) may result in recomputing many more tensors that \(t\) recursively depends on. Thus, the paper proposes an improved heuristic to take the recursive recomputations into account (with more maintenance cost). These tensors are called <em>evicted neighborhood \(e ^{*} (t)\)</em>.</p><p>\[
h_ {DTR-improved}(s, m, c) (t) := \frac{c(t) + \sum _{u \in e ^{*} (t)} c(u)}{m(t) \cdot s(t)&rsquo;}
\]</p><p>This heuristic captures the recomputation costs for all tensors that \(t\) recursively depend on.</p><script src=https://giscus.app/client.js data-repo=BDHU/blog-comments data-repo-id=R_kgDOKZLDLA data-category=Announcements data-category-id=DIC_kwDOKZLDLM4CZrU- data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async></script></article></main><footer id=footer><p>Â© 2025 Bodun Hu. All rights reserved.
<a href=/blog/index.xml>Subscribe</a></p></footer></body></html>