<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><title>Maximum Likelihood for Classification</title></head><body><header id=return><h2></h2><a href=https://www.bodunhu.com/blog/><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" style="vertical-align:-.125em" width="1em" height="1em" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m11 5-7 7 7 7m-7-7h16"/></svg>&nbspHome
       </a></header><main id=content><article><header id=post-header><h1>Maximum Likelihood for Classification</h1><div><time>Updated Feb 14, 2022</time></div></header><p>Let&rsquo;s say we want to classify an input text \(y\) and give it a label \(x\). Formally, we want to find:</p><p>\[
\textrm{argmax} P(x | y)
\]</p><p>By Bayes&rsquo; rule this is the same as</p><p>\[
\textrm{argmax} \frac{P(y|x)P(y)}{P(x)}
\]</p><p>Suppose we have five documents as training data and one document as the input as testing data. Our objective is to give a label to the test sentence.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/MLE/text-example.png alt=text-example></p><center>Credit: Eunsol Choi</center><p>Let&rsquo;s define the probability of class as (\(N\) is the total number of classes)</p><p>\[
p(x) = \frac{count(x)}{N}
\]</p><p>and the probability of a word appearing given a class label (total number of vocabs)</p><p>\[
p(w_i|x) = \frac{count(w_i,x) + 1}{count(x) + |V|}
\]</p><p>The conditional probabilities for \(p(w_i|y)\) is</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/MLE/conditional_prob.png alt=conditional-probabilities></p><p>Now, we want to find out which language label should we assign the sentence &ldquo;Chinese Chinese Chinese Tokyo Japan&rdquo;. This is the same as asking which labels (\(x\))) should we pick so that \(P(W|x)P(x)\) yields the greatest value. Mathematically, we want to find out where the gradient of the function \(P(W|x)P(x)\) is flat.</p><p>If we label the sentence as j (Japanese), we have \(P(j | d_5) \propto \frac{1}{4}\cdot (\frac{2}{9}^3)\cdot \frac{2}{9}\cdot \frac{2}{9} \approx 0.0001\). If we calculate \(P(c|d_5)\), we get 0.0003, which generates the largest value for \(P(x | y)\).</p><div style=width:100% id=comment><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></article></main><footer id=footer><p>Â© 2022 Bodun Hu. All rights reserved.</p></footer></body></html>