<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Maximum Likelihood for Classification | std::bodun::blog</title><meta name=title content="Maximum Likelihood for Classification"><meta name=description content="Let&rsquo;s say we want to classify an input text \(y\) and give it a label \(x\). Formally, we want to find:
\[ \textrm{argmax} P(x | y) \]
By Bayes&rsquo; rule this is the same as
\[ \textrm{argmax} \frac{P(y|x)P(y)}{P(x)} \]
Suppose we have five documents as training data and one document as the input as testing data. Our objective is to give a label to the test sentence.
Credit: Eunsol Choi Let&rsquo;s define the probability of class as (\(N\) is the total number of classes)"><meta name=keywords content="mlsys,ml,"><meta property="og:title" content="Maximum Likelihood for Classification"><meta property="og:description" content="Let&rsquo;s say we want to classify an input text \(y\) and give it a label \(x\). Formally, we want to find:
\[ \textrm{argmax} P(x | y) \]
By Bayes&rsquo; rule this is the same as
\[ \textrm{argmax} \frac{P(y|x)P(y)}{P(x)} \]
Suppose we have five documents as training data and one document as the input as testing data. Our objective is to give a label to the test sentence.
Credit: Eunsol Choi Let&rsquo;s define the probability of class as (\(N\) is the total number of classes)"><meta property="og:type" content="article"><meta property="og:url" content="https://www.bodunhu.com/blog/posts/maximum-likelihood-for-classification/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-24T00:00:00+00:00"><meta property="article:modified_time" content="2022-02-14T18:31:56-06:00"><meta property="og:site_name" content="std::bodun::blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Maximum Likelihood for Classification"><meta name=twitter:description content="Let&rsquo;s say we want to classify an input text \(y\) and give it a label \(x\). Formally, we want to find:
\[ \textrm{argmax} P(x | y) \]
By Bayes&rsquo; rule this is the same as
\[ \textrm{argmax} \frac{P(y|x)P(y)}{P(x)} \]
Suppose we have five documents as training data and one document as the input as testing data. Our objective is to give a label to the test sentence.
Credit: Eunsol Choi Let&rsquo;s define the probability of class as (\(N\) is the total number of classes)"><meta name=twitter:site content="@https://twitter.com/BodunHu"><meta itemprop=name content="Maximum Likelihood for Classification"><meta itemprop=description content="Let&rsquo;s say we want to classify an input text \(y\) and give it a label \(x\). Formally, we want to find:
\[ \textrm{argmax} P(x | y) \]
By Bayes&rsquo; rule this is the same as
\[ \textrm{argmax} \frac{P(y|x)P(y)}{P(x)} \]
Suppose we have five documents as training data and one document as the input as testing data. Our objective is to give a label to the test sentence.
Credit: Eunsol Choi Let&rsquo;s define the probability of class as (\(N\) is the total number of classes)"><meta itemprop=datePublished content="2022-01-24T00:00:00+00:00"><meta itemprop=dateModified content="2022-02-14T18:31:56-06:00"><meta itemprop=wordCount content="208"><meta itemprop=keywords content="mlsys,ml,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{ <!-- font-family: Verdana, sans-serif; --> font-family: "Hoefler Text", "Palatino", "Palatino Linotype", "Libre Caslon Text", serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style></head><body><header><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><a href=/blog/ class=title><h2>std::bodun::blog</h2></a><nav><a href=/blog/>Home</a>
<a href=/blog/posts/>Archive</a>
<a href=https://www.bodunhu.com/>About</a></nav></header><main><h1>Maximum Likelihood for Classification</h1><p><i><time datetime=2022-01-24 pubdate>24 Jan, 2022</time></i></p><content><p>Let&rsquo;s say we want to classify an input text \(y\) and give it a label \(x\). Formally, we want to find:</p><p>\[
\textrm{argmax} P(x | y)
\]</p><p>By Bayes&rsquo; rule this is the same as</p><p>\[
\textrm{argmax} \frac{P(y|x)P(y)}{P(x)}
\]</p><p>Suppose we have five documents as training data and one document as the input as testing data. Our objective is to give a label to the test sentence.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/MLE/text-example.png alt=text-example></p><center>Credit: Eunsol Choi</center><p>Let&rsquo;s define the probability of class as (\(N\) is the total number of classes)</p><p>\[
p(x) = \frac{count(x)}{N}
\]</p><p>and the probability of a word appearing given a class label (total number of vocabs)</p><p>\[
p(w_i|x) = \frac{count(w_i,x) + 1}{count(x) + |V|}
\]</p><p>The conditional probabilities for \(p(w_i|y)\) is</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/MLE/conditional_prob.png alt=conditional-probabilities></p><p>Now, we want to find out which language label should we assign the sentence &ldquo;Chinese Chinese Chinese Tokyo Japan&rdquo;. This is the same as asking which labels (\(x\))) should we pick so that \(P(W|x)P(x)\) yields the greatest value. Mathematically, we want to find out where the gradient of the function \(P(W|x)P(x)\) is flat.</p><p>If we label the sentence as j (Japanese), we have \(P(j | d_5) \propto \frac{1}{4}\cdot (\frac{2}{9}^3)\cdot \frac{2}{9}\cdot \frac{2}{9} \approx 0.0001\). If we calculate \(P(c|d_5)\), we get 0.0003, which generates the largest value for \(P(x | y)\).</p></content><p></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>