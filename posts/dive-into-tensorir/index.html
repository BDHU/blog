<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<link rel=canonical href=https://www.bodunhu.com/blog/posts/dive-into-tensorir/><title>Dive into TensorIR</title></head><body><header id=banner><h2><a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></h2><nav><ul><li><a href=/blog/posts/ title=posts>Archive</a></li><li><a href=https://www.bodunhu.com/ title=about>About</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>Dive into TensorIR</h1><div><time>August 29, 2022</time></div></header><p><a href=https://arxiv.org/abs/2207.04296>TensorIR</a> is a compiler abstraction for optimizing programs with tensor computation primitives in <a href=https://tvm.apache.org/>TVM</a>. Imagine a DNN task as a graph, where each node represents a tensor computation. TensorIR explains how each node/tensor computation primitive in the graph is carried out. This post explains my attempt to implement 2D convolution using TensorIR. It is derived from the <a href=https://mlc.ai/summer22/>Machine Learning Compilation</a> course offered by <a href=https://tqchen.com/>Tianqi Chen</a>.</p><h2 id=implement-2d-convolution>Implement 2D Convolution</h2><p>2D convolution is a common operation in image processing. The image below captures how 2D convolution operates. I won&rsquo;t go into details here. But you can find plenty information online regarding convolution.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/dive-into-tensorIR/2d-conv.png alt=2D-convolution></p><p>First, we initialize both the input matrix and the weight matrix:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># batch, input_channel_dim, image_height, image_width, output_channel_dim, kernel_width &amp; height</span>
</span></span><span style=display:flex><span>N, CI, H, W, CO, K <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#75715e># output_height, output_width, assuming kernel has stride=1 and padding=0</span>
</span></span><span style=display:flex><span>OUT_H, OUT_W <span style=color:#f92672>=</span> H <span style=color:#f92672>-</span> K <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, W <span style=color:#f92672>-</span> K <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(N<span style=color:#f92672>*</span>CI<span style=color:#f92672>*</span>H<span style=color:#f92672>*</span>W)<span style=color:#f92672>.</span>reshape(N, CI, H, W)
</span></span><span style=display:flex><span>weight <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(CO<span style=color:#f92672>*</span>CI<span style=color:#f92672>*</span>K<span style=color:#f92672>*</span>K)<span style=color:#f92672>.</span>reshape(CO, CI, K, K)
</span></span></code></pre></div><p>We can validate the results using <code>torch.nn.functional.conv2d()</code> from PyTorch.</p><p>One thing Tianqi recommended for starters is to write the implementation first in numpy, and then translate the numpy implementation to TensorIR. I started my implementation directly from TensorIR, before totally getting confused. So here&rsquo;s how I approach the problem.</p><p>First, and perhaps most importantly, you should figure out the accessing pattern of the output matrix, and gradually fill up the compute rules for each element in the output matrix. So, we know the output matrix has a shape of <code>(N, CO, OUT_H, OUT_w)</code> (which corresponds to batch, number of output channels, output height, and output width). The numpy loop will look like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> b <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, N):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> co <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, CO):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> h <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, OUT_H):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, OUT_W):
</span></span><span style=display:flex><span>                Y[b, co, h, w] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span></code></pre></div><p>Here, we access element in the output matrix one by one and initialize each element to be 0. Next, we will try to figure out how to compute each element. We know each element in the output matrix is just the sum of element-wise multiplication of both the 2D convolutional kernel (1 by 3 by 3) and the corresponding area in the input matrix (1 by 3 by 3):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> b <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, N):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> co <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, CO):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> h <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, OUT_H):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, OUT_W):
</span></span><span style=display:flex><span>                <span style=color:#75715e># init to 0</span>
</span></span><span style=display:flex><span>                Y[b, co, h, w] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># 2d conv kernel</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> ci <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, CI):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>for</span> kh <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, K):
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>for</span> kw <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, K):
</span></span><span style=display:flex><span>                            <span style=color:#75715e># reduction</span>
</span></span><span style=display:flex><span>                            Y[b, co, h, w] <span style=color:#f92672>+=</span> A[b, ci, h<span style=color:#f92672>+</span>kh, w<span style=color:#f92672>+</span>kw] <span style=color:#f92672>*</span> W[co, ci, kh, kw]
</span></span></code></pre></div><p>We can verify the function has the same output as <code>torch.nn.functional.conv2d()</code> from PyTorch.</p><p>The next part is to translate the numpy code into TensorIR. I won&rsquo;t go into every the details of every single line here, but you can find all explanations from this <a href=https://mlc.ai/chapter_tensor_program/case_study.html>note</a>.</p><p>The nested loop can be encapsulated using <code>T.grid()</code> like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyConv</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>conv2d</span>(data: T<span style=color:#f92672>.</span>Buffer[(N, CI, H, W), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                weight: T<span style=color:#f92672>.</span>Buffer[(CO, CI, K, K), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                result: T<span style=color:#f92672>.</span>Buffer[(N, CO, OUT_H, OUT_W), <span style=color:#e6db74>&#34;int64&#34;</span>]):
</span></span><span style=display:flex><span>        T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;conv2d&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        <span style=color:#75715e># loop through each elem in the output matrix</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> b, o, h, w <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(N, CO, OUT_H, OUT_W):
</span></span><span style=display:flex><span>            <span style=color:#75715e># kernel access pattern</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> kc, kh, kw <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(CI, K, K):
</span></span></code></pre></div><p>Next, we define the block (a basic unit of computation in TensorIR). A block contains a set of block axes <code>(vi, vj, vk)</code> and computations defined around them. Here, we define the property about each block axes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyConv</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>conv2d</span>(data: T<span style=color:#f92672>.</span>Buffer[(N, CI, H, W), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                weight: T<span style=color:#f92672>.</span>Buffer[(CO, CI, K, K), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                result: T<span style=color:#f92672>.</span>Buffer[(N, CO, OUT_H, OUT_W), <span style=color:#e6db74>&#34;int64&#34;</span>]):
</span></span><span style=display:flex><span>        T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;conv2d&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        <span style=color:#75715e># impl</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> b, o, h, w <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(N, CO, OUT_H, OUT_W):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> kc, kh, kw <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(CI, K, K):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;A&#34;</span>):
</span></span><span style=display:flex><span>                    vb <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(N, b)
</span></span><span style=display:flex><span>                    vc_o <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(CO, o)
</span></span><span style=display:flex><span>                    vh <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(OUT_H, h)
</span></span><span style=display:flex><span>                    vw <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(OUT_W, w)
</span></span><span style=display:flex><span>                    vc_i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(CI, kc)
</span></span><span style=display:flex><span>                    vw_h <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(K, kh)
</span></span><span style=display:flex><span>                    vw_w <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(K, kw)
</span></span></code></pre></div><p>The outer loop all receives <code>T.axis.spatial()</code>, because we access each element in the output matrix element by element (spatially), without doing anything else. On the other hand, we see parameters in the innter loop receives <code>T.axis.reduce()</code>. Remember, each element in the output matrix is just the sum of element-wise multiplication of both the 2D convolutional kernel (1 by 3 by 3) and the corresponding area in the input matrix (1 by 3 by 3). Therefore, after the element-wise multiplication finishes, we need perform a reduction operation over all three axes. More concretely, we will sum up all elements in the row(K), column(K), and channel(CI): (1, 3, 3) -> (1)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyConv</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>conv2d</span>(data: T<span style=color:#f92672>.</span>Buffer[(N, CI, H, W), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                weight: T<span style=color:#f92672>.</span>Buffer[(CO, CI, K, K), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                result: T<span style=color:#f92672>.</span>Buffer[(N, CO, OUT_H, OUT_W), <span style=color:#e6db74>&#34;int64&#34;</span>]):
</span></span><span style=display:flex><span>        T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;conv2d&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        <span style=color:#75715e># impl</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> b, o, h, w <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(N, CO, OUT_H, OUT_W):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> kc, kh, kw <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(CI, K, K):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;A&#34;</span>):
</span></span><span style=display:flex><span>                    vb <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(N, b)
</span></span><span style=display:flex><span>                    vc_o <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(CO, o)
</span></span><span style=display:flex><span>                    vh <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(OUT_H, h)
</span></span><span style=display:flex><span>                    vw <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(OUT_W, w)
</span></span><span style=display:flex><span>                    vc_i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(CI, kc)
</span></span><span style=display:flex><span>                    vw_h <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(K, kh)
</span></span><span style=display:flex><span>                    vw_w <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(K, kw)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>init():
</span></span><span style=display:flex><span>                        result[vb, vc_o, vh, vw] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>int64(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                    <span style=color:#75715e># compute rule</span>
</span></span><span style=display:flex><span>                    result[vb, vc_o, vh, vw] <span style=color:#f92672>+=</span> data[vb, vc_i, vh<span style=color:#f92672>+</span>vw_h, vw<span style=color:#f92672>+</span>vw_w] <span style=color:#f92672>*</span> weight[vc_o, vc_i, vw_h, vw_w]
</span></span></code></pre></div><div style=width:100% id=comment><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=preferred-color-scheme crossorigin=anonymous async></script></div></article></main><footer id=footer><p>Â© 2023 Bodun Hu. All rights reserved.</p></footer></body></html>