<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Dive into TensorIR | std::bodun::blog</title><meta name=title content="Dive into TensorIR"><meta name=description content="TensorIR is a compiler abstraction for optimizing programs with tensor computation primitives in TVM. Imagine a DNN task as a graph, where each node represents a tensor computation. TensorIR explains how each node/tensor computation primitive in the graph is carried out. This post explains my attempt to implement 2D convolution using TensorIR. It is derived from the Machine Learning Compilation course offered by Tianqi Chen.
Implement 2D Convolution 2D convolution is a common operation in image processing."><meta name=keywords content="mlsys,tvm,"><meta property="og:title" content="Dive into TensorIR"><meta property="og:description" content="TensorIR is a compiler abstraction for optimizing programs with tensor computation primitives in TVM. Imagine a DNN task as a graph, where each node represents a tensor computation. TensorIR explains how each node/tensor computation primitive in the graph is carried out. This post explains my attempt to implement 2D convolution using TensorIR. It is derived from the Machine Learning Compilation course offered by Tianqi Chen.
Implement 2D Convolution 2D convolution is a common operation in image processing."><meta property="og:type" content="article"><meta property="og:url" content="https://www.bodunhu.com/blog/posts/dive-into-tensorir/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-28T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-29T23:19:21-05:00"><meta property="og:site_name" content="std::bodun::blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dive into TensorIR"><meta name=twitter:description content="TensorIR is a compiler abstraction for optimizing programs with tensor computation primitives in TVM. Imagine a DNN task as a graph, where each node represents a tensor computation. TensorIR explains how each node/tensor computation primitive in the graph is carried out. This post explains my attempt to implement 2D convolution using TensorIR. It is derived from the Machine Learning Compilation course offered by Tianqi Chen.
Implement 2D Convolution 2D convolution is a common operation in image processing."><meta name=twitter:site content="@https://twitter.com/BodunHu"><meta itemprop=name content="Dive into TensorIR"><meta itemprop=description content="TensorIR is a compiler abstraction for optimizing programs with tensor computation primitives in TVM. Imagine a DNN task as a graph, where each node represents a tensor computation. TensorIR explains how each node/tensor computation primitive in the graph is carried out. This post explains my attempt to implement 2D convolution using TensorIR. It is derived from the Machine Learning Compilation course offered by Tianqi Chen.
Implement 2D Convolution 2D convolution is a common operation in image processing."><meta itemprop=datePublished content="2022-08-28T00:00:00+00:00"><meta itemprop=dateModified content="2022-08-29T23:19:21-05:00"><meta itemprop=wordCount content="876"><meta itemprop=keywords content="mlsys,tvm,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style></head><body><header><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><a href=/blog/ class=title><h2>std::bodun::blog</h2></a><nav><a href=/blog/>Home</a>
<a href=/blog/posts/>Archive</a>
<a href=https://www.bodunhu.com/>About</a></nav></header><main><h1>Dive into TensorIR</h1><p><i><time datetime=2022-08-28 pubdate>28 Aug, 2022</time></i></p><content><p><a href=https://arxiv.org/abs/2207.04296>TensorIR</a> is a compiler abstraction for optimizing programs with tensor computation primitives in <a href=https://tvm.apache.org/>TVM</a>. Imagine a DNN task as a graph, where each node represents a tensor computation. TensorIR explains how each node/tensor computation primitive in the graph is carried out. This post explains my attempt to implement 2D convolution using TensorIR. It is derived from the <a href=https://mlc.ai/summer22/>Machine Learning Compilation</a> course offered by <a href=https://tqchen.com/>Tianqi Chen</a>.</p><h2 id=implement-2d-convolution>Implement 2D Convolution</h2><p>2D convolution is a common operation in image processing. The image below captures how 2D convolution operates. I won&rsquo;t go into details here. But you can find plenty information online regarding convolution.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/dive-into-tensorIR/2d-conv.png alt=2D-convolution></p><p>First, we initialize both the input matrix and the weight matrix:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># batch, input_channel_dim, image_height, image_width, output_channel_dim, kernel_width &amp; height</span>
</span></span><span style=display:flex><span>N, CI, H, W, CO, K <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#75715e># output_height, output_width, assuming kernel has stride=1 and padding=0</span>
</span></span><span style=display:flex><span>OUT_H, OUT_W <span style=color:#f92672>=</span> H <span style=color:#f92672>-</span> K <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, W <span style=color:#f92672>-</span> K <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(N<span style=color:#f92672>*</span>CI<span style=color:#f92672>*</span>H<span style=color:#f92672>*</span>W)<span style=color:#f92672>.</span>reshape(N, CI, H, W)
</span></span><span style=display:flex><span>weight <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(CO<span style=color:#f92672>*</span>CI<span style=color:#f92672>*</span>K<span style=color:#f92672>*</span>K)<span style=color:#f92672>.</span>reshape(CO, CI, K, K)
</span></span></code></pre></div><p>We can validate the results using <code>torch.nn.functional.conv2d()</code> from PyTorch.</p><p>One thing Tianqi recommended for starters is to write the implementation first in numpy, and then translate the numpy implementation to TensorIR. I started my implementation directly from TensorIR, before totally getting confused. So here&rsquo;s how I approach the problem.</p><p>First, and perhaps most importantly, you should figure out the accessing pattern of the output matrix, and gradually fill up the compute rules for each element in the output matrix. So, we know the output matrix has a shape of <code>(N, CO, OUT_H, OUT_w)</code> (which corresponds to batch, number of output channels, output height, and output width). The numpy loop will look like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> b <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, N):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> co <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, CO):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> h <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, OUT_H):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, OUT_W):
</span></span><span style=display:flex><span>                Y[b, co, h, w] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span></code></pre></div><p>Here, we access element in the output matrix one by one and initialize each element to be 0. Next, we will try to figure out how to compute each element. We know each element in the output matrix is just the sum of element-wise multiplication of both the 2D convolutional kernel (1 by 3 by 3) and the corresponding area in the input matrix (1 by 3 by 3):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> b <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, N):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> co <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, CO):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> h <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, OUT_H):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, OUT_W):
</span></span><span style=display:flex><span>                <span style=color:#75715e># init to 0</span>
</span></span><span style=display:flex><span>                Y[b, co, h, w] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># 2d conv kernel</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> ci <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, CI):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>for</span> kh <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, K):
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>for</span> kw <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, K):
</span></span><span style=display:flex><span>                            <span style=color:#75715e># reduction</span>
</span></span><span style=display:flex><span>                            Y[b, co, h, w] <span style=color:#f92672>+=</span> A[b, ci, h<span style=color:#f92672>+</span>kh, w<span style=color:#f92672>+</span>kw] <span style=color:#f92672>*</span> W[co, ci, kh, kw]
</span></span></code></pre></div><p>We can verify the function has the same output as <code>torch.nn.functional.conv2d()</code> from PyTorch.</p><p>The next part is to translate the numpy code into TensorIR. I won&rsquo;t go into every the details of every single line here, but you can find all explanations from this <a href=https://mlc.ai/chapter_tensor_program/case_study.html>note</a>.</p><p>The nested loop can be encapsulated using <code>T.grid()</code> like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyConv</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>conv2d</span>(data: T<span style=color:#f92672>.</span>Buffer[(N, CI, H, W), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                weight: T<span style=color:#f92672>.</span>Buffer[(CO, CI, K, K), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                result: T<span style=color:#f92672>.</span>Buffer[(N, CO, OUT_H, OUT_W), <span style=color:#e6db74>&#34;int64&#34;</span>]):
</span></span><span style=display:flex><span>        T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;conv2d&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        <span style=color:#75715e># loop through each elem in the output matrix</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> b, o, h, w <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(N, CO, OUT_H, OUT_W):
</span></span><span style=display:flex><span>            <span style=color:#75715e># kernel access pattern</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> kc, kh, kw <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(CI, K, K):
</span></span></code></pre></div><p>Next, we define the block (a basic unit of computation in TensorIR). A block contains a set of block axes <code>(vi, vj, vk)</code> and computations defined around them. Here, we define the property about each block axes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyConv</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>conv2d</span>(data: T<span style=color:#f92672>.</span>Buffer[(N, CI, H, W), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                weight: T<span style=color:#f92672>.</span>Buffer[(CO, CI, K, K), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                result: T<span style=color:#f92672>.</span>Buffer[(N, CO, OUT_H, OUT_W), <span style=color:#e6db74>&#34;int64&#34;</span>]):
</span></span><span style=display:flex><span>        T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;conv2d&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        <span style=color:#75715e># impl</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> b, o, h, w <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(N, CO, OUT_H, OUT_W):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> kc, kh, kw <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(CI, K, K):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;A&#34;</span>):
</span></span><span style=display:flex><span>                    vb <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(N, b)
</span></span><span style=display:flex><span>                    vc_o <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(CO, o)
</span></span><span style=display:flex><span>                    vh <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(OUT_H, h)
</span></span><span style=display:flex><span>                    vw <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(OUT_W, w)
</span></span><span style=display:flex><span>                    vc_i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(CI, kc)
</span></span><span style=display:flex><span>                    vw_h <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(K, kh)
</span></span><span style=display:flex><span>                    vw_w <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(K, kw)
</span></span></code></pre></div><p>The outer loop all receives <code>T.axis.spatial()</code>, because we access each element in the output matrix element by element (spatially), without doing anything else. On the other hand, we see parameters in the innter loop receives <code>T.axis.reduce()</code>. Remember, each element in the output matrix is just the sum of element-wise multiplication of both the 2D convolutional kernel (1 by 3 by 3) and the corresponding area in the input matrix (1 by 3 by 3). Therefore, after the element-wise multiplication finishes, we need perform a reduction operation over all three axes. More concretely, we will sum up all elements in the row(K), column(K), and channel(CI): (1, 3, 3) -> (1)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@tvm.script.ir_module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyConv</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@T.prim_func</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>conv2d</span>(data: T<span style=color:#f92672>.</span>Buffer[(N, CI, H, W), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                weight: T<span style=color:#f92672>.</span>Buffer[(CO, CI, K, K), <span style=color:#e6db74>&#34;int64&#34;</span>],
</span></span><span style=display:flex><span>                result: T<span style=color:#f92672>.</span>Buffer[(N, CO, OUT_H, OUT_W), <span style=color:#e6db74>&#34;int64&#34;</span>]):
</span></span><span style=display:flex><span>        T<span style=color:#f92672>.</span>func_attr({<span style=color:#e6db74>&#34;global_symbol&#34;</span>: <span style=color:#e6db74>&#34;conv2d&#34;</span>, <span style=color:#e6db74>&#34;tir.noalias&#34;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>        <span style=color:#75715e># impl</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> b, o, h, w <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(N, CO, OUT_H, OUT_W):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> kc, kh, kw <span style=color:#f92672>in</span> T<span style=color:#f92672>.</span>grid(CI, K, K):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>block(<span style=color:#e6db74>&#34;A&#34;</span>):
</span></span><span style=display:flex><span>                    vb <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(N, b)
</span></span><span style=display:flex><span>                    vc_o <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(CO, o)
</span></span><span style=display:flex><span>                    vh <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(OUT_H, h)
</span></span><span style=display:flex><span>                    vw <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>spatial(OUT_W, w)
</span></span><span style=display:flex><span>                    vc_i <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(CI, kc)
</span></span><span style=display:flex><span>                    vw_h <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(K, kh)
</span></span><span style=display:flex><span>                    vw_w <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>axis<span style=color:#f92672>.</span>reduce(K, kw)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>with</span> T<span style=color:#f92672>.</span>init():
</span></span><span style=display:flex><span>                        result[vb, vc_o, vh, vw] <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>int64(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                    <span style=color:#75715e># compute rule</span>
</span></span><span style=display:flex><span>                    result[vb, vc_o, vh, vw] <span style=color:#f92672>+=</span> data[vb, vc_i, vh<span style=color:#f92672>+</span>vw_h, vw<span style=color:#f92672>+</span>vw_w] <span style=color:#f92672>*</span> weight[vc_o, vc_i, vw_h, vw_w]
</span></span></code></pre></div></content><p><a href=https://www.bodunhu.com/blog/tags/mlsys/>#mlsys</a>
<a href=https://www.bodunhu.com/blog/tags/tvm/>#tvm</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>