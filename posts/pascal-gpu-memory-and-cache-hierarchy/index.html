<!doctype html><html lang=en-us><head><title>Pascal GPU memory and cache hierarchy | std::bodun::blog</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program's performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation. Turing architecture might have different results, but the method we used here can be applied as well with slight modification. The method we use in this guide is inspired by the research paper: [Dissecting GPU Memory Hierarchy through Microbenchmarking](https://ieeexplore.ieee.org/document/7445236). Here we will explain how P-Chase works and walk through a small example"><meta name=generator content="Hugo 0.68.3"><meta name=ROBOTS content="INDEX, FOLLOW"><link rel=stylesheet href=/blog/css/style.css><link rel="shortcut icon" href=/blog/images/favicon.ico type=image/x-icon><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-108144808-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Pascal GPU memory and cache hierarchy"><meta name=twitter:description content="Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program's performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation. Turing architecture might have different results, but the method we used here can be applied as well with slight modification. The method we use in this guide is inspired by the research paper: [Dissecting GPU Memory Hierarchy through Microbenchmarking](https://ieeexplore.ieee.org/document/7445236). Here we will explain how P-Chase works and walk through a small example"><meta name=twitter:site content="@https://twitter.com/BodunHu"><script>document.getElementById("scroll-to-top").addEventListener("click",function(){window.scrollTo({top:0,left:0,behavior:'smooth'});});</script></head><body><nav class=navigation><a href=/blog/><span class=arrow>←</span>Home</a>
<a href=/blog/posts>Archive</a>
<a href=/blog/tags>Tags</a>
<a href=https://www.bodunhu.com/>About</a>
<a class=button href=https://www.bodunhu.com/blog/index.xml>Subscribe</a></nav><main class=main><section id=single><h1 class=title>Pascal GPU memory and cache hierarchy</h1><div class=tip><time datetime="2019-01-15 00:00:00 +0000 UTC">Jan 15, 2019</time>
<span class=split>·</span>
<span>10 minute read</span></div><div class=content><p>Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program&rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation. Turing architecture might have different results, but the method we used here can be applied as well with slight modification. The method we use in this guide is inspired by the research paper: <a href=https://ieeexplore.ieee.org/document/7445236 target=_blank rel=noopener>Dissecting GPU Memory Hierarchy through Microbenchmarking</a>. Here we will explain how P-Chase works and walk through a small example.</p><h2 id=memory-hierarchy-overview>Memory Hierarchy Overview</h2><p>GPU memory hierarchy is different compared to CPU memory hierarchy. Using the terminologies of CUDA, GPU memory space can be categorized in these groups: register, constant memory, shared memory, texture memory, local memory, and global memory. Each different memory space have its own properties. Since we are interested the
cache systems, here is a picture demonstrating the memory hierarchy of a NVIDIA GPU:</p><p align=center><img src=https://gistbok.ucgis.org/sites/default/files/1000px-Memory.svg_.png width=450></p><p align=center><a href=https://gistbok.ucgis.org/bok-topics/graphics-processing-units-gpus>Image source</a></p><p>The characteristics of each memory space can be found in <a href=https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA_C_Programming_Guide.pdf target=_blank rel=noopener>NVIDIA CUDA C Programming Guide
</a>. Here we will focus on some target memory space we are interested in. The paper lists some properties of our target memory space:</p><table><thead><tr><th>Memory</th><th align=center>Type</th><th align=right>Cached</th><th align=right>Scope</th></tr></thead><tbody><tr><td>Global</td><td align=center>R/W</td><td align=right>Yes</td><td align=right>All Threads</td></tr><tr><td>Shared</td><td align=center>R/W</td><td align=right>N/A</td><td align=right>Thread Blocks</td></tr><tr><td>Texture</td><td align=center>R</td><td align=right>Yes</td><td align=right>All Threads</td></tr></tbody></table><p>Even though the paper targets Fermi, Kepler and Maxwell generations of GPU, the properties of the table still holds for Pascal GPU and possibly Turing as well. The cached global/texture memory uses a two-level caching system. The L1 cache is located in each stream multiprocessor (SM), while the L2 cache is off-chip and shared among all SMs. It is unified for instruction, data and page table access. According to CUDA documentation, like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. This function previously was served by the separate L1 cache in Fermi and Kepler. Page table is used by GPU to map virtual addresses to physical addresses, and is usually stored in the global memory. The page table is cached in TLB to reduce memory access latency. Once a thread cannot ﬁnd the page entry in the TLB, it would access the global memory to search in the page table, which introduced significant memory access latency. The GPU-specific shared memory is located in the SMs. On the Fermi and Kepler devices, it shares memory space with the L1 data cache. On Maxwell and Pascal devices, it has a dedicated space, since the functionality of the L1 and texture caches have been merged. One thing to note here is that shared memory is accessed by the thread blocks. Thread-blocks remain limited to 48 KB of shared memory in Pascal. Therefore, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block. This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM.</p><p>However, we should be careful that by default, GP100 caches global loads in the L1/Texture cache. In contrast, GP104 follows Kepler and Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler. As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to <code>nvcc</code> at compile time. Even though both GP100 and GP104 belongs to Pascal family, we only focus on GP100 here because that&rsquo;s the GPU we use. Another thing to notice is that unlike Maxwell but similar to Kepler, Pascal caches thread-local memory in the L1 cache. This can mitigate the cost of register spills compared to Maxwell. To illustrate our point, we checked both cudaDevAttrGlobalL1CacheSupported and cudaDevAttrLocalL1CacheSupported on Tesla P100 and GTX 1080 and find both attributes to be 1.</p><p>In addition to the L2 data cache, global memory data that is read-only for the entire lifetime of a kernel can be cached in the read-only data cache with a compute capability of 3.5 or above. We will also explore the size of this read-only cache using __ldg() intrinsic.</p><h2 id=p-chase>P-Chase</h2><p>Most existing GPU microbenchmark studies on cache architecture assume a classical set-associative cache model with the least recently used (LRU) replacement policy, the same as the conventional CPU cache. So here we will use this assumption and proceed with our experiments. Here are some notations we will use throughout this post.</p><table><thead><tr><th>Notation</th><th align=center>Description</th><th align=right>Notation</th><th align=right>Description</th></tr></thead><tbody><tr><td>C</td><td align=center>Cache Size</td><td align=right>N</td><td align=right>array size</td></tr><tr><td>b</td><td align=center>cache line size</td><td align=right>s</td><td align=right>stride size</td></tr><tr><td>a</td><td align=center>cache associativity</td><td align=right>k</td><td align=right>iterations</td></tr><tr><td>T</td><td align=center>number of cache set</td><td align=right>r</td><td align=right>cache miss rate</td></tr></tbody></table><p>Under our assumptions, data is loaded from main memory to lower cache in the basic unit of a cache line. The number of words in a cache line is referred to as the line size (b). For the LRU set-associative cache, the cache memory is divided into T cache sets, each of which consists of \(a\) cache lines. It is essential to have these three assumptions using this kind of cache model:</p><ul><li><p><strong>Assumption 1</strong> All cache sets have the same size. The cache parameter should satisfy \(T \cdot a \cdot b = C\).</p></li><li><p><strong>Assumption 2</strong> In the memory address, the bits representing the cache set are immediately followed by the bits representing the offset.</p></li><li><p><strong>Assumption 3</strong> Cache replacement policy should be LRU.</p></li></ul><p>We will later see why these assumptions are essential as we proceed with the experiment. We won&rsquo;t go through how P-Chase work exactly. To find more information, this <a href=https://arxiv.org/pdf/1509.02308.pdf target=_blank rel=noopener>paper</a> does a good job illustrating how P-Chase work. The takeaway is, we need to brute force an array with one element more than a cache can hold so that cache miss will start to occur periodically whereas such array with less or equal elements to the cache capacity will always result in cache hit and thus no access overhead will be introduced after all data is loaded into the cache. This is the algorithm the paper proposed and we will use it to do the experiment:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>__global__ <span style=color:#0b0;font-weight:700>void</span> <span style=color:#00a000>KernelFunction</span> ( . . . ) {
    <span style=color:#080;font-style:italic>//declare shared memory space
</span><span style=color:#080;font-style:italic></span>    __shared__ <span style=color:#0b0;font-weight:700>unsigned</span> <span style=color:#0b0;font-weight:700>int</span> s tvalue [ ] ;
    __shared__ <span style=color:#0b0;font-weight:700>unsigned</span> <span style=color:#0b0;font-weight:700>int</span> s index [ ] ;
    preheat the data ; <span style=color:#080;font-style:italic>// implementation varies
</span><span style=color:#080;font-style:italic></span>    <span style=color:#a2f;font-weight:700>for</span> (it <span style=color:#666>=</span> <span style=color:#666>0</span>; it <span style=color:#666>&lt;</span> iter ; it<span style=color:#666>++</span>) {
        start_time<span style=color:#666>=</span>clock();
        j <span style=color:#666>=</span> my_array[j];
        <span style=color:#080;font-style:italic>//store the array index
</span><span style=color:#080;font-style:italic></span>
        <span style=color:#080;font-style:italic>// This following line is essential because due to
</span><span style=color:#080;font-style:italic></span>        <span style=color:#080;font-style:italic>// instruction-level parallelism (ILP), function clock() may
</span><span style=color:#080;font-style:italic></span>        <span style=color:#080;font-style:italic>// overlap with its previous instruction and even return before
</span><span style=color:#080;font-style:italic></span>        <span style=color:#080;font-style:italic>// the previous instruction finishes. For example,
</span><span style=color:#080;font-style:italic></span>        <span style=color:#080;font-style:italic>// end_time=clock() can return before j = my_array[j] returns.
</span><span style=color:#080;font-style:italic></span>        <span style=color:#080;font-style:italic>// adding s_index [it]= j since it have data dependency on the
</span><span style=color:#080;font-style:italic></span>        <span style=color:#080;font-style:italic>// previous line. Thus the memory access will be over before
</span><span style=color:#080;font-style:italic></span>        <span style=color:#080;font-style:italic>// end_time=clock() started.
</span><span style=color:#080;font-style:italic></span>        s_index [it]<span style=color:#666>=</span> j;
        end_time<span style=color:#666>=</span>clock();
        <span style=color:#080;font-style:italic>//store the access latency
</span><span style=color:#080;font-style:italic></span>        s_tvalue[it]<span style=color:#666>=</span> end_time<span>−</span>start_time ;
    }
}
</code></pre></div><p>The steps is the same as the paper proposes, so here we show the paper&rsquo;s method:</p><ol><li><p>Determine cache size C . We set s to 1. We then initialize N with a small value and increase it gradually until the ﬁrst cache miss appears. C equals the maximum N where all memory accesses are cache hits.</p></li><li><p>Determine cache line size b. We set s to 1. We begin with N = C + 1 and increase N gradually again. When N &lt; C + b + 1, the numbers of cache misses are close. When N is increased to C + b + 1, there is a sudden increase on the number of cache misses, despite that we only increase N by 1. Accordingly we can ﬁnd b. Based on the memory access patterns, we can also have a general idea on the cache replacement policy.</p></li><li><p>Determine number of cache sets T . We set s to b. We then start with N = C and increase N at the granularity of b. Every increment causes cache misses of a new cache set. When N > C + (T − 1)b, all cache sets are missed. We can then deduce T from cache miss patterns accordingly.</p></li><li><p>Determine cache replacement policy. As mentioned before, if the cache replacement policy is LRU, then the memory access process should be periodic and all the cache ways in the cache set are missed. If memory access process is aperiodic, then the replacement policy cannot be LRU. Under this circumstance, we set N = C + b, s = b with a considerable large k (k &#187; N/s) so that we can traverse the array multiple times. All cache misses are from one cache set. Every cache miss is caused by its former cache replacement because we overﬂow the cache by only one cache line. We have the accessed data indices thus we can reproduce the full memory access process and ﬁnd how the cache lines are updated.</p></li></ol><h2 id=texture-l1-cache-and-read-only-data-cache>Texture L1 Cache and Read-only Data Cache</h2><p>When use the <a href=http://www.comp.hkbu.edu.hk/~chxw/Code/fine_grain_Maxwell_texture_L1.cu target=_blank rel=noopener>code</a> with increased our own data preheat implementation because the texture L1 cache can potentially be greater than the shared memory. The original code uses the first iteration of the loop in the algorithm as a way to preheat data:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#a2f;font-weight:700>const</span> <span style=color:#0b0;font-weight:700>int</span> it <span style=color:#666>=</span>  <span style=color:#666>6144</span> <span style=color:#080;font-style:italic>// texture L1 may hold more elements,
</span><span style=color:#080;font-style:italic></span>                     <span style=color:#080;font-style:italic>// So the first iteration may not cold
</span><span style=color:#080;font-style:italic></span>                     <span style=color:#080;font-style:italic>// hit all elements, some cold hits can
</span><span style=color:#080;font-style:italic></span>                     <span style=color:#080;font-style:italic>// be moved to the second iteration,
</span><span style=color:#080;font-style:italic></span>                     <span style=color:#080;font-style:italic>// causing confusion
</span><span style=color:#080;font-style:italic></span><span style=color:#a2f;font-weight:700>for</span> (<span style=color:#0b0;font-weight:700>int</span> cnt<span style=color:#666>=</span><span style=color:#666>0</span>; cnt <span style=color:#666>&lt;</span> it; cnt<span style=color:#666>++</span>) {
    start<span style=color:#666>=</span>clock();
    j<span style=color:#666>=</span>tex1Dfetch(tex_ref, j);
    s_value[cnt] <span style=color:#666>=</span> j;
    end<span style=color:#666>=</span>clock();
    s_tvalue[cnt] <span style=color:#666>=</span> (end <span style=color:#666>-</span>start);
}
</code></pre></div><p>However, if texture L1 cache is greater than the shared memory allowed for each thread block, then some reads in the second loop will trigger cache misses. But such misses are in fact cold misses, not misses caused after the texture L1 cache is completely filled up. One solution is increase iteration to a much larger number so that the first iteration will always fill up the texture L1 cache. Note that if you move the data
preheat out such as</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#a2f;font-weight:700>for</span> (<span style=color:#0b0;font-weight:700>int</span> cnt<span style=color:#666>=</span><span style=color:#666>0</span>; cnt <span style=color:#666>&lt;</span> it; cnt<span style=color:#666>++</span>) {
        tmp<span style=color:#666>=</span>tex1Dfetch(tex_ref, tmp);
}
</code></pre></div><p>The compiler can optimize this whole step out and thus nothing actually gets executed.</p><p>After we run the modified code, the result shows that the we the cache missed starts when we set our array size to 6145, indicating the texture L1 cache can hold 6144 ints, which is equivalent to 24 kb. We also notice that each miss is followed by 7 consecutive hits. This means the cache line size is 8 words(b = 32 bytes). The structure of the L1 TLB is shown below, notice there are 192 lines in each set:</p><table><thead><tr><th>Set1</th><th align=center>Set2</th><th align=right>Set3</th><th align=right>Set4</th></tr></thead><tbody><tr><td>1-8</td><td align=center>33-40</td><td align=right>65-72</td><td align=right>97-104</td></tr><tr><td>9-16</td><td align=center>41-48</td><td align=right>&mldr;</td><td align=right>&mldr;</td></tr><tr><td>17-24</td><td align=center>46-56</td><td align=right>&mldr;</td><td align=right>&mldr;</td></tr><tr><td>25-32</td><td align=center>57-64</td><td align=right>89-96</td><td align=right>121-128</td></tr><tr><td>129-136</td><td></td><td></td><td></td></tr><tr><td>&mldr;</td><td align=center>&mldr;</td><td align=right>&mldr;</td><td align=right>&mldr;</td></tr><tr><td>2969-2976</td><td align=center>3001-3008</td><td align=right>3033-3040</td><td align=right>3065-3072</td></tr></tbody></table><p>According to CUDA documentation, GK110 adds the ability for read-only data in global memory to be loaded through the same cache used by the texture pipeline via a standard pointer without the need to bind a texture beforehand and without the sizing limitations of standard textures. The read-only data cache is loaded by calling __ldg(const restricted * address). We modified the code used to test texture L1 cache. The basic logic remains the same. When the arrays size is set to 6144 integers no cache misses occur with stride set as 32 (s=32 bytes). As soon as we increased one more element in the array cache misses start occurring. This shows the read-only cache is 24kb. We then noticed that the misses occur in a group of either 4 or 8. We infer the cache line to be 32 bytes and the replacement policy is LRU, same as Maxwell. We we increase the array to include 6248 elements(6144+32<em>3+8, 6144 is the max capacity of the cache, 32 consecutive number in a set, 32</em>3 to cause cache miss in set1, set2, and set3, only need to include 8 more to cause cache miss in set4 since s=32bytes), no caches hits occur. Therefore, we infer the caches set number to be 4, each cache line is 32 bytes, and each set contains 192 cache lines, the same as the texture L1 cache. The memory mapping seems arbitrary because the hit and miss patterns didn&rsquo;t follow that of the texture L1 cache.</p></div><div class=tags><a href=https://www.bodunhu.com/blog/tags/gpu>gpu</a>
<a href=https://www.bodunhu.com/blog/tags/os>os</a>
<a href=https://www.bodunhu.com/blog/tags/mm>mm</a></div><div id=comment><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></section></main><footer id=footer><div id=social><a class=symbol href=https://github.com/BDHU rel=me target=_blank><svg fill="#bbb" width="28" height="28" viewBox="0 0 72 72" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>Github</title><desc>Created with Sketch.</desc><defs/><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Social-Icons---Rounded-Black" transform="translate(-264.000000, -939.000000)"><g id="Github" transform="translate(264.000000, 939.000000)"><path d="M8 72H64C68.418278 72 72 68.418278 72 64V8c0-4.418278-3.581721999999999-8-8-8H8c-4.418278 811624501e-24-8 3.581722-8 8V64C541083001e-24 68.418278 3.581722 72 8 72z" id="Rounded" fill="#bbb"/><path d="M35.9985 13C22.746 13 12 23.7870921 12 37.096644 12 47.7406712 18.876 56.7718301 28.4145 59.9584121 29.6145 60.1797862 30.0525 59.4358488 30.0525 58.7973276 30.0525 58.2250681 30.0315 56.7100863 30.0195 54.6996482 23.343 56.1558981 21.9345 51.4693938 21.9345 51.4693938 20.844 48.6864054 19.2705 47.9454799 19.2705 47.9454799 17.091 46.4500754 19.4355 46.4801943 19.4355 46.4801943 21.843 46.6503662 23.1105 48.9634994 23.1105 48.9634994 25.2525 52.6455377 28.728 51.5823398 30.096 50.9649018 30.3135 49.4077535 30.9345 48.3460615 31.62 47.7436831 26.2905 47.1352808 20.688 45.0691228 20.688 35.8361671 20.688 33.2052792 21.6225 31.0547881 23.1585 29.3696344 22.911 28.7597262 22.0875 26.3110578 23.3925 22.9934585 23.3925 22.9934585 25.4085 22.3459017 29.9925 25.4632101 31.908 24.9285993 33.96 24.6620468 36.0015 24.6515052 38.04 24.6620468 40.0935 24.9285993 42.0105 25.4632101 46.5915 22.3459017 48.603 22.9934585 48.603 22.9934585 49.9125 26.3110578 49.089 28.7597262 48.8415 29.3696344 50.3805 31.0547881 51.309 33.2052792 51.309 35.8361671 51.309 45.0917119 45.6975 47.1292571 40.3515 47.7256117 41.2125 48.4695491 41.9805 49.9393525 41.9805 52.1877301 41.9805 55.4089489 41.9505 58.0067059 41.9505 58.7973276 41.9505 59.4418726 42.3825 60.1918338 43.6005 59.9554002 53.13 56.7627944 60 47.7376593 60 37.096644 60 23.7870921 49.254 13 35.9985 13" fill="#fff"/></g></g></g></svg></a><a class=symbol href=https://twitter.com/BodunHu rel=me target=_blank><svg fill="#bbb" width="28" height="28" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="438.536" height="438.536" viewBox="0 0 438.536 438.536" style="enable-background:new 0 0 438.536 438.536"><g><path d="M414.41 24.123C398.333 8.042 378.963.0 356.315.0H82.228C59.58.0 40.21 8.042 24.126 24.123 8.045 40.207.003 59.576.003 82.225v274.084c0 22.647 8.042 42.018 24.123 58.102 16.084 16.084 35.454 24.126 58.102 24.126h274.084c22.648.0 42.018-8.042 58.095-24.126 16.084-16.084 24.126-35.454 24.126-58.102V82.225C438.532 59.576 430.49 40.204 414.41 24.123zM335.471 168.735c.191 1.713.288 4.278.288 7.71.0 15.989-2.334 32.025-6.995 48.104-4.661 16.087-11.8 31.504-21.416 46.254-9.606 14.749-21.074 27.791-34.396 39.115-13.325 11.32-29.311 20.365-47.968 27.117-18.648 6.762-38.637 10.143-59.953 10.143-33.116.0-63.76-8.952-91.931-26.836 4.568.568 9.329.855 14.275.855 27.6.0 52.439-8.565 74.519-25.7-12.941-.185-24.506-4.179-34.688-11.991-10.185-7.803-17.273-17.699-21.271-29.691 4.947.76 8.658 1.137 11.132 1.137 4.187.0 9.042-.76 14.56-2.279-13.894-2.669-25.598-9.562-35.115-20.697-9.519-11.136-14.277-23.84-14.277-38.114v-.571c10.085 4.755 19.602 7.229 28.549 7.422-17.321-11.613-25.981-28.265-25.981-49.963.0-10.66 2.758-20.747 8.278-30.264 15.035 18.464 33.311 33.213 54.816 44.252 21.507 11.038 44.54 17.227 69.092 18.558-.95-3.616-1.427-8.186-1.427-13.704.0-16.562 5.853-30.692 17.56-42.399 11.703-11.706 25.837-17.561 42.394-17.561 17.515.0 32.079 6.283 43.688 18.846 13.134-2.474 25.892-7.33 38.26-14.56-4.757 14.652-13.613 25.788-26.55 33.402 12.368-1.716 23.88-4.95 34.537-9.708C357.458 149.793 347.462 160.166 335.471 168.735z"/></g></svg></a></div><div class=copyright>©
2016-2021
<a href=https://www.bodunhu.com/>Bodun Hu</a>. All rights reserved.</div><div class=powerby>Powered by <a href=http://www.gohugo.io/>Hugo</a> Theme By <a href=https://github.com/nodejh/hugo-theme-cactus-plus>nodejh</a></div><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link><i class="fa fa-chevron-up"></i></button></a></footer></body></html>