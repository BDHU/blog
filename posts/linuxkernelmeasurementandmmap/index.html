<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Linux Program Measurement and mmap | std::bodun::blog</title><meta name=keywords content="os,kernel,mm,linux"><meta name=description content="This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to cpu world. This is the system spec:
   Attribute Value     Processor name (BIOS) Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz   Cores 6   Logical processors 12   TLB/Cache details 64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries."><meta name=author content><link rel=canonical href=https://www.bodunhu.com/blog/posts/linuxkernelmeasurementandmmap/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.758151a3f029732139c5ed917da59127bed78e4d7fce57610c1cd24d4cb52b2a.css integrity="sha256-dYFRo/ApcyE5xe2RfaWRJ77Xjk1/zldhDBzSTUy1Kyo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.utexas.edu/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.utexas.edu/favicon-32x32.png><link rel=apple-touch-icon href=https://www.utexas.edu/apple-touch-icon.png><link rel=mask-icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-108144808-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Linux Program Measurement and mmap"><meta property="og:description" content="This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to cpu world. This is the system spec:
   Attribute Value     Processor name (BIOS) Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz   Cores 6   Logical processors 12   TLB/Cache details 64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries."><meta property="og:type" content="article"><meta property="og:url" content="https://www.bodunhu.com/blog/posts/linuxkernelmeasurementandmmap/"><meta property="og:image" content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-09-23T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-23T00:00:00+00:00"><meta property="og:site_name" content="Bodun's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Linux Program Measurement and mmap"><meta name=twitter:description content="This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to cpu world. This is the system spec:
   Attribute Value     Processor name (BIOS) Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz   Cores 6   Logical processors 12   TLB/Cache details 64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.bodunhu.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Linux Program Measurement and mmap","item":"https://www.bodunhu.com/blog/posts/linuxkernelmeasurementandmmap/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Linux Program Measurement and mmap","name":"Linux Program Measurement and mmap","description":"This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to cpu world. This is the system spec:\n   Attribute Value     Processor name (BIOS) Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz   Cores 6   Logical processors 12   TLB/Cache details 64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries.","keywords":["os","kernel","mm","linux"],"articleBody":"This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to cpu world. This is the system spec:\n   Attribute Value     Processor name (BIOS) Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz   Cores 6   Logical processors 12   TLB/Cache details 64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries. Plus, 1-GB pages, 4-way, 16 entries   RAM 32GB   Operating System Ubuntu 20.04.1 LTS   Kernel Version 5.4.0-47-generic    \n 8-way set associative means the CPU cache is made up of sets that can fit 8 blocks each.\n  Here are the details for the CPU cache, which we will need later:\n   Cache L1 data L1 instruction L2 L3     Size 6 x 32 KB 6 x 32 KB 6 x 256 KB 15 MB   Associativity 8-way set associative 8-way set associative 8-way set associative 20-way set associative   Line size: 64 bytes 64 bytes 64 bytes 64 bytes   Comments: Direct-mapped Direct-mapped Non-inclusive Direct-mapped Inclusive Shared between all cores    \nMemory Map To print the /proc/self/maps file for a process, we use the sprintf to construct the file name and then use the system from stdlib to cat the contents of the running processâ€™s address space. If we execute the program, it shows (also available on gist)\naddress perms offset dev inode pathname 559e3e51f000-559e3e520000 r--p 00000000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 559e3e520000-559e3e521000 r-xp 00001000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 559e3e521000-559e3e522000 r--p 00002000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 559e3e522000-559e3e523000 r--p 00002000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 559e3e523000-559e3e524000 rw-p 00003000 00:31 1199787 /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out 7faf5c477000-7faf5c49c000 r--p 00000000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c49c000-7faf5c614000 r-xp 00025000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c614000-7faf5c65e000 r--p 0019d000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c65e000-7faf5c65f000 ---p 001e7000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c65f000-7faf5c662000 r--p 001e7000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c662000-7faf5c665000 rw-p 001ea000 08:22 11932543 /usr/lib/x86_64-linux-gnu/libc-2.31.so 7faf5c665000-7faf5c66b000 rw-p 00000000 00:00 0 7faf5c685000-7faf5c686000 r--p 00000000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c686000-7faf5c6a9000 r-xp 00001000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c6a9000-7faf5c6b1000 r--p 00024000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c6b2000-7faf5c6b3000 r--p 0002c000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c6b3000-7faf5c6b4000 rw-p 0002d000 08:22 11932535 /usr/lib/x86_64-linux-gnu/ld-2.31.so 7faf5c6b4000-7faf5c6b5000 rw-p 00000000 00:00 0 7ffcddb8d000-7ffcddbae000 rw-p 00000000 00:00 0 [stack] 7ffcddbe0000-7ffcddbe3000 r--p 00000000 00:00 0 [vvar] 7ffcddbe3000-7ffcddbe4000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] Based on the linux man page, we can see each column has different definition. The address field is the address space in the process that the mapping occupies. The perms field is a set of permissions:\n r = read w = write x = execute s = shared p = private (copy on write)  The offset field is the offset into the file/whatever; dev is the device (major:minor); inode is the inode on that device. 0 indicates that no inode is associated with the memory region, as would be the case with BSS (uninitialized data).\nThe pathname field will usually be the file that is backing the mapping. For ELF files, you can easily coordinate with the offset field by looking at the Offset field in the ELF program headers (readelf -l). In addition, we can see a few other pseudo-paths:\n  [stack]: the initial processâ€™s (also known as the main threadâ€™s) stack.\n  [vdso]: The virtual dynamically linked shared object. More detailed descriptions can be found on lwn.\n  [vvar]: location of kernel space variables mapped in user space needed by virtual system calls. Essentially, a kernel-space physical address is mapped into the userspace.\n  [vsyscall]: similar to vDSO, vsyscall is another segment used to accelerate certain system calls in Linux. Vsyscall has some limitations; among other things, there is only space for a handful of virtual system calls. More detailed descriptions can be found on lwn.\n  One thing interesting here is that when we execute the same program twice, we can see after the first run, the output is\n7fffbc92f000-7fffbc930000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] Type the same command again:\n7ffd6a94d000-7ffd6a94e000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] Note that the vDSO area has moved, while the vsyscall page remains at the same location. The location of the vsyscall page is nailed down in the kernel ABI, but the vDSO area - like most other areas in the user-space memory layout - has its location randomized every time it is mapped. The vsyscall is legacy implementation of user-space sys call acceleration. Since it has fixed addresses, it is vulnerable to security issues. Because applications depend on the existence and exact address of that page, most functions are simply removed and replaced by a special trap instruction. More detailed explanation can be found on lwn.net.\nAnother interesting thing we observed is the base address of the executable (the start of the text section) and the start address of libc is rather different. This is also the result of using ASLR which is used to prevent return-to-libc attack.\ngetrusage Then, we call getrusage at the end of our program and print out the fields. We will need getrusage later. Here is a sample output for some fields inside struct rusage:\nutime: 1306 stime: 0 maxrss: 2692 minflt: 76 majflt: 0 inblock: 0 oublock: 0 nvcsw: 2 nivcsw: 0 Here is a short list of descriptions for each of these fields. More detailed information can be found on gnu website\n utime: time spent executing user instructions. stime: time spent in operating system code on behalf of processes. maxrss: the maximum resident set size used, in kilobytes. That is, the maximum number of kilobytes of physical memory that processes used simultaneously. minflt: the number of page faults which were serviced without requiring any I/O. majflt: the number of page faults which were serviced by doing I/O. inblock: the number of times the file system had to read from the disk on behalf of processes. oublock: the number of times the file system had to write to the disk on behalf of processes. nvcsw: the number of times processes voluntarily invoked a context switch (usually to wait for some service). nivcsw: the number of times an involuntary context switch took place (because a time slice expired, or another process of higher priority was scheduled).  perf_event_open perf_event_open interface is useful to measurement numerous system events. However, glibc doesnâ€™t provide wrapper for this system call. Instead, we need to use syscall directly.\nTo use perf_event_open, we call create a function wrapper that does the actual syscall for us. Take the example from the Linux man page\nstatic int perf_event_open(struct perf_event_attr *hw_event, pid_t pid, int cpu, int group_fd, unsigned long flags) { int ret; ret = syscall(__NR_perf_event_open, hw_event, pid, cpu, group_fd, flags); return ret; } Here the __NR_perf_event_open specifies the syscall number. On our local machine, we can go to /usr/include/x86_64-linux-gnu/sys/syscall.h, which specifies the location of __NR_perf_event_open. In our case, it is located at /usr/include/x86_64-linux-gnu/asm/unistd_64.h.\nIf we call objdump -d on the binary file, we will see something like this\n000000000000119a : 119a:\t55 push %rbp 119b:\t48 89 e5 mov %rsp,%rbp 119e:\t48 83 ec 30 sub $0x30,%rsp 11a2:\t48 89 7d e8 mov %rdi,-0x18(%rbp) 11a6:\t89 75 e4 mov %esi,-0x1c(%rbp) 11a9:\t89 55 e0 mov %edx,-0x20(%rbp) 11ac:\t89 4d dc mov %ecx,-0x24(%rbp) 11af:\t4c 89 45 d0 mov %r8,-0x30(%rbp) 11b3:\t48 8b 7d d0 mov -0x30(%rbp),%rdi 11b7:\t8b 75 dc mov -0x24(%rbp),%esi 11ba:\t8b 4d e0 mov -0x20(%rbp),%ecx 11bd:\t8b 55 e4 mov -0x1c(%rbp),%edx 11c0:\t48 8b 45 e8 mov -0x18(%rbp),%rax 11c4:\t49 89 f9 mov %rdi,%r9 11c7:\t41 89 f0 mov %esi,%r8d 11ca:\t48 89 c6 mov %rax,%rsi 11cd:\tbf 2a 01 00 00 mov $0x12a,%edi 11d2:\tb8 00 00 00 00 mov $0x0,%eax 11d7:\te8 84 fe ff ff callq 1060  11dc:\t89 45 fc mov %eax,-0x4(%rbp) 11df:\t8b 45 fc mov -0x4(%rbp),%eax 11e2:\t48 98 cltq 11e4:\tc9 leaveq 11e5:\tc3 retq We notice thereâ€™s one interesting line\ncallq 1060  The plt stands for Procedure Linkage Table. This lines indicates a call to the syscall in the procedure linking table. The PLT allows us to resolve the absolute addresses of shared libraries at runtime.\nTake a look at the  section of the disassembly of section .plt, we see\n0000000000001060 : 1060:\tff 25 62 2f 00 00 jmpq *0x2f62(%rip) #3fc8 1066:\t68 03 00 00 00 pushq $0x3 106b:\te9 b0 ff ff ff jmpq 1020  Notice this jump is a pointer to an address. The address lies inside the GOT (Global Offset Table). The GOT will eventually hold the absolute address call to syscall. On the first call the address will point back to the instruction after the jump in the PLT - 0x1066. Then we see another jump instruction. This jump is a jump into the eventual runtime linker code that will load the shared library which has syscall.\nWe also see the comment for the first jump instruction\n#3fc8 Use objdump -R, we see the dynamic relocation entries in the file\nDYNAMIC RELOCATION RECORDS OFFSET TYPE VALUE 0000000000003d98 R_X86_64_RELATIVE *ABS*+0x0000000000001190 0000000000003da0 R_X86_64_RELATIVE *ABS*+0x0000000000001150 0000000000004008 R_X86_64_RELATIVE *ABS*+0x0000000000004008 0000000000003fd8 R_X86_64_GLOB_DAT _ITM_deregisterTMCloneTable 0000000000003fe0 R_X86_64_GLOB_DAT __libc_start_main@GLIBC_2.2.5 0000000000003fe8 R_X86_64_GLOB_DAT __gmon_start__ 0000000000003ff0 R_X86_64_GLOB_DAT _ITM_registerTMCloneTable 0000000000003ff8 R_X86_64_GLOB_DAT __cxa_finalize@GLIBC_2.2.5 0000000000003fb0 R_X86_64_JUMP_SLOT getpid@GLIBC_2.2.5 0000000000003fb8 R_X86_64_JUMP_SLOT __stack_chk_fail@GLIBC_2.4 0000000000003fc0 R_X86_64_JUMP_SLOT system@GLIBC_2.2.5 0000000000003fc8 R_X86_64_JUMP_SLOT syscall@GLIBC_2.2.5 0000000000003fd0 R_X86_64_JUMP_SLOT sprintf@GLIBC_2.2.5 : 1020:\tff 35 7a 2f 00 00 pushq 0x2f7a(%rip) # 3fa0  1026:\tff 25 7c 2f 00 00 jmpq *0x2f7c(%rip) # 3fa8  102c:\t0f 1f 40 00 nopl 0x0(%rax) ``` -- Monitor Events Next, we are going to look at L1 data cache metrics. We are interested in L1 data cache accesses, misses, and data TLB misses. We will measure this code in our experiment. CACHE_LINE_SIZE is defined as 64 to match our CPU specs.\n// p points to a region that is 1GB (ideally) void do_mem_access(char* p, int size) { int i, j, count, outer, locality; int ws_base = 0; int max_base = ((size / CACHE_LINE_SIZE) - 512); for(outer = 0; outer  (120); ++outer) { long r = simplerand() % max_base; // Pick a starting offset  if( opt_random_access ) { ws_base = r; } else { ws_base += 512; if( ws_base = max_base ) { ws_base = 0; } } for(locality = 0; locality  16; locality++) { volatile char *a; char c; for(i = 0; i  512; i++) { // Working set of 512 cache lines, 32KB  a = p + (ws_base + i) * CACHE_LINE_SIZE; if((i%8) == 0) { *a = 1; } else { c = *a; } } } } } What this routine does is essentially pick a working set of 512 cache lines, periodically perform a write or otherwise read operation. This process is repeated 16 times during each interaction. Each read or write access will operate on a new cache line. The innermost loop will perform this set of operations for the entire L1 data cache.\nWhen opt_random_access is true, the starting base address of the cache line is randomly picked. Otherwise, it is incremented by 512 cache lines (or one working set) during each outer iteration. The main difference is that with opt_random_access set to true, the starting base address of the cache line canâ€™t be precomputed by the hardware, thus likely increase miss rate.\nTo measure L1 data cache metrics, we will use the perf_event_open interface we discussed above. To measure L1 data cache read misses, we will configure our struct perf_event_attr as follows:\n#define CALC_CONFIG(perf_hw_cache_id, perf_hw_cache_op_id, perf_hw_cache_op_result_id) \\ ((perf_hw_cache_id) | (perf_hw_cache_op_id  hw_event.type = PERF_TYPE_HW_CACHE; hw_event.size = sizeof(struct perf_event_attr); hw_event.disabled = 1; // disable at init time hw_event.exclude_kernel = 1; hw_event.config = CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS); The exact details can be found in linux man page. The important part is:\nhw_event.config = CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS); These configurations allows us to measure the L1 data cahe read misses. The arguments passed to perf_event_open is\npid_t pid = 0; int cpu = -1; int group_fd = -1; unsigned long flags = 0; The choice of these parameters can also be found on the linux man page. After perf_event_open is called, we will re-enable event measurements by calling\nioctl(fd, PERF_EVENT_IOC_RESET, 0); ioctl(fd, PERF_EVENT_IOC_ENABLE, 0); What it does is resetting the event count specified by the file descriptor argument to zero, then enables the individual event specified by the file descriptor argument. After do_mem_access(p, size) is executed, we call ioctl(fd, PERF_EVENT_IOC_DISABLE, 0) to disable the event and then read the result by read(fd, \u0026result, sizeof(long long)). How result is defined is up to how PERF_FORMAT_* was specified. You can also check lxr to see how __perf_event_read_size calculates the size of event that is read. In our case, itâ€™s simple a u64.\n Be aware that simply executing the binary might cause perf_event_open to fail (in which case will always return -1). Using sudo is one workaround. Execute cat /proc/sys/kernel/perf_event_paranoid and see what returns. -1 means you have raw access to kernel tracepoints. Otherwise, you might have trouble accessing the performance counter without root privilege. Check this stackexchange post for more details.\n To be even more careful about generating repeatable results we should flush the level 1 data cache before enabling the performance counters. We will do this by reading a memory buffer larger than per-core L1 data cache size\nsize_t buffer_size = 32 * 1024 + 1024; char *buff = malloc(buffer_size); for (int i = 0; i  buffer_size; i++) { buff[i] = rand(); } We will also lock the process onto a single processor by using the sched_setaffinity function. Our example is\ncpu_set_t set; CPU_ZERO(\u0026set); CPU_SET(7, \u0026set); int aff = sched_setaffinity(0, sizeof(cpu_set_t), \u0026set); We perform the each of the above experiments 5 times. First, we turn on random cache line base address generation. On average, we have around 1010665367 L1 data cache read misses wtih standard deviation to be 61010967 misses. When random access is disabled, we have on average 964420324 read misses with standard deviation of 65787193 misses. We can also measure the number L1 data cache write misses by using the PERF_COUNT_HW_CACHE_OP_WRITE config instead. Use PERF_COUNT_HW_CACHE_OP_PREFETCH gives us prefetch misses, in our case, both of these metrics are unavailable. We can check the /arch/x86/events/intel/core.c in lxr and we can see these metrics are not available.\nWe can also use the PERF_COUNT_HW_CACHE_DTLB config option for data TLB measurement. For read access we have on average 3390719 misses with std dev being 17579, while write access has 1486451 misses with std dev being 13455. The prefetch metrics for TLB are unavailable in our case. To find out more about available metrics supported, please check the constant static __initconst const u64 skl_hw_cache_event_ids for specific kernel version.\nWith random cache line access turned off, we have 517335 read misses data TLB with standard deviation of 3820 misses. For write we have on average 809671 misses with standard deviation being 9580 misses. It is a significant reduction compared to the random access implementation.\nTo calculate the L1 cache miss rate and data TLB miss rate, we can use 100.0 * cache misses / cache_accesses and 100.0 * tlb misses / cache_accesses to calculate the results. With random access turned off, we get L1 read access miss rate to be $$miss_{cache} = 1.5%$$ and TLB read miss rate $$miss_{tlb} \\approx 0$$. When random access is turned on, we have $$miss_{cache} = 1.4%$$ and $$miss_{tlb} \\approx 0$$. We can see the miss rate in all scenarios is really low. This is mainly because the inner most loop in our routine is performing operations on working set already presented in L1 cache and TLB. The read/write operations use continous cache lines, which means there will almost be no faults while we access the 512 cache lines. If one fault causes the entire new working set to be cached, then there would be no subsequent faults until the entire working set is iterated.\nIf we use getrusage we can see the metrics listed below:\n   Metrics Mean std dev     utime 868629 126044   stime 253586 20112   maxrss 1049691 43   minflt 262214 1   majflt 0 0   inblock 0 0   oublock 0 0   nvcsw 0.4 0.54   nivcsw 47 7    \nmmap Next we are going to explore the behavior of mmap. Previously, we used malloc for data allocation. Next, we are going to instead use mmap and see what happens. Here we will only use read access for benchmark metrics since itâ€™s available in both L1 and TLB metrics.\nFirst, we use the MAP_ANONYMOUS as a flag passed to mmap. This flag means the mapping is not backed by any file; its contents are initialized to zero. The complete call is\nmmap(NULL, length, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, fd_ignore, offset); For more details, refer to mmap man page for information.\nWhen we turn on the random access and use perf_event_open interface to collect metrics, we see the L1 data cache read misses are 956148031 (std dev 84631843). The TLB data cache read misses are 3370309 (std dev 17792). We see it is not really different to the malloc approach we used before. Doing a simple strace shows malloc calls mmap. The memory that backs malloc() allocations is handled by the kernel in much the same way as the memory that backs private anonymous mappings created with mmap().\nThen, we try to use mmap() to create mapping in the virtual address space backed by a file instead of using MAP_ANONYMOUS.\nWe first test mmap with MAP_PRIVATE. According to the man page, this flags means creating a private copy-on-write mapping. Updates to the mapping are not visible to other processes mapping the same file, and are not carried through to the underlying file. It is unspecified whether changes made to the file after the mmap() call are visible in the mapped region.\n Note we should call fallocate() for the newly created file, otherwise mmap is gonna throw bur error.\n When we measure the L1 data cache miss, itâ€™s around 946128512 (std dev 956148031), nothing special happens. When we use MAP_SHARED flag, the result was similar. The result seems to fluctuates as time passes, but overall they are not much different. After all, itâ€™s just reading from the memory, whether the address is backed by a file or not doesnâ€™t play a big role in affecting the cache miss rate. The L1 data cache misses is shown below:\n   Flag PRIVATE PRIVATE+POPULATE SHARED SHARED+POPULATE     Mean 783864673 769314361 842915231 816749524   Std dev 77816766 53913082 54613278 60580595    \nIf we take a look at TLB data cache, the result is\n   Flag PRIVATE PRIVATE+POPULATE SHARED SHARED+POPULATE     Mean 3372303 3370740 3381755 3377370   Std dev 9884 13567 17626 11776    \nStill, there doesnâ€™t seem have any significant fluctuation in the number of misses in data TLB. This pattern also applies to sequential access, except the TLB data cache misses is alot lower in sequentual access.\nNow If we instead use getrusage(), we will get something like this\n   Flag PRIVATE PRIVATE+POPULATE SHARED SHARED+POPULATE     Usec/std dev 20/0 20/0 20/0 20/0   usec/std dev 801512/ 78346 793452/ 143556 872342/ 124124 671957/ 229314   Ssec/std dev 0/0 0/0 0/0 0/0   ssec/std dev 475977/ 54355 475678/ 134253 445467/ 99345 536041/ 98797   oublock/std dev 0/0 0/0 2997152/ 82256 2097152/ 19760    \nThe most interesting part here is when MAP_SHARED is enabled, the oublock immediately changes. As we mentioned previously, oublock specifies the number of times the file system had to write to the disk on behalf of processes. Because the address is now backed by a file, all write operations will cause the file system to write the contents back to the file.\nmmap() creates a new mapping in the virtual address space of the calling process. However, it doesnâ€™t allocate RAM. If we call memset() then followed by msync() with MS_SYNC flag, we can get some interesting results in getrusage, these observations are summarized here:\n kernel space time is much higher. It usually take 1 sec (no std dev) as opposed to 0. Synchronizing to files on disk will require more kernel participation. minflt (the number of page faults which were serviced without requiring any I/O) was muich higher, the value is around 540782(std dev 3). More memory mapped means the faults by I/O will be less likely. oublock is much higher, the value is around 4196512(std dev 1). The sync operating means there will be approximatly double amount of writes to disk. nvcsw was higher, there are more voluntary context switches. Writing results to disk has delay, and thus the process likely need to context switch while waiting for I/O to be finished.  We may notice the number data TLB misses is lower than the total number of page the application uses. One obvious answer the use of huge page. One huge page can cover many small pages. Also, because we have prefetching TLB and the working set access pattern is contiguous, TLB hit rate will be high. Because we have a set-associative TLB cache, and we access the memory in a fairly deterministic way, itâ€™s easy to predict where the next access is pointing to. For example, if the replacement policy is FIFO, then each cache line will remain untouched for exact same clock cycle before replaced. This also applies to other policies. One way to determine the replacement algorithm is using P-Chase.\nstrace We then use strace to trace syscalls of our application. The output contains some interesting information, one is\naccess(\"/etc/ld.so.preload\", R_OK) = -1 ENOENT (No such file or directory) ... arch_prctl(ARCH_SET_FS, 0x7fdc6ad83540) = 0 According to arch_prctl man page, arch_prctl() sets architecture-specific process or thread state. The ARCH_SET_FS option sets the 64-bit base for the FS register to addr, in our case itâ€™s 0x7fdc6ad83540. Letâ€™s set a break point at arch_prctl and backtrace from there\n#0 0x00007ffff7febb55 in ?? () from /lib64/ld-linux-x86-64.so.2 #1 0x00007ffff7fd104c in ?? () from /lib64/ld-linux-x86-64.so.2 #2 0x00007ffff7fd0108 in ?? () from /lib64/ld-linux-x86-64.so.2 #3 0x0000000000000001 in ?? () #4 0x00007fffffffe2fa in ?? () #5 0x0000000000000000 in ?? () We can see the FS segment base is set by the ld-linux, which is a part of glibc, during the program loading. A simple google seach tells us /lib64/ld-linux-x86-64.so.2 is a dynamic linker. A more detailed description can be found on this post and lwn.net. During the startup, the loader initalizes TLS. The includes memory allocation and setting FS base value to point to the TLS beignning, which is done via the arch_prctl syscall. More can be found here. This init_tls() is called here, which subsequently calls the actuall syscall in tls.h.\nThe /etc/ld.so.preload has similarities to LD_PRELOAD, in addition, it doesnâ€™t suffer security limitation posed by LD_PRELOAD (explanation here). This a feature of glibc.\nCompeting for Memory Next we are going to fork another process that will compete for memory with our process under test. We will use this code snippet which is going to be executed by both the parent and the child process\nint compete_for_memory(void* unused) { long mem_size = get_mem_size(); int page_sz = sysconf(_SC_PAGE_SIZE); printf(\"Total memsize is %3.2f GBs\\n\", (double)mem_size/(1024*1024*1024)); fflush(stdout); char* p = mmap(NULL, mem_size, PROT_READ | PROT_WRITE, MAP_NORESERVE|MAP_PRIVATE|MAP_ANONYMOUS, -1, (off_t) 0); if (p == MAP_FAILED) perror(\"Failed anon MMAP competition\"); int i = 0; while(1) { volatile char *a; long r = simplerand() % (mem_size/page_sz); char c; if( i = mem_size/page_sz ) { i = 0; } // One read and write per page  //a = p + i * page_sz; // sequential access  a = p + r * page_sz; c += *a; if((i%8) == 0) { *a = 1; } i++; } return 0; } The get_mem_size() is implemented using this portable code\n#if defined(_WIN32) #include  #elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) \u0026\u0026 defined(__MACH__)) #include #include #include #if defined(BSD) #include #endif  #else #error \"Unable to define getMemorySize( ) for an unknown OS.\" #endif  /** * Returns the size of physical memory (RAM) in bytes. */ size_t getMemorySize( ) { #if defined(_WIN32) \u0026\u0026 (defined(__CYGWIN__) || defined(__CYGWIN32__))  /* Cygwin under Windows. ------------------------------------ */ /* New 64-bit MEMORYSTATUSEX isn't available. Use old 32.bit */ MEMORYSTATUS status; status.dwLength = sizeof(status); GlobalMemoryStatus( \u0026status ); return (size_t)status.dwTotalPhys; #elif defined(_WIN32)  /* Windows. ------------------------------------------------- */ /* Use new 64-bit MEMORYSTATUSEX, not old 32-bit MEMORYSTATUS */ MEMORYSTATUSEX status; status.dwLength = sizeof(status); GlobalMemoryStatusEx( \u0026status ); return (size_t)status.ullTotalPhys; #elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) \u0026\u0026 defined(__MACH__))  /* UNIX variants. ------------------------------------------- */ /* Prefer sysctl() over sysconf() except sysctl() HW_REALMEM and HW_PHYSMEM */ #if defined(CTL_HW) \u0026\u0026 (defined(HW_MEMSIZE) || defined(HW_PHYSMEM64))  int mib[2]; mib[0] = CTL_HW; #if defined(HW_MEMSIZE)  mib[1] = HW_MEMSIZE; /* OSX. --------------------- */ #elif defined(HW_PHYSMEM64)  mib[1] = HW_PHYSMEM64; /* NetBSD, OpenBSD. --------- */ #endif  int64_t size = 0; /* 64-bit */ size_t len = sizeof( size ); if ( sysctl( mib, 2, \u0026size, \u0026len, NULL, 0 ) == 0 ) return (size_t)size; return 0L; /* Failed? */ #elif defined(_SC_AIX_REALMEM)  /* AIX. ----------------------------------------------------- */ return (size_t)sysconf( _SC_AIX_REALMEM ) * (size_t)1024L; #elif defined(_SC_PHYS_PAGES) \u0026\u0026 defined(_SC_PAGESIZE)  /* FreeBSD, Linux, OpenBSD, and Solaris. -------------------- */ return (size_t)sysconf( _SC_PHYS_PAGES ) * (size_t)sysconf( _SC_PAGESIZE ); #elif defined(_SC_PHYS_PAGES) \u0026\u0026 defined(_SC_PAGE_SIZE)  /* Legacy. -------------------------------------------------- */ return (size_t)sysconf( _SC_PHYS_PAGES ) * (size_t)sysconf( _SC_PAGE_SIZE ); #elif defined(CTL_HW) \u0026\u0026 (defined(HW_PHYSMEM) || defined(HW_REALMEM))  /* DragonFly BSD, FreeBSD, NetBSD, OpenBSD, and OSX. -------- */ int mib[2]; mib[0] = CTL_HW; #if defined(HW_REALMEM)  mib[1] = HW_REALMEM; /* FreeBSD. ----------------- */ #elif defined(HW_PYSMEM)  mib[1] = HW_PHYSMEM; /* Others. ------------------ */ #endif  unsigned int size = 0; /* 32-bit */ size_t len = sizeof( size ); if ( sysctl( mib, 2, \u0026size, \u0026len, NULL, 0 ) == 0 ) return (size_t)size; return 0L; /* Failed? */ #endif /* sysctl and sysconf variants */ #else  return 0L; /* Unknown OS. */ #endif } The important line is\nreturn (size_t)sysconf( _SC_PHYS_PAGES ) * (size_t)sysconf( _SC_PAGESIZE ); One thing to notice in the routine for competing for memory is we used fflush after the printf. The purpose of fflush(stream) is to make the operating system flush any buffers to the underlying file. This mainly because stdout is buffered. The buffer is not flushed until newline. fflush will cause this process to happen with the absense of newline. stderr is unbuffered and thus fflush would not be necessary.\nFor this experiment, we tested it on a VM. The reason is because the contending process will take all RAM and completely hault the mahcine if tested on the host. To ensure our VM has enough swap space, we follow this tutorial to create 4GB of swap area (we allocated 2GB RAM for VM).\nOne thing we observe is that the execution time of the program become significantly longer to run. In our experiement we need to limit the number of iterations from 1 Â« 20 to 1 Â« 8 to get some sensible results without running for days.\nWhen we use PRIVATE and ANONYMUS option and random access turned on, the misses in data TLB is 335009(std dev 7298). We canâ€™t get access to L1 cache data because it will cause the session to be automatically logged out whenever L1D is used. here are some interesting things to notice:\n MAP_PRIVATE + MAP_ANONYMOUS: TLB misses:335009(std dev 17298)\nminflt: 4220(std dev 231)\noublock: 8(std dev 4)\nnivcsw: 19(10) MAP_SHARED: TLB misses:251284std dev 103292)\nminflt: 2784(std dev 231)\nmajflt: 247(std 65)\noublock: 18200(std dev 2987)\nnivcsw: 8(7)  The most important difference here is that the oublock is much easier to trigger because the constant swapping. When file backed memory is used we also notice that majflt is much higher. Because pages are constantly traveling between swap area and memory, the page fault rate becomes a lot higher. The oublock also follows previous patterns as the file backed memory requires filesystem involvement.\nFinally, we also modify the kernelâ€™s head (or more precisely its LRU page replacement algorithm). Look in mm/vmscan.c thereâ€™s a function calleed shrink_page_list. In it, you will see a switch statement with a PAGEREF_ACTIVATE case, which is the case where the kernel sees the page has been recently accessed. In this case the kernel gotos activate_locked, but you will change it to to do the same thing as the PAGEREF_RECLAIM case. We can simply move the case down and change its default behavior to direct to the PAGEREF_RECLAIM case. After that, we need to recompile the kernel for VM. We also summarize the most interesting results:\n MAP_PRIVATE + MAP_ANONYMOUS: TLB misses:308031(std dev 17298)\nminflt: 4223(std dev 791)\noublock: 8(std dev 1)\nnivcsw: 11(5) MAP_SHARED: TLB misses: 251284std dev 103292)\nminflt: 2724(std dev 231)\nmajflt: 0(std 0)\noublock: 18200(std dev 2987)\nnivcsw: 8(7)  We can see that the most of the pattern follow the previous result after the modified kernel is installed. One main difference is majflt value is reduced back down.\n","wordCount":"4734","inLanguage":"en","datePublished":"2020-09-23T00:00:00Z","dateModified":"2020-09-23T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.bodunhu.com/blog/posts/linuxkernelmeasurementandmmap/"},"publisher":{"@type":"Organization","name":"std::bodun::blog","logo":{"@type":"ImageObject","url":"https://www.cs.utexas.edu/sites/default/files/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://www.bodunhu.com/blog/ accesskey=h title="std::bodun::blog (Alt + H)">std::bodun::blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.bodunhu.com/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://www.bodunhu.com/blog/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://www.bodunhu.com/blog/about/ title=About><span>About</span></a></li><li><a href=https://www.bodunhu.com/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.bodunhu.com/blog/>Home</a>&nbsp;Â»&nbsp;<a href=https://www.bodunhu.com/blog/posts/>Posts</a></div><h1 class=post-title>Linux Program Measurement and mmap</h1><div class=post-meta>September 23, 2020&nbsp;Â·&nbsp;23 min</div></header><div class=post-content><p>This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to <a href=http://www.cpu-world.com/CPUs/Core_i7/Intel-Core%20i7%20i7-6800K.html>cpu world</a>. This is the system spec:</p><table><thead><tr><th>Attribute</th><th style=text-align:center>Value</th></tr></thead><tbody><tr><td>Processor name (BIOS)</td><td style=text-align:center>Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz</td></tr><tr><td>Cores</td><td style=text-align:center>6</td></tr><tr><td>Logical processors</td><td style=text-align:center>12</td></tr><tr><td>TLB/Cache details</td><td style=text-align:center>64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries. Plus, 1-GB pages, 4-way, 16 entries</td></tr><tr><td>RAM</td><td style=text-align:center>32GB</td></tr><tr><td>Operating System</td><td style=text-align:center>Ubuntu 20.04.1 LTS</td></tr><tr><td>Kernel Version</td><td style=text-align:center>5.4.0-47-generic</td></tr></tbody></table><p><br></p><blockquote><p>8-way set associative means the CPU cache is made up of sets that can fit 8 blocks each.</p></blockquote><p>Here are the details for the CPU cache, which we will need later:</p><table><thead><tr><th>Cache</th><th>L1 data</th><th>L1 instruction</th><th>L2</th><th>L3</th></tr></thead><tbody><tr><td>Size</td><td>6 x 32 KB</td><td>6 x 32 KB</td><td>6 x 256 KB</td><td>15 MB</td></tr><tr><td>Associativity</td><td>8-way set associative</td><td>8-way set associative</td><td>8-way set associative</td><td>20-way set associative</td></tr><tr><td>Line size:</td><td>64 bytes</td><td>64 bytes</td><td>64 bytes</td><td>64 bytes</td></tr><tr><td>Comments:</td><td>Direct-mapped</td><td>Direct-mapped</td><td>Non-inclusive Direct-mapped</td><td>Inclusive Shared between all cores</td></tr></tbody></table><p><br></p><h2 id=memory-map>Memory Map<a hidden class=anchor aria-hidden=true href=#memory-map>#</a></h2><p>To print the <code>/proc/self/maps</code> file for a process, we use the <code>sprintf</code> to construct the file name and then use the <code>system</code> from stdlib to cat the contents of the running process&rsquo;s address space. If we execute the program, it shows (also available on <a href=https://gist.github.com/BDHU/9ad2f0b6353b789cfb7c29c804a6088a#file-proc_mem_map>gist</a>)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>address                   perms offset  dev   inode                      pathname
559e3e51f000-559e3e520000 r--p 00000000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e520000-559e3e521000 r-xp 00001000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e521000-559e3e522000 r--p 00002000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e522000-559e3e523000 r--p 00002000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e523000-559e3e524000 rw-p 00003000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
7faf5c477000-7faf5c49c000 r--p 00000000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c49c000-7faf5c614000 r-xp 00025000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c614000-7faf5c65e000 r--p 0019d000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c65e000-7faf5c65f000 ---p 001e7000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c65f000-7faf5c662000 r--p 001e7000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c662000-7faf5c665000 rw-p 001ea000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c665000-7faf5c66b000 rw-p 00000000 00:00 0 
7faf5c685000-7faf5c686000 r--p 00000000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c686000-7faf5c6a9000 r-xp 00001000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6a9000-7faf5c6b1000 r--p 00024000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6b2000-7faf5c6b3000 r--p 0002c000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6b3000-7faf5c6b4000 rw-p 0002d000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6b4000-7faf5c6b5000 rw-p 00000000 00:00 0 
7ffcddb8d000-7ffcddbae000 rw-p 00000000 00:00 0                          [stack]
7ffcddbe0000-7ffcddbe3000 r--p 00000000 00:00 0                          [vvar]
7ffcddbe3000-7ffcddbe4000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
</code></pre></div><p>Based on the <a href=https://man7.org/linux/man-pages/man5/proc.5.html>linux man page</a>, we can see each column has different definition. The <em>address</em> field is the address space in the process that the mapping occupies. The <em>perms</em> field is a set of permissions:</p><ul><li>r = read</li><li>w = write</li><li>x = execute</li><li>s = shared</li><li>p = private (copy on write)</li></ul><p>The <em>offset</em> field is the offset into the file/whatever; <em>dev</em> is the device (major:minor); <em>inode</em> is the inode on that device. 0 indicates that no inode is associated with the memory region, as would be the case with BSS (uninitialized data).</p><p>The <em>pathname</em> field will usually be the file that is backing the mapping. For ELF files, you can easily coordinate with the <em>offset</em> field by looking at the Offset field in the ELF program headers (readelf -l). In addition, we can see a few other pseudo-paths:</p><ul><li><p><em>[stack]</em>: the initial process&rsquo;s (also known as the main thread&rsquo;s) stack.</p></li><li><p><em>[vdso]</em>: The virtual dynamically linked shared object. More detailed descriptions can be found on <a href=https://lwn.net/Articles/615809/>lwn</a>.</p></li><li><p><em>[vvar]</em>: location of kernel space variables mapped in user space needed by virtual system calls. Essentially, a kernel-space physical address is mapped into the userspace.</p></li><li><p><em>[vsyscall]</em>: similar to vDSO, vsyscall is another segment used to accelerate certain system calls in Linux. Vsyscall has some limitations; among other things, there is only space for a handful of virtual system calls. More detailed descriptions can be found on <a href=https://lwn.net/Articles/446528/>lwn</a>.</p></li></ul><p>One thing interesting here is that when we execute the same program twice, we can see after the first run, the output is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>7fffbc92f000-7fffbc930000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
</code></pre></div><p>Type the same command again:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>7ffd6a94d000-7ffd6a94e000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
</code></pre></div><p>Note that the vDSO area has moved, while the vsyscall page remains at the same location. The location of the vsyscall page is nailed down in the kernel ABI, but the vDSO area - like most other areas in the user-space memory layout - has its location randomized every time it is mapped. The vsyscall is legacy implementation of user-space sys call acceleration. Since it has fixed addresses, it is vulnerable to security issues. Because applications depend on the existence and exact address of that page, most functions are simply removed and replaced by a special trap instruction. More detailed explanation can be found on <a href=https://lwn.net/Articles/446528/>lwn.net</a>.</p><p>Another interesting thing we observed is the base address of the executable (the start of the text section) and the start address of libc is rather different. This is also the result of using ASLR which is used to prevent return-to-libc attack.</p><h2 id=getrusage>getrusage<a hidden class=anchor aria-hidden=true href=#getrusage>#</a></h2><p>Then, we call <code>getrusage</code> at the end of our program and print out the fields. We will need <code>getrusage</code> later. Here is a sample output for some fields inside <code>struct rusage</code>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>utime: 1306
stime: 0
maxrss: 2692
minflt: 76
majflt: 0
inblock: 0
oublock: 0
nvcsw: 2
nivcsw: 0
</code></pre></div><p>Here is a short list of descriptions for each of these fields. More detailed information can be found on <a href=https://www.gnu.org/software/libc/manual/html_node/Resource-Usage.html>gnu website</a></p><ul><li><strong>utime</strong>: time spent executing user instructions.</li><li><strong>stime</strong>: time spent in operating system code on behalf of processes.</li><li><strong>maxrss</strong>: the maximum resident set size used, in kilobytes. That is, the maximum number of kilobytes of physical memory that processes used simultaneously.</li><li><strong>minflt</strong>: the number of page faults which were serviced without requiring any I/O.</li><li><strong>majflt</strong>: the number of page faults which were serviced by doing I/O.</li><li><strong>inblock</strong>: the number of times the file system had to read from the disk on behalf of processes.</li><li><strong>oublock</strong>: the number of times the file system had to write to the disk on behalf of processes.</li><li><strong>nvcsw</strong>: the number of times processes voluntarily invoked a context switch (usually to wait for some service).</li><li><strong>nivcsw</strong>: the number of times an involuntary context switch took place (because a time slice expired, or another process of higher priority was scheduled).</li></ul><h2 id=perf_event_open>perf_event_open<a hidden class=anchor aria-hidden=true href=#perf_event_open>#</a></h2><p><code>perf_event_open</code> interface is useful to measurement numerous system events. However, glibc doesn&rsquo;t provide wrapper for this system call. Instead, we need to use <code>syscall</code> directly.</p><p>To use <code>perf_event_open</code>, we call create a function wrapper that does the actual syscall for us. Take the example from the <a href=https://man7.org/linux/man-pages/man2/perf_event_open.2.html>Linux man page</a></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span>
<span style=color:#a6e22e>perf_event_open</span>(<span style=color:#66d9ef>struct</span> perf_event_attr <span style=color:#f92672>*</span>hw_event, pid_t pid,
                <span style=color:#66d9ef>int</span> cpu, <span style=color:#66d9ef>int</span> group_fd, <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>long</span> flags)
{
    <span style=color:#66d9ef>int</span> ret;

    ret <span style=color:#f92672>=</span> syscall(__NR_perf_event_open, hw_event, pid, cpu,
                    group_fd, flags);
    <span style=color:#66d9ef>return</span> ret;
}
</code></pre></div><p>Here the <code>__NR_perf_event_open</code> specifies the syscall number. On our local machine, we can go to <code>/usr/include/x86_64-linux-gnu/sys/syscall.h</code>, which specifies the location of <code>__NR_perf_event_open</code>. In our case, it is located at <code>/usr/include/x86_64-linux-gnu/asm/unistd_64.h</code>.</p><p>If we call <code>objdump -d</code> on the binary file, we will see something like this</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>000000000000119a &lt;perf_event_open&gt;:
    119a:	55                   	push   %rbp
    119b:	48 89 e5             	mov    %rsp,%rbp
    119e:	48 83 ec 30          	sub    $0x30,%rsp
    11a2:	48 89 7d e8          	mov    %rdi,-0x18(%rbp)
    11a6:	89 75 e4             	mov    %esi,-0x1c(%rbp)
    11a9:	89 55 e0             	mov    %edx,-0x20(%rbp)
    11ac:	89 4d dc             	mov    %ecx,-0x24(%rbp)
    11af:	4c 89 45 d0          	mov    %r8,-0x30(%rbp)
    11b3:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    11b7:	8b 75 dc             	mov    -0x24(%rbp),%esi
    11ba:	8b 4d e0             	mov    -0x20(%rbp),%ecx
    11bd:	8b 55 e4             	mov    -0x1c(%rbp),%edx
    11c0:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
    11c4:	49 89 f9             	mov    %rdi,%r9
    11c7:	41 89 f0             	mov    %esi,%r8d
    11ca:	48 89 c6             	mov    %rax,%rsi
    11cd:	bf 2a 01 00 00       	mov    $0x12a,%edi
    11d2:	b8 00 00 00 00       	mov    $0x0,%eax
    11d7:	e8 84 fe ff ff       	callq  1060 &lt;syscall@plt&gt;
    11dc:	89 45 fc             	mov    %eax,-0x4(%rbp)
    11df:	8b 45 fc             	mov    -0x4(%rbp),%eax
    11e2:	48 98                	cltq   
    11e4:	c9                   	leaveq 
    11e5:	c3                   	retq   
</code></pre></div><p>We notice there&rsquo;s one interesting line</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>callq  1060 &lt;syscall@plt&gt;
</code></pre></div><p>The <code>plt</code> stands for Procedure Linkage Table. This lines indicates a call to the <code>syscall</code> in the procedure linking table. The PLT allows us to resolve the absolute addresses of shared libraries at runtime.</p><p>Take a look at the <code>&lt;syscall@plt></code> section of the disassembly of section .plt, we see</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>0000000000001060 &lt;syscall@plt&gt;:
    1060:	ff 25 62 2f 00 00    	jmpq   *0x2f62(%rip)    #3fc8&lt;syscall@GLIBC_2.2.5&gt;
    1066:	68 03 00 00 00       	pushq  $0x3
    106b:	e9 b0 ff ff ff       	jmpq   1020 &lt;.plt&gt;
</code></pre></div><p>Notice this jump is a pointer to an address. The address lies inside the GOT (Global Offset Table). The GOT will eventually hold the absolute address call to <code>syscall</code>. On the first call the address will point back to the instruction after the jump in the PLT - <code>0x1066</code>. Then we see another jump instruction. This jump is a jump into the eventual runtime linker code that will load the shared library which has syscall.</p><p>We also see the comment for the first jump instruction</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>#3fc8&lt;syscall@GLIBC_2.2.5&gt;
</code></pre></div><p>Use <code>objdump -R</code>, we see the dynamic relocation entries in the file</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>DYNAMIC RELOCATION RECORDS
OFFSET           TYPE              VALUE 
0000000000003d98 R_X86_64_RELATIVE  *ABS*+0x0000000000001190
0000000000003da0 R_X86_64_RELATIVE  *ABS*+0x0000000000001150
0000000000004008 R_X86_64_RELATIVE  *ABS*+0x0000000000004008
0000000000003fd8 R_X86_64_GLOB_DAT  _ITM_deregisterTMCloneTable
0000000000003fe0 R_X86_64_GLOB_DAT  __libc_start_main@GLIBC_2.2.5
0000000000003fe8 R_X86_64_GLOB_DAT  __gmon_start__
0000000000003ff0 R_X86_64_GLOB_DAT  _ITM_registerTMCloneTable
0000000000003ff8 R_X86_64_GLOB_DAT  __cxa_finalize@GLIBC_2.2.5
0000000000003fb0 R_X86_64_JUMP_SLOT  getpid@GLIBC_2.2.5
0000000000003fb8 R_X86_64_JUMP_SLOT  __stack_chk_fail@GLIBC_2.4
0000000000003fc0 R_X86_64_JUMP_SLOT  system@GLIBC_2.2.5
0000000000003fc8 R_X86_64_JUMP_SLOT  syscall@GLIBC_2.2.5
0000000000003fd0 R_X86_64_JUMP_SLOT  sprintf@GLIBC_2.2.5
</code></pre></div><h2 id=monitor-events>Monitor Events<a hidden class=anchor aria-hidden=true href=#monitor-events>#</a></h2><p>Next, we are going to look at L1 data cache metrics. We are interested in L1 data cache accesses, misses, and data TLB misses. We will measure this code in our experiment. CACHE_LINE_SIZE is defined as 64 to match our CPU specs.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>// p points to a region that is 1GB (ideally)
</span><span style=color:#75715e></span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>do_mem_access</span>(<span style=color:#66d9ef>char</span><span style=color:#f92672>*</span> p, <span style=color:#66d9ef>int</span> size) {
    <span style=color:#66d9ef>int</span> i, j, count, outer, locality;
    <span style=color:#66d9ef>int</span> ws_base <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
    <span style=color:#66d9ef>int</span> max_base <span style=color:#f92672>=</span> ((size <span style=color:#f92672>/</span> CACHE_LINE_SIZE) <span style=color:#f92672>-</span> <span style=color:#ae81ff>512</span>);
    <span style=color:#66d9ef>for</span>(outer <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; outer <span style=color:#f92672>&lt;</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>&lt;&lt;</span><span style=color:#ae81ff>20</span>); <span style=color:#f92672>++</span>outer) {
      <span style=color:#66d9ef>long</span> r <span style=color:#f92672>=</span> simplerand() <span style=color:#f92672>%</span> max_base;
      <span style=color:#75715e>// Pick a starting offset
</span><span style=color:#75715e></span>      <span style=color:#66d9ef>if</span>( opt_random_access ) {
         ws_base <span style=color:#f92672>=</span> r;
      } <span style=color:#66d9ef>else</span> {
         ws_base <span style=color:#f92672>+=</span> <span style=color:#ae81ff>512</span>;
         <span style=color:#66d9ef>if</span>( ws_base <span style=color:#f92672>&gt;=</span> max_base ) {
            ws_base <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
         }
      }
      <span style=color:#66d9ef>for</span>(locality <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; locality <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>16</span>; locality<span style=color:#f92672>++</span>) {
         <span style=color:#66d9ef>volatile</span> <span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>a;
         <span style=color:#66d9ef>char</span> c;
         <span style=color:#66d9ef>for</span>(i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>512</span>; i<span style=color:#f92672>++</span>) {
            <span style=color:#75715e>// Working set of 512 cache lines, 32KB
</span><span style=color:#75715e></span>            a <span style=color:#f92672>=</span> p <span style=color:#f92672>+</span> (ws_base <span style=color:#f92672>+</span> i) <span style=color:#f92672>*</span> CACHE_LINE_SIZE;
            <span style=color:#66d9ef>if</span>((i<span style=color:#f92672>%</span><span style=color:#ae81ff>8</span>) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
               <span style=color:#f92672>*</span>a <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
            } <span style=color:#66d9ef>else</span> {
               c <span style=color:#f92672>=</span> <span style=color:#f92672>*</span>a;
            }
         }
      }
   }
}
</code></pre></div><p>What this routine does is essentially pick a working set of 512 cache lines, periodically perform a write or otherwise read operation. This process is repeated 16 times during each interaction. Each read or write access will operate on a new cache line. The innermost loop will perform this set of operations for the entire L1 data cache.</p><p>When opt_random_access is true, the starting base address of the cache line is randomly picked. Otherwise, it is incremented by 512 cache lines (or one working set) during each outer iteration. The main difference is that with opt_random_access set to true, the starting base address of the cache line can&rsquo;t be precomputed by the hardware, thus likely increase miss rate.</p><p>To measure L1 data cache metrics, we will use the <code>perf_event_open</code> interface we discussed above. To measure L1 data cache read misses, we will configure our <code>struct perf_event_attr</code> as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>#define CALC_CONFIG(perf_hw_cache_id, perf_hw_cache_op_id, perf_hw_cache_op_result_id) \
</span><span style=color:#75715e>((perf_hw_cache_id) | (perf_hw_cache_op_id &lt;&lt; 8) | (perf_hw_cache_op_result_id &lt;&lt; 16))
</span><span style=color:#75715e></span>
hw_event.type <span style=color:#f92672>=</span> PERF_TYPE_HW_CACHE; 
hw_event.size <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>struct</span> perf_event_attr);
hw_event.disabled <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>; <span style=color:#75715e>// disable at init time
</span><span style=color:#75715e></span>hw_event.exclude_kernel <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
hw_event.config <span style=color:#f92672>=</span> CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS);
</code></pre></div><p>The exact details can be found in <a href=https://man7.org/linux/man-pages/man2/perf_event_open.2.html>linux man page</a>. The important part is:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>hw_event.config <span style=color:#f92672>=</span> CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS);
</code></pre></div><p>These configurations allows us to measure the L1 data cahe read misses. The arguments passed to <code>perf_event_open</code> is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>pid_t pid <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
<span style=color:#66d9ef>int</span> cpu <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>;
<span style=color:#66d9ef>int</span> group_fd <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>;
<span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>long</span> flags <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</code></pre></div><p>The choice of these parameters can also be found on the <a href=https://man7.org/linux/man-pages/man2/perf_event_open.2.html>linux man page</a>. After <code>perf_event_open</code> is called, we will re-enable event measurements by calling</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>ioctl(fd, PERF_EVENT_IOC_RESET, <span style=color:#ae81ff>0</span>);
ioctl(fd, PERF_EVENT_IOC_ENABLE, <span style=color:#ae81ff>0</span>);
</code></pre></div><p>What it does is resetting the event count specified by the file descriptor argument to zero, then enables the individual event specified by the file descriptor argument. After <code>do_mem_access(p, size)</code> is executed, we call <code>ioctl(fd, PERF_EVENT_IOC_DISABLE, 0)</code> to disable the event and then read the result by <code>read(fd, &result, sizeof(long long))</code>. How result is defined is up to how <code>PERF_FORMAT_*</code> was specified. You can also check <a href=https://elixir.bootlin.com/linux/latest/source/kernel/events/core.c#L1833>lxr</a> to see how <code>__perf_event_read_size</code> calculates the size of event that is read. In our case, it&rsquo;s simple a <code>u64</code>.</p><blockquote><p>Be aware that simply executing the binary might cause <code>perf_event_open</code> to fail (in which case will always return -1). Using <code>sudo</code> is one workaround. Execute <code>cat /proc/sys/kernel/perf_event_paranoid</code> and see what returns. <code>-1</code> means you have raw access to kernel tracepoints. Otherwise, you might have trouble accessing the performance counter without root privilege. Check this <a href=https://unix.stackexchange.com/questions/14227/do-i-need-root-admin-permissions-to-run-userspace-perf-tool-perf-events-ar>stackexchange post</a> for more details.</p></blockquote><p>To be even more careful about generating repeatable results we should flush the level 1 data cache before enabling the performance counters. We will do this by reading a memory buffer larger than per-core L1 data cache size</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>size_t buffer_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>1024</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1024</span>;
<span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>buff <span style=color:#f92672>=</span> malloc(buffer_size);
<span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> buffer_size; i<span style=color:#f92672>++</span>) {
    buff[i] <span style=color:#f92672>=</span> rand();
}
</code></pre></div><p>We will also lock the process onto a single processor by using the <code>sched_setaffinity</code> function. Our example is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>cpu_set_t set;
CPU_ZERO(<span style=color:#f92672>&amp;</span>set);
CPU_SET(<span style=color:#ae81ff>7</span>, <span style=color:#f92672>&amp;</span>set);
<span style=color:#66d9ef>int</span> aff <span style=color:#f92672>=</span> sched_setaffinity(<span style=color:#ae81ff>0</span>, <span style=color:#66d9ef>sizeof</span>(cpu_set_t), <span style=color:#f92672>&amp;</span>set);
</code></pre></div><p>We perform the each of the above experiments 5 times. First, we turn on random cache line base address generation. On average, we have around 1010665367 L1 data cache read misses wtih standard deviation to be 61010967 misses. When random access is disabled, we have on average 964420324 read misses with standard deviation of 65787193 misses. We can also measure the number L1 data cache write misses by using the <code>PERF_COUNT_HW_CACHE_OP_WRITE</code> config instead. Use <code>PERF_COUNT_HW_CACHE_OP_PREFETCH</code> gives us prefetch misses, in our case, both of these metrics are unavailable. We can check the <code>/arch/x86/events/intel/core.c</code> in <a href=https://elixir.bootlin.com/linux/v5.6/source/arch/x86/events/intel/core.c>lxr</a> and we can see these metrics are not available.</p><p>We can also use the <code>PERF_COUNT_HW_CACHE_DTLB</code> config option for data TLB measurement. For read access we have on average 3390719 misses with std dev being 17579, while write access has 1486451 misses with std dev being 13455. The prefetch metrics for TLB are unavailable in our case. To find out more about available metrics supported, please check the constant <code>static __initconst const u64 skl_hw_cache_event_ids</code> for specific kernel version.</p><p>With random cache line access turned off, we have 517335 read misses data TLB with standard deviation of 3820 misses. For write we have on average 809671 misses with standard deviation being 9580 misses. It is a significant reduction compared to the random access implementation.</p><p>To calculate the L1 cache miss rate and data TLB miss rate, we can use 100.0 * cache misses / cache_accesses and 100.0 * tlb misses / cache_accesses to calculate the results. With random access turned off, we get L1 read access miss rate to be $$miss_{cache} = 1.5%$$ and TLB read miss rate $$miss_{tlb} \approx 0$$. When random access is turned on, we have $$miss_{cache} = 1.4%$$ and $$miss_{tlb} \approx 0$$. We can see the miss rate in all scenarios is really low. This is mainly because the inner most loop in our routine is performing operations on working set already presented in L1 cache and TLB. The read/write operations use continous cache lines, which means there will almost be no faults while we access the 512 cache lines. If one fault causes the entire new working set to be cached, then there would be no subsequent faults until the entire working set is iterated.</p><p>If we use <code>getrusage</code> we can see the metrics listed below:</p><table><thead><tr><th>Metrics</th><th>Mean</th><th>std dev</th></tr></thead><tbody><tr><td>utime</td><td>868629</td><td>126044</td></tr><tr><td>stime</td><td>253586</td><td>20112</td></tr><tr><td>maxrss</td><td>1049691</td><td>43</td></tr><tr><td>minflt</td><td>262214</td><td>1</td></tr><tr><td>majflt</td><td>0</td><td>0</td></tr><tr><td>inblock</td><td>0</td><td>0</td></tr><tr><td>oublock</td><td>0</td><td>0</td></tr><tr><td>nvcsw</td><td>0.4</td><td>0.54</td></tr><tr><td>nivcsw</td><td>47</td><td>7</td></tr></tbody></table><p><br></p><h2 id=mmap>mmap<a hidden class=anchor aria-hidden=true href=#mmap>#</a></h2><p>Next we are going to explore the behavior of mmap. Previously, we used <code>malloc</code> for data allocation. Next, we are going to instead use <code>mmap</code> and see what happens. Here we will only use read access for benchmark metrics since it&rsquo;s available in both L1 and TLB metrics.</p><p>First, we use the <code>MAP_ANONYMOUS</code> as a flag passed to <code>mmap</code>. This flag means the mapping is not backed by any file; its contents are initialized to zero. The complete call is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>mmap(NULL, length, PROT_READ <span style=color:#f92672>|</span> PROT_WRITE,
                MAP_PRIVATE <span style=color:#f92672>|</span> MAP_ANONYMOUS, fd_ignore, offset);
</code></pre></div><p>For more details, refer to <a href=https://man7.org/linux/man-pages/man2/mmap.2.html>mmap man page</a> for information.</p><p>When we turn on the random access and use <code>perf_event_open</code> interface to collect metrics, we see the L1 data cache read misses are 956148031 (std dev 84631843). The TLB data cache read misses are 3370309 (std dev 17792). We see it is not really different to the malloc approach we used before. Doing a simple <code>strace</code> shows <code>malloc</code> calls <code>mmap</code>. The memory that backs <code>malloc()</code> allocations is handled by the kernel in much the same way as the memory that backs private anonymous mappings created with <code>mmap()</code>.</p><p>Then, we try to use <code>mmap()</code> to create mapping in the virtual address space backed by a file instead of using <code>MAP_ANONYMOUS</code>.</p><p>We first test <code>mmap</code> with <code>MAP_PRIVATE</code>. According to the man page, this flags means creating a private copy-on-write mapping. Updates to the mapping are not visible to other processes mapping the same file, and are not carried through to the underlying file. It is unspecified whether changes made to the file after the mmap() call are visible in the mapped region.</p><blockquote><p>Note we should call <code>fallocate()</code> for the newly created file, otherwise mmap is gonna throw bur error.</p></blockquote><p>When we measure the L1 data cache miss, it&rsquo;s around 946128512 (std dev 956148031), nothing special happens. When we use <code>MAP_SHARED</code> flag, the result was similar. The result seems to fluctuates as time passes, but overall they are not much different. After all, it&rsquo;s just reading from the memory, whether the address is backed by a file or not doesn&rsquo;t play a big role in affecting the cache miss rate. The L1 data cache misses is shown below:</p><table><thead><tr><th>Flag</th><th>PRIVATE</th><th>PRIVATE+POPULATE</th><th>SHARED</th><th>SHARED+POPULATE</th></tr></thead><tbody><tr><td>Mean</td><td>783864673</td><td>769314361</td><td>842915231</td><td>816749524</td></tr><tr><td>Std dev</td><td>77816766</td><td>53913082</td><td>54613278</td><td>60580595</td></tr></tbody></table><p><br></p><p>If we take a look at TLB data cache, the result is</p><table><thead><tr><th>Flag</th><th>PRIVATE</th><th>PRIVATE+POPULATE</th><th>SHARED</th><th>SHARED+POPULATE</th></tr></thead><tbody><tr><td>Mean</td><td>3372303</td><td>3370740</td><td>3381755</td><td>3377370</td></tr><tr><td>Std dev</td><td>9884</td><td>13567</td><td>17626</td><td>11776</td></tr></tbody></table><p><br></p><p>Still, there doesn&rsquo;t seem have any significant fluctuation in the number of misses in data TLB. This pattern also applies to sequential access, except the TLB data cache misses is alot lower in sequentual access.</p><p>Now If we instead use <code>getrusage()</code>, we will get something like this</p><table><thead><tr><th>Flag</th><th>PRIVATE</th><th>PRIVATE+POPULATE</th><th>SHARED</th><th>SHARED+POPULATE</th></tr></thead><tbody><tr><td>Usec/std dev</td><td>20/0</td><td>20/0</td><td>20/0</td><td>20/0</td></tr><tr><td>usec/std dev</td><td>801512/ 78346</td><td>793452/ 143556</td><td>872342/ 124124</td><td>671957/ 229314</td></tr><tr><td>Ssec/std dev</td><td>0/0</td><td>0/0</td><td>0/0</td><td>0/0</td></tr><tr><td>ssec/std dev</td><td>475977/ 54355</td><td>475678/ 134253</td><td>445467/ 99345</td><td>536041/ 98797</td></tr><tr><td>oublock/std dev</td><td>0/0</td><td>0/0</td><td>2997152/ 82256</td><td>2097152/ 19760</td></tr></tbody></table><p><br></p><p>The most interesting part here is when <code>MAP_SHARED</code> is enabled, the <code>oublock</code> immediately changes. As we mentioned previously, <code>oublock</code> specifies the number of times the file system had to write to the disk on behalf of processes. Because the address is now backed by a file, all write operations will cause the file system to write the contents back to the file.</p><p><code>mmap()</code> creates a new mapping in the virtual address space of the
calling process. However, it doesn&rsquo;t allocate RAM. If we call <code>memset()</code> then followed by <code>msync()</code> with <code>MS_SYNC</code> flag, we can get some interesting results in <code>getrusage</code>, these observations are summarized here:</p><ul><li>kernel space time is much higher. It usually take 1 sec (no std dev) as opposed to 0. Synchronizing to files on disk will require more kernel participation.</li><li>minflt (the number of page faults which were serviced without requiring any I/O) was muich higher, the value is around 540782(std dev 3). More memory mapped means the faults by I/O will be less likely.</li><li>oublock is much higher, the value is around 4196512(std dev 1). The sync operating means there will be approximatly double amount of writes to disk.</li><li>nvcsw was higher, there are more voluntary context switches. Writing results to disk has delay, and thus the process likely need to context switch while waiting for I/O to be finished.</li></ul><p>We may notice the number data TLB misses is lower than the total number of page the application uses. One obvious answer the use of huge page. One huge page can cover many small pages. Also, because we have prefetching TLB and the working set access pattern is contiguous, TLB hit rate will be high. Because we have a set-associative TLB cache, and we access the memory in a fairly deterministic way, it&rsquo;s easy to predict where the next access is pointing to. For example, if the replacement policy is FIFO, then each cache line will remain untouched for exact same clock cycle before replaced. This also applies to other policies. One way to determine the replacement algorithm is using P-Chase.</p><h2 id=strace>strace<a hidden class=anchor aria-hidden=true href=#strace>#</a></h2><p>We then use <code>strace</code> to trace syscalls of our application. The output contains some interesting information, one is</p><pre><code>access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)
...
arch_prctl(ARCH_SET_FS, 0x7fdc6ad83540) = 0
</code></pre><p>According to <a href=https://man7.org/linux/man-pages/man2/arch_prctl.2.html>arch_prctl man page</a>, <code>arch_prctl()</code> sets architecture-specific process or thread state. The <code>ARCH_SET_FS</code> option sets the 64-bit base for the FS register to addr, in our case it&rsquo;s 0x7fdc6ad83540. Let&rsquo;s set a break point at <code>arch_prctl</code> and backtrace from there</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>#0  0x00007ffff7febb55 in ?? () from /lib64/ld-linux-x86-64.so.2
#1  0x00007ffff7fd104c in ?? () from /lib64/ld-linux-x86-64.so.2
#2  0x00007ffff7fd0108 in ?? () from /lib64/ld-linux-x86-64.so.2
#3  0x0000000000000001 in ?? ()
#4  0x00007fffffffe2fa in ?? ()
#5  0x0000000000000000 in ?? ()
</code></pre></div><p>We can see the FS segment base is set by the <code>ld-linux</code>, which is a part of glibc, during the program loading. A simple google seach tells us <code>/lib64/ld-linux-x86-64.so.2</code> is a dynamic linker. A more detailed description can be found on this <a href=https://unix.stackexchange.com/questions/400621/what-is-lib64-ld-linux-x86-64-so-2-and-why-can-it-be-used-to-execute-file>post</a> and <a href=https://lwn.net/Articles/631631/>lwn.net</a>. During the startup, the loader initalizes TLS. The includes memory allocation and setting FS base value to point to the TLS beignning, which is done via the <code>arch_prctl</code> syscall. More can be found <a href=https://unix.stackexchange.com/questions/453749/what-sets-fs0x28-stack-canary/453772>here</a>. This <code>init_tls()</code> is called <a href="https://git.launchpad.net/glibc/tree/elf/rtld.c?id=916124ed841745b7a1e0fbc43f9909340b47d373#n1397">here</a>, which subsequently calls the actuall <a href=https://git.launchpad.net/glibc/tree/sysdeps/x86_64/nptl/tls.h#n153>syscall</a> in <code>tls.h</code>.</p><p>The /etc/ld.so.preload has similarities to LD_PRELOAD, in addition, it doesn&rsquo;t suffer security limitation posed by LD_PRELOAD (<a href=https://superuser.com/questions/1183037/what-is-does-ld-so-preload-do>explanation here</a>). This a feature of <em>glibc</em>.</p><h2 id=competing-for-memory>Competing for Memory<a hidden class=anchor aria-hidden=true href=#competing-for-memory>#</a></h2><p>Next we are going to fork another process that will compete for memory with our process under test. We will use this code snippet which is going to be executed by both the parent and the child process</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>compete_for_memory</span>(<span style=color:#66d9ef>void</span><span style=color:#f92672>*</span> unused) {
   <span style=color:#66d9ef>long</span> mem_size <span style=color:#f92672>=</span> get_mem_size();
   <span style=color:#66d9ef>int</span> page_sz <span style=color:#f92672>=</span> sysconf(_SC_PAGE_SIZE);
   printf(<span style=color:#e6db74>&#34;Total memsize is %3.2f GBs</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>,
         (<span style=color:#66d9ef>double</span>)mem_size<span style=color:#f92672>/</span>(<span style=color:#ae81ff>1024</span><span style=color:#f92672>*</span><span style=color:#ae81ff>1024</span><span style=color:#f92672>*</span><span style=color:#ae81ff>1024</span>));
   fflush(stdout);
   <span style=color:#66d9ef>char</span><span style=color:#f92672>*</span> p <span style=color:#f92672>=</span> mmap(NULL, mem_size, PROT_READ <span style=color:#f92672>|</span> PROT_WRITE,
                  MAP_NORESERVE<span style=color:#f92672>|</span>MAP_PRIVATE<span style=color:#f92672>|</span>MAP_ANONYMOUS, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, (off_t) <span style=color:#ae81ff>0</span>);
   <span style=color:#66d9ef>if</span> (p <span style=color:#f92672>==</span> MAP_FAILED)
      perror(<span style=color:#e6db74>&#34;Failed anon MMAP competition&#34;</span>);

   <span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
   <span style=color:#66d9ef>while</span>(<span style=color:#ae81ff>1</span>) {
      <span style=color:#66d9ef>volatile</span> <span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>a;
      <span style=color:#66d9ef>long</span> r <span style=color:#f92672>=</span> simplerand() <span style=color:#f92672>%</span> (mem_size<span style=color:#f92672>/</span>page_sz);
      <span style=color:#66d9ef>char</span> c;
      <span style=color:#66d9ef>if</span>( i <span style=color:#f92672>&gt;=</span> mem_size<span style=color:#f92672>/</span>page_sz ) {
         i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
      }
      <span style=color:#75715e>// One read and write per page
</span><span style=color:#75715e></span>      <span style=color:#75715e>//a = p + i * page_sz; // sequential access
</span><span style=color:#75715e></span>      a <span style=color:#f92672>=</span> p <span style=color:#f92672>+</span> r <span style=color:#f92672>*</span> page_sz;
      c <span style=color:#f92672>+=</span> <span style=color:#f92672>*</span>a;
      <span style=color:#66d9ef>if</span>((i<span style=color:#f92672>%</span><span style=color:#ae81ff>8</span>) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
         <span style=color:#f92672>*</span>a <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
      }
      i<span style=color:#f92672>++</span>;
   }
   <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
}
</code></pre></div><p>The <code>get_mem_size()</code> is implemented using this <a href=https://stackoverflow.com/questions/22670257/getting-ram-size-in-c-linux-non-precise-result>portable code</a></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>#if defined(_WIN32)
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;Windows.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#75715e>#elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) &amp;&amp; defined(__MACH__))
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;unistd.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;sys/types.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;sys/param.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#if defined(BSD)
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;sys/sysctl.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#endif
</span><span style=color:#75715e></span>
<span style=color:#75715e>#else
</span><span style=color:#75715e>#error &#34;Unable to define getMemorySize( ) for an unknown OS.&#34;
</span><span style=color:#75715e>#endif
</span><span style=color:#75715e></span>
<span style=color:#75715e>/**
</span><span style=color:#75715e> * Returns the size of physical memory (RAM) in bytes.
</span><span style=color:#75715e> */</span>
size_t <span style=color:#a6e22e>getMemorySize</span>( )
{
<span style=color:#75715e>#if defined(_WIN32) &amp;&amp; (defined(__CYGWIN__) || defined(__CYGWIN32__))
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* Cygwin under Windows. ------------------------------------ */</span>
    <span style=color:#75715e>/* New 64-bit MEMORYSTATUSEX isn&#39;t available.  Use old 32.bit */</span>
    MEMORYSTATUS status;
    status.dwLength <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(status);
    GlobalMemoryStatus( <span style=color:#f92672>&amp;</span>status );
    <span style=color:#66d9ef>return</span> (size_t)status.dwTotalPhys;

<span style=color:#75715e>#elif defined(_WIN32)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* Windows. ------------------------------------------------- */</span>
    <span style=color:#75715e>/* Use new 64-bit MEMORYSTATUSEX, not old 32-bit MEMORYSTATUS */</span>
    MEMORYSTATUSEX status;
    status.dwLength <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(status);
    GlobalMemoryStatusEx( <span style=color:#f92672>&amp;</span>status );
    <span style=color:#66d9ef>return</span> (size_t)status.ullTotalPhys;

<span style=color:#75715e>#elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) &amp;&amp; defined(__MACH__))
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* UNIX variants. ------------------------------------------- */</span>
    <span style=color:#75715e>/* Prefer sysctl() over sysconf() except sysctl() HW_REALMEM and HW_PHYSMEM */</span>

<span style=color:#75715e>#if defined(CTL_HW) &amp;&amp; (defined(HW_MEMSIZE) || defined(HW_PHYSMEM64))
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>int</span> mib[<span style=color:#ae81ff>2</span>];
    mib[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> CTL_HW;
<span style=color:#75715e>#if defined(HW_MEMSIZE)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_MEMSIZE;            <span style=color:#75715e>/* OSX. --------------------- */</span>
<span style=color:#75715e>#elif defined(HW_PHYSMEM64)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_PHYSMEM64;          <span style=color:#75715e>/* NetBSD, OpenBSD. --------- */</span>
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>    int64_t size <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;               <span style=color:#75715e>/* 64-bit */</span>
    size_t len <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>( size );
    <span style=color:#66d9ef>if</span> ( sysctl( mib, <span style=color:#ae81ff>2</span>, <span style=color:#f92672>&amp;</span>size, <span style=color:#f92672>&amp;</span>len, NULL, <span style=color:#ae81ff>0</span> ) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> )
        <span style=color:#66d9ef>return</span> (size_t)size;
    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0L</span>;          <span style=color:#75715e>/* Failed? */</span>

<span style=color:#75715e>#elif defined(_SC_AIX_REALMEM)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* AIX. ----------------------------------------------------- */</span>
    <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_AIX_REALMEM ) <span style=color:#f92672>*</span> (size_t)<span style=color:#ae81ff>1024L</span>;

<span style=color:#75715e>#elif defined(_SC_PHYS_PAGES) &amp;&amp; defined(_SC_PAGESIZE)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* FreeBSD, Linux, OpenBSD, and Solaris. -------------------- */</span>
    <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_PHYS_PAGES ) <span style=color:#f92672>*</span>
        (size_t)sysconf( _SC_PAGESIZE );

<span style=color:#75715e>#elif defined(_SC_PHYS_PAGES) &amp;&amp; defined(_SC_PAGE_SIZE)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* Legacy. -------------------------------------------------- */</span>
    <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_PHYS_PAGES ) <span style=color:#f92672>*</span>
        (size_t)sysconf( _SC_PAGE_SIZE );

<span style=color:#75715e>#elif defined(CTL_HW) &amp;&amp; (defined(HW_PHYSMEM) || defined(HW_REALMEM))
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* DragonFly BSD, FreeBSD, NetBSD, OpenBSD, and OSX. -------- */</span>
    <span style=color:#66d9ef>int</span> mib[<span style=color:#ae81ff>2</span>];
    mib[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> CTL_HW;
<span style=color:#75715e>#if defined(HW_REALMEM)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_REALMEM;        <span style=color:#75715e>/* FreeBSD. ----------------- */</span>
<span style=color:#75715e>#elif defined(HW_PYSMEM)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_PHYSMEM;        <span style=color:#75715e>/* Others. ------------------ */</span>
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> size <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;      <span style=color:#75715e>/* 32-bit */</span>
    size_t len <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>( size );
    <span style=color:#66d9ef>if</span> ( sysctl( mib, <span style=color:#ae81ff>2</span>, <span style=color:#f92672>&amp;</span>size, <span style=color:#f92672>&amp;</span>len, NULL, <span style=color:#ae81ff>0</span> ) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> )
        <span style=color:#66d9ef>return</span> (size_t)size;
    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0L</span>;          <span style=color:#75715e>/* Failed? */</span>
<span style=color:#75715e>#endif </span><span style=color:#75715e>/* sysctl and sysconf variants */</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#75715e>#else
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0L</span>;          <span style=color:#75715e>/* Unknown OS. */</span>
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>}
</code></pre></div><p>The important line is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c> <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_PHYS_PAGES ) <span style=color:#f92672>*</span>
        (size_t)sysconf( _SC_PAGESIZE );
</code></pre></div><p>One thing to notice in the routine for competing for memory is we used <code>fflush</code> after the <code>printf</code>. The purpose of <code>fflush(stream)</code> is to make the operating system flush any buffers to the underlying file. This mainly because stdout is buffered. The buffer is not flushed until newline. <code>fflush</code> will cause this process to happen with the absense of newline. stderr is unbuffered and thus fflush would not be necessary.</p><p>For this experiment, we tested it on a VM. The reason is because the contending process will take all RAM and completely hault the mahcine if tested on the host. To ensure our VM has enough swap space, we follow this <a href=https://wiki.crowncloud.net/?adding_swap_kvm>tutorial</a> to create 4GB of swap area (we allocated 2GB RAM for VM).</p><p>One thing we observe is that the execution time of the program become significantly longer to run. In our experiement we need to limit the number of iterations from 1 &#171; 20 to 1 &#171; 8 to get some sensible results without running for days.</p><p>When we use PRIVATE and ANONYMUS option and random access turned on, the misses in data TLB is 335009(std dev 7298). We can&rsquo;t get access to L1 cache data because it will cause the session to be automatically logged out whenever L1D is used. here are some interesting things to notice:</p><ul><li><em>MAP_PRIVATE + MAP_ANONYMOUS</em>: TLB misses:335009(std dev 17298)<br>minflt: 4220(std dev 231)<br>oublock: 8(std dev 4)<br>nivcsw: 19(10)</li><li><em>MAP_SHARED</em>: TLB misses:251284std dev 103292)<br>minflt: 2784(std dev 231)<br>majflt: 247(std 65)<br>oublock: 18200(std dev 2987)<br>nivcsw: 8(7)</li></ul><p>The most important difference here is that the oublock is much easier to trigger because the constant swapping. When file backed memory is used we also notice that majflt is much higher. Because pages are constantly traveling between swap area and memory, the page fault rate becomes a lot higher. The oublock also follows previous patterns as the file backed memory requires filesystem involvement.</p><p>Finally, we also modify the kernel&rsquo;s head (or more precisely its LRU page replacement algorithm). Look in <code>mm/vmscan.c</code> there&rsquo;s a function calleed <code>shrink_page_list</code>. In it, you will see a switch statement with a PAGEREF_ACTIVATE case, which is the case where the kernel sees the page has been recently accessed. In this case the kernel gotos activate_locked, but you will change it to to do the same thing as the PAGEREF_RECLAIM case. We can simply move the case down and change its default behavior to direct to the PAGEREF_RECLAIM case. After that, we need to recompile the kernel for VM. We also summarize the most interesting results:</p><ul><li><em>MAP_PRIVATE + MAP_ANONYMOUS</em>: TLB misses:308031(std dev 17298)<br>minflt: 4223(std dev 791)<br>oublock: 8(std dev 1)<br>nivcsw: 11(5)</li><li><em>MAP_SHARED</em>: TLB misses: 251284std dev 103292)<br>minflt: 2724(std dev 231)<br>majflt: 0(std 0)<br>oublock: 18200(std dev 2987)<br>nivcsw: 8(7)</li></ul><p>We can see that the most of the pattern follow the previous result after the modified kernel is installed. One main difference is majflt value is reduced back down.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.bodunhu.com/blog/tags/os/>os</a></li><li><a href=https://www.bodunhu.com/blog/tags/kernel/>kernel</a></li><li><a href=https://www.bodunhu.com/blog/tags/mm/>mm</a></li><li><a href=https://www.bodunhu.com/blog/tags/linux/>linux</a></li></ul><nav class=paginav><a class=prev href=https://www.bodunhu.com/blog/posts/mathjaxjekyll/><span class=title>Â« Prev Page</span><br><span>Add MathJax v3 Support to Jekyll and Hugo</span></a>
<a class=next href=https://www.bodunhu.com/blog/posts/vmwareesxserver/><span class=title>Next Page Â»</span><br><span>Memory Resource Management in VMware ESX Server</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Linux Program Measurement and mmap on twitter" href="https://twitter.com/intent/tweet/?text=Linux%20Program%20Measurement%20and%20mmap&url=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2flinuxkernelmeasurementandmmap%2f&hashtags=os%2ckernel%2cmm%2clinux"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Linux Program Measurement and mmap on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2flinuxkernelmeasurementandmmap%2f&title=Linux%20Program%20Measurement%20and%20mmap&summary=Linux%20Program%20Measurement%20and%20mmap&source=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2flinuxkernelmeasurementandmmap%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Linux Program Measurement and mmap on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2flinuxkernelmeasurementandmmap%2f&title=Linux%20Program%20Measurement%20and%20mmap"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Linux Program Measurement and mmap on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2flinuxkernelmeasurementandmmap%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Linux Program Measurement and mmap on whatsapp" href="https://api.whatsapp.com/send?text=Linux%20Program%20Measurement%20and%20mmap%20-%20https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2flinuxkernelmeasurementandmmap%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Linux Program Measurement and mmap on telegram" href="https://telegram.me/share/url?text=Linux%20Program%20Measurement%20and%20mmap&url=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2flinuxkernelmeasurementandmmap%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2021 <a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>