<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZLK2GHB055"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZLK2GHB055")}</script><link rel=canonical href=https://www.bodunhu.com/blog/posts/pathways-googles-new-ml-system/><title>std::bodun::blog</title></head><body><header id=banner><span><a href=https://www.bodunhu.com/blog/>blog</a> | <a href=https://www.bodunhu.com/blog//posts>posts</a> | <a href=https://www.bodunhu.com/>about</a></span></header><main id=content><article><header id=post-header><h2>Pathways: Google's New ML System</h2><div><time>November 17, 2025</time></div></header><aside id=toc><h4>Table of Contents</h4><nav id=TableOfContents><ul><li><a href=#single-controller>Single-Controller</a></li><li><a href=#multi-controller-systems>Multi-Controller Systems</a></li><li><a href=#going-back-to-single-controller>Going Back to Single-Controller</a></li><li><a href=#deadlock>Deadlock</a></li><li><a href=#solutions-to-deadlock>Solutions to Deadlock</a></li></ul></nav></aside><p>Google recently released the paper about its new ML system called <a href=https://arxiv.org/pdf/2203.12533.pdf>Pathways</a>. I&rsquo;m a bit surprised since I expect it to introduce a brand new model architecture. In fact, this paper is not easy to digest at all. I feel like it&rsquo;s written for people who spent many years developing ML frameworks. Anyway, we will try to understand why it is developed and how it works. Also, you should check this <a href=https://zhuanlan.zhihu.com/p/495592456>post</a> (in Chinese). This post explains many concepts in Pathways much more clearly. Many contents here are credited to this post.</p><p>This paper spends a long time discussing single-controller and multi-controller. It&rsquo;s really confusing to understand all these SPMD, MPMD, single-controller, and multi-controller stuffs. Pathways claims the future ML framework should go <strong>back</strong> to single-controller. By &ldquo;back&rdquo; I mean ML frameworks were originally single-controller, then they adopted multi-controller. Now, we are going back to single-controller again.</p><h2 id=single-controller>Single-Controller</h2><p>TensorFlow v1 is a classic example of single-controller system. The high level idea is the user would define a dataflow graph through a Python client. This graph is then submitted to the <code>session.run</code> (runtime system). The system consists of a single master and many other workers. The mater will compile and the dataflow graph submitted by the client, then divides the graph into sub-graphs. Then the master submits those subgraphs to other workers.</p><p>In this case, each worker computes its own share of sub-graph. The client + master are the controller.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_pics/posts/pathways/tf1-spmd.png alt=tf1-spmd></p><center>Fig. control messages (oranges lines) need to go through slow DCN between Ctrlr and hosts</center><p>As the paper suggests, dispatching computations in a single-controller system requires communnication across (data center network) DCN. All the orange lines are control messages flowing through DCN. We can see the workers are idle for a long time between each step, even though there&rsquo;s no gap between adjust steps on the controller.</p><p>The controller submits jobs to all workers in each step, then waits all workers to finish computing their own sub-graphs. The problem is: 1) waiting for all workers to finish computation in a lock-step fashion is inefficient; 2) send and wait for control messages (orange line) is costly since these messages go through slow DCN.</p><h2 id=multi-controller-systems>Multi-Controller Systems</h2><p>Contrary to single-controller systems, multi-controller systems like Jax adopts a different philosophy. Under multi-controller systems, each worker shares the same code and executes different stage/branch of the code. This is why they are called SPMD systems (single-program-multiple-data).</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_pics/posts/pathways/jax-spmd.png alt=jax-spmd></p><center>Fig. Dispatching jobs only happens locally on hosts without going through DCN</center><p>Take MPI process as an example, every MPI process is an entrance (client) to the program (In single-controller systems, only the client-master can be the entrance).</p><p>Since multi-controller systems doesn&rsquo;t have a centralized coordinator, all workers in can initiate communication with each other, using much faster channels such as PCIe or NVLink. In the multi-controller graph, the black dotted lines represents message between hosts and devices (through PCIe); the communication between devices happens through fast NVLink. So we don&rsquo;t have the big overhead introduced by DCN.</p><p>If you want to get a taste of how PyTorch vs TensorFlow v1&rsquo;s (multi-controller vs single-controller) programming style feels like, here are two examples: <a href=https://pytorch.org/tutorials/intermediate/dist_tuto.html>Writing Distributed Applications with PyTorch</a> and <a href=https://netweblog.wordpress.com/2018/04/10/distributed-tensorflow-sample-code-and-how-it-works/>End-to-End Tutorial for Distributed TensorFlow 1.x</a>.</p><h2 id=going-back-to-single-controller>Going Back to Single-Controller</h2><p>We could stick with multi-controller systems forever. If every worker node shares symmetric workloads and communications (like all-reduce, all-gather, etc.), then there&rsquo;s nothing to be worried about. After all, multi-controller seems much more efficient than single-controller based on what we&rsquo;ve discussed so far.</p><p>However, pipeline parallelism changes the story. Under pipeline parallelism, different workers in the pipeline will execute at different programs. Thus we have MPMD (multi-program-multi-data). For example, we can have one worker doing convolution for batch 1 while another worker is doing encoding work on batch 2. At each stage of the pipeline, the worker is doing different jobs on a different data batch (think of a CPU pipeline where each stage is executing different instructions).</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_pics/posts/pathways/tf1-non-spmd.png alt=tf1-non-spmd></p><p>Take the above graph as an example, assume we have three workers 1, 2, 3 from top to bottom. Each worker is performing asymmetric workloads and doing irregular point-to-point communications (instead of symmetric communications like all-gather). Obviously, multi-controller doesn&rsquo;t fit into this kind of workload. How do you write a single copy of code that does all these irregular communications under multi-process scenarios?</p><p>Thus, Pathways proposes we should go back to single-controller, so that we can let the master node handle all these nasty communication patterns.</p><h2 id=deadlock>Deadlock</h2><p>Single-Controller brings back <em>gang-scheduling</em> and <em>centralized coordinator</em>. The reason to use <em>gang-scheduling</em> and <em>centralized coordinator</em> is to help preventing deadlocks. However, the rational behind this design decision is hard to interpret from reading the paper. I&rsquo;m going to use the <a href=https://zhuanlan.zhihu.com/p/495592456>post</a> from Jinhui Yan (the developer behind <a href=https://github.com/Oneflow-Inc/oneflow>OneFlow</a>) to explain why <em>gang-scheduling</em> and <em>centralized coordinator</em> prevent deadlocks.</p><blockquote><p>Gang-scheduling is essential in the case of TPUs, since they are single-threaded and only run non-preemptible kernels, so the system will deadlock if communicating computations are not enqueued in a consistent order.</p></blockquote><p>We can think of a computing device as a FIFO task queue (e,g. CUDA streams, TPU, or CPU&mldr;). Each FIFO task queue essentially have a stream of tasks to process.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_pics/posts/pathways/fifo.jpg alt=FIFO-queue></p><center>Src. Jinhui Yan</center><p>The paper emphasizes that TPUs are single-threaded and only run non-preemptible kernels. That means we can think of each TPU as a single FIFO task queue. Once we enqueue a task, it can not be preempted from the queue. We need to wait until this task finishes its computation before we can execute the next task in the queue. This is a problem!</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_pics/posts/pathways/deadlock.jpg alt=deadlock></p><center>Src. Jinhui Yan</center><p>Imagine we have two devices (1 and 2), represented as two FIFO queues. Device 1 chooses to enqueue task <code>A</code> first and then <code>B</code>; device 2 decides to enqueue task <code>B</code> first and then <code>A</code>. Both tasks <code>A</code> and <code>B</code> are performing an all-scatter operation. Therefore, task <code>A</code> on device 1 needs to wait for messages from task <code>A</code> on device 2. Similarly, task <code>B</code> on device 2 needs to wait for messages from task <code>B</code> on device 1.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_pics/posts/pathways/deadlock-conditions.png alt=deadlock-conditions></p><p>This is a classical example of deadlock in operating systems.</p><h2 id=solutions-to-deadlock>Solutions to Deadlock</h2><p>Using gang-scheduling helps preventing deadlocks, because it enforces a global enqueueing order across multiple FIFO queues, instead of letting each queue handling tasks separately.</p><p>The paper also mentions allowing device (e.g. GPUs) to execute tasks concurrently can prevent deadlocks. This is because concurrency eliminates the non-preemption property which is required for deadlocks to happen.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_pics/posts/pathways/concurrency.jpg alt=concurrency></p><center>Src. Jinhui Yan</center><p>If each devices allows concurrency executions (each device has multiple queues), then the task on one queue can be preemptied to allow the other task start executing, thus no deadlock (this is not strictly the case, the <a href=https://zhuanlan.zhihu.com/p/495592456>post</a> explains an interesting scenario in <a href=https://developer.nvidia.com/nccl>NCCL</a> where deadlocks can still happen if there are too many communications).</p><script src=https://giscus.app/client.js data-repo=BDHU/blog-comments data-repo-id=R_kgDOKZLDLA data-category=Announcements data-category-id=DIC_kwDOKZLDLM4CZrU- data-mapping=pathname data-strict=0 data-reactions-enabled=0 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async></script></article></main><footer id=footer><p>Â© 2025 Bodun Hu â€¢
<a href=/blog/index.xml>RSS</a></p></footer></body></html>