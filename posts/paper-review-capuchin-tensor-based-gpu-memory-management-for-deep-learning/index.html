<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning | std::bodun::blog</title><meta name=title content="Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning"><meta name=description content="This paper aims to reduce GPU memory usage during DNN training. Capuchin achieves this goal though swapping and recomputation, using tensor as unit of operation. The major question is how to balance between swapping and recomputation to achieve max resource utilization.
Swap and Recomputation Benefit The ultimate goal of swapping and recomputation is to hide the overhead as much as possible to minimize the wait time of back-access (a tensor evicted earlier being accessed again)."><meta name=keywords content="paper-review,ml,"><meta property="og:title" content="Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning"><meta property="og:description" content="This paper aims to reduce GPU memory usage during DNN training. Capuchin achieves this goal though swapping and recomputation, using tensor as unit of operation. The major question is how to balance between swapping and recomputation to achieve max resource utilization.
Swap and Recomputation Benefit The ultimate goal of swapping and recomputation is to hide the overhead as much as possible to minimize the wait time of back-access (a tensor evicted earlier being accessed again)."><meta property="og:type" content="article"><meta property="og:url" content="https://www.bodunhu.com/blog/posts/paper-review-capuchin-tensor-based-gpu-memory-management-for-deep-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-07T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-19T11:05:38-05:00"><meta property="og:site_name" content="std::bodun::blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning"><meta name=twitter:description content="This paper aims to reduce GPU memory usage during DNN training. Capuchin achieves this goal though swapping and recomputation, using tensor as unit of operation. The major question is how to balance between swapping and recomputation to achieve max resource utilization.
Swap and Recomputation Benefit The ultimate goal of swapping and recomputation is to hide the overhead as much as possible to minimize the wait time of back-access (a tensor evicted earlier being accessed again)."><meta name=twitter:site content="@https://twitter.com/BodunHu"><meta itemprop=name content="Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning"><meta itemprop=description content="This paper aims to reduce GPU memory usage during DNN training. Capuchin achieves this goal though swapping and recomputation, using tensor as unit of operation. The major question is how to balance between swapping and recomputation to achieve max resource utilization.
Swap and Recomputation Benefit The ultimate goal of swapping and recomputation is to hide the overhead as much as possible to minimize the wait time of back-access (a tensor evicted earlier being accessed again)."><meta itemprop=datePublished content="2021-11-07T00:00:00+00:00"><meta itemprop=dateModified content="2023-04-19T11:05:38-05:00"><meta itemprop=wordCount content="402"><meta itemprop=keywords content="paper-review,ml,"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style></head><body><header><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><a href=/blog/ class=title><h2>std::bodun::blog</h2></a><nav><a href=/blog/>Home</a>
<a href=/posts/>Archive</a>
<a href=https://www.bodunhu.com/>About</a></nav></header><main><h1>Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning</h1><p><i><time datetime=2021-11-07 pubdate>07 Nov, 2021</time></i></p><content><p>This <a href=https://dl.acm.org/doi/pdf/10.1145/3373376.3378505>paper</a> aims to reduce GPU memory usage during DNN training. Capuchin achieves this goal though <em>swapping</em> and <em>recomputation</em>, using <em>tensor</em> as unit of operation. The major question is how to balance between swapping and recomputation to achieve max resource utilization.</p><h2 id=swap-and-recomputation-benefit>Swap and Recomputation Benefit</h2><p>The ultimate goal of swapping and recomputation is to hide the overhead as much as possible to minimize the wait time of <em>back-access</em> (a tensor evicted earlier being accessed again). For swapping, we should increase the overlap between swapping and computing; for recomputation, we should use cheap operations.</p><h3 id=determining-tensor-re-generation-cost>Determining Tensor Re-generation Cost</h3><p>For swapping, it is usually not optimal to swap back in a tensor only when we access it. The reason is copying tensor from CPU memory to GPU memory usually introduces overhead greater than the computation itself. It&rsquo;s thus better to swap in a tensor <em>earlier</em> or <em>proactively</em>.</p><p>The paper uses <em>in-trigger</em> as the term. It means we use other tensor access between <em>evicted-access</em> (a tensor access that triggers the self-eviction after used in the computation) and <em>back-access</em> to bring back an evicted tensor a little bit earlier.</p><p>Of course, this may raise two questions:</p><ul><li>How do we know when <em>in-trigger</em> should happen?</li><li>How to deal with PCIe lane interferences? E.g. one swap-in may happen later than in-trigger due to a previous swap-in still not finished.</li></ul><p>The answer is quite simple. We use the runtime feedback at the back-access of a tensor. If the tensor is still being swapped in, it means the in-trigger time should be adjusted earlier. Note, this is based on the assumption of <strong>regular tensor access pattern in deep learning training</strong>, as illustrated in the paper.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/capuchin/tensor-access-pattern.png#center alt=tensor-access-pattern></p><p>Recomputation, on the other hand, is performed only in <em>on-demand</em> manner. No in-trigger is used for recomputation.</p><p>Capuchin relies on the principle that swap can be largely overlapped with computation, while recomputation will certainly incur performance penalty. Thus, it chooses swapping as the first choice until we cannot choose an in-trigger to perfectly hide prefetching overhead.</p><p>One thing to note here is when we select a tensor \(T\) to be recomputed, but such tensor relies on another tensor that is evicted, then we need to recompute the parent of the evicted tensor instead. This could potentially happen multiple times if more recomputation targets tensor \(T\). In short, recomputation and swapping cannot occur at the same time.</p><p>For more information, please refer to the original <a href=https://dl.acm.org/doi/pdf/10.1145/3373376.3378505>paper</a>.</p></content><p><a href=https://www.bodunhu.com/blog/tags/paper-review/>#paper-review</a>
<a href=https://www.bodunhu.com/blog/tags/ml/>#ml</a></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>