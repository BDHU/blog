<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="PhD student at University of Texas at Austin ðŸ¤˜. Doing systems for ML."><link rel="shortcut icon" href=https://www.bodunhu.com/blog/favicon.ico><link rel=stylesheet href=/blog/css/style.min.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js async></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-108144808-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<link rel=canonical href=https://www.bodunhu.com/blog/posts/paper-review-capuchin-tensor-based-gpu-memory-management-for-deep-learning/><title>Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning</title></head><body><header id=return><h2></h2><a href=https://www.bodunhu.com/blog/>std::bodun::blog::</a></header><main id=content><article><header id=post-header><h1>Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning</h1><div><time>May 1, 2022</time></div></header><p>This <a href=https://dl.acm.org/doi/pdf/10.1145/3373376.3378505>paper</a> aims to reduce GPU memory usage during DNN training. Capuchin achieves this goal though <em>swapping</em> and <em>recomputation</em>, using <em>tensor</em> as unit of operation. The major question is how to balance between swapping and recomputation to achieve max resource utilization.</p><h2 id=swap-and-recomputation-benefit>Swap and Recomputation Benefit</h2><p>The ultimate goal of swapping and recomputation is to hide the overhead as much as possible to minimize the wait time of <em>back-access</em> (a tensor evicted earlier being accessed again). For swapping, we should increase the overlap between swapping and computing; for recomputation, we should use cheap operations.</p><h3 id=determining-tensor-re-generation-cost>Determining Tensor Re-generation Cost</h3><p>For swapping, it is usually not optimal to swap back in a tensor only when we access it. The reason is copying tensor from CPU memory to GPU memory usually introduces overhead greater than the computation itself. It&rsquo;s thus better to swap in a tensor <em>earlier</em> or <em>proactively</em>.</p><p>The paper uses <em>in-trigger</em> as the term. It means we use other tensor access between <em>evicted-access</em> (a tensor access that triggers the self-eviction after used in the computation) and <em>back-access</em> to bring back an evicted tensor a little bit earlier.</p><p>Of course, this may raise two questions:</p><ul><li>How do we know when <em>in-trigger</em> should happen?</li><li>How to deal with PCIe lane interferences? E.g. one swap-in may happen later than in-trigger due to a previous swap-in still not finished.</li></ul><p>The answer is quite simple. We use the runtime feedback at the back-access of a tensor. If the tensor is still being swapped in, it means the in-trigger time should be adjusted earlier. Note, this is based on the assumption of <strong>regular tensor access pattern in deep learning training</strong>, as illustrated in the paper.</p><p><img src=https://cdn.jsdelivr.net/gh/BDHU/Page_Pics/posts/capuchin/tensor-access-pattern.png#center alt=tensor-access-pattern></p><p>Recomputation, on the other hand, is performed only in <em>on-demand</em> manner. No in-trigger is used for recomputation.</p><p>Capuchin relies on the principle that swap can be largely overlapped with computation, while recomputation will certainly incur performance penalty. Thus, it chooses swapping as the first choice until we cannot choose an in-trigger to perfectly hide prefetching overhead.</p><p>One thing to note here is when we select a tensor \(T\) to be recomputed, but such tensor relies on another tensor that is evicted, then we need to recompute the parent of the evicted tensor instead. This could potentially happen multiple times if more recomputation targets tensor \(T\). In short, recomputation and swapping cannot occur at the same time.</p><p>For more information, please refer to the original <a href=https://dl.acm.org/doi/pdf/10.1145/3373376.3378505>paper</a>.</p><div style=width:100% id=comment><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=preferred-color-scheme crossorigin=anonymous async></script></div></article></main><footer id=footer><p>Â© 2022 Bodun Hu. All rights reserved.</p></footer></body></html>