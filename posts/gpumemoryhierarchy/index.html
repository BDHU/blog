<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pascal GPU memory and cache hierarchy | std::bodun::blog</title><meta name=keywords content="gpu,os,mm"><meta name=description content="Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program&rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation."><meta name=author content><link rel=canonical href=https://www.bodunhu.com/blog/posts/gpumemoryhierarchy/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.758151a3f029732139c5ed917da59127bed78e4d7fce57610c1cd24d4cb52b2a.css integrity="sha256-dYFRo/ApcyE5xe2RfaWRJ77Xjk1/zldhDBzSTUy1Kyo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.utexas.edu/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.utexas.edu/favicon-32x32.png><link rel=apple-touch-icon href=https://www.utexas.edu/apple-touch-icon.png><link rel=mask-icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-108144808-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Pascal GPU memory and cache hierarchy"><meta property="og:description" content="Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program&rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation."><meta property="og:type" content="article"><meta property="og:url" content="https://www.bodunhu.com/blog/posts/gpumemoryhierarchy/"><meta property="og:image" content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-01-15T00:00:00+00:00"><meta property="article:modified_time" content="2019-01-15T00:00:00+00:00"><meta property="og:site_name" content="std::bodun::blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Pascal GPU memory and cache hierarchy"><meta name=twitter:description content="Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program&rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.bodunhu.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Pascal GPU memory and cache hierarchy","item":"https://www.bodunhu.com/blog/posts/gpumemoryhierarchy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pascal GPU memory and cache hierarchy","name":"Pascal GPU memory and cache hierarchy","description":"Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program\u0026rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation.","keywords":["gpu","os","mm"],"articleBody":"Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program’s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation. Turing architecture might have different results, but the method we used here can be applied as well with slight modification. The method we use in this guide is inspired by the research paper: Dissecting GPU Memory Hierarchy through Microbenchmarking. Here we will explain how P-Chase works and walk through a small example.\n Memory Hierarchy Overview GPU memory hierarchy is different compared to CPU memory hierarchy. Using the terminologies of CUDA, GPU memory space can be categorized in these groups: register, constant memory, shared memory, texture memory, local memory, and global memory. Each different memory space have its own properties. Since we are interested the cache systems, here is a picture demonstrating the memory hierarchy of a NVIDIA GPU:\nImage source\nThe characteristics of each memory space can be found in NVIDIA CUDA C Programming Guide . Here we will focus on some target memory space we are interested in. The paper lists some properties of our target memory space:\n   Memory Type Cached Scope     Global R/W Yes All Threads   Shared R/W N/A Thread Blocks   Texture R Yes All Threads    Even though the paper targets Fermi, Kepler and Maxwell generations of GPU, the properties of the table still holds for Pascal GPU and possibly Turing as well. The cached global/texture memory uses a two-level caching system. The L1 cache is located in each stream multiprocessor (SM), while the L2 cache is off-chip and shared among all SMs. It is unified for instruction, data and page table access. According to CUDA documentation, like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. This function previously was served by the separate L1 cache in Fermi and Kepler. Page table is used by GPU to map virtual addresses to physical addresses, and is usually stored in the global memory. The page table is cached in TLB to reduce memory access latency. Once a thread cannot ﬁnd the page entry in the TLB, it would access the global memory to search in the page table, which introduced significant memory access latency. The GPU-specific shared memory is located in the SMs. On the Fermi and Kepler devices, it shares memory space with the L1 data cache. On Maxwell and Pascal devices, it has a dedicated space, since the functionality of the L1 and texture caches have been merged. One thing to note here is that shared memory is accessed by the thread blocks. Thread-blocks remain limited to 48 KB of shared memory in Pascal. Therefore, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block. This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM.\nHowever, we should be careful that by default, GP100 caches global loads in the L1/Texture cache. In contrast, GP104 follows Kepler and Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler. As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to nvcc at compile time. Even though both GP100 and GP104 belongs to Pascal family, we only focus on GP100 here because that’s the GPU we use. Another thing to notice is that unlike Maxwell but similar to Kepler, Pascal caches thread-local memory in the L1 cache. This can mitigate the cost of register spills compared to Maxwell. To illustrate our point, we checked both cudaDevAttrGlobalL1CacheSupported and cudaDevAttrLocalL1CacheSupported on Tesla P100 and GTX 1080 and find both attributes to be 1.\nIn addition to the L2 data cache, global memory data that is read-only for the entire lifetime of a kernel can be cached in the read-only data cache with a compute capability of 3.5 or above. We will also explore the size of this read-only cache using __ldg() intrinsic.\nP-Chase Most existing GPU microbenchmark studies on cache architecture assume a classical set-associative cache model with the least recently used (LRU) replacement policy, the same as the conventional CPU cache. So here we will use this assumption and proceed with our experiments. Here are some notations we will use throughout this post.\n   Notation Description Notation Description     C Cache Size N array size   b cache line size s stride size   a cache associativity k iterations   T number of cache set r cache miss rate    Under our assumptions, data is loaded from main memory to lower cache in the basic unit of a cache line. The number of words in a cache line is referred to as the line size (b). For the LRU set-associative cache, the cache memory is divided into T cache sets, each of which consists of \\(a\\) cache lines. It is essential to have these three assumptions using this kind of cache model:\n  Assumption 1 All cache sets have the same size. The cache parameter should satisfy \\(T \\cdot a \\cdot b = C\\).\n  Assumption 2 In the memory address, the bits representing the cache set are immediately followed by the bits representing the offset.\n  Assumption 3 Cache replacement policy should be LRU.\n  We will later see why these assumptions are essential as we proceed with the experiment. We won’t go through how P-Chase work exactly. To find more information, this paper does a good job illustrating how P-Chase work. The takeaway is, we need to brute force an array with one element more than a cache can hold so that cache miss will start to occur periodically whereas such array with less or equal elements to the cache capacity will always result in cache hit and thus no access overhead will be introduced after all data is loaded into the cache. This is the algorithm the paper proposed and we will use it to do the experiment:\n__global__ void KernelFunction ( . . . ) { //declare shared memory space  __shared__ unsigned int s tvalue [ ] ; __shared__ unsigned int s index [ ] ; preheat the data ; // implementation varies  for (it = 0; it  iter ; it++) { start_time=clock(); j = my_array[j]; //store the array index  // This following line is essential because due to  // instruction-level parallelism (ILP), function clock() may  // overlap with its previous instruction and even return before  // the previous instruction finishes. For example,  // end_time=clock() can return before j = my_array[j] returns.  // adding s_index [it]= j since it have data dependency on the  // previous line. Thus the memory access will be over before  // end_time=clock() started.  s_index [it]= j; end_time=clock(); //store the access latency  s_tvalue[it]= end_time−start_time ; } } The steps is the same as the paper proposes, so here we show the paper’s method:\n  Determine cache size C . We set s to 1. We then initialize N with a small value and increase it gradually until the ﬁrst cache miss appears. C equals the maximum N where all memory accesses are cache hits.\n  Determine cache line size b. We set s to 1. We begin with N = C + 1 and increase N gradually again. When N  Determine number of cache sets T . We set s to b. We then start with N = C and increase N at the granularity of b. Every increment causes cache misses of a new cache set. When N  C + (T − 1)b, all cache sets are missed. We can then deduce T from cache miss patterns accordingly.\n  Determine cache replacement policy. As mentioned before, if the cache replacement policy is LRU, then the memory access process should be periodic and all the cache ways in the cache set are missed. If memory access process is aperiodic, then the replacement policy cannot be LRU. Under this circumstance, we set N = C + b, s = b with a considerable large k (k » N/s) so that we can traverse the array multiple times. All cache misses are from one cache set. Every cache miss is caused by its former cache replacement because we overﬂow the cache by only one cache line. We have the accessed data indices thus we can reproduce the full memory access process and ﬁnd how the cache lines are updated.\n  Texture L1 Cache and Read-only Data Cache When use the code with increased our own data preheat implementation because the texture L1 cache can potentially be greater than the shared memory. The original code uses the first iteration of the loop in the algorithm as a way to preheat data:\nconst int it = 6144 // texture L1 may hold more elements,  // So the first iteration may not cold  // hit all elements, some cold hits can  // be moved to the second iteration,  // causing confusion for (int cnt=0; cnt  it; cnt++) { start=clock(); j=tex1Dfetch(tex_ref, j); s_value[cnt] = j; end=clock();\ts_tvalue[cnt] = (end -start); } However, if texture L1 cache is greater than the shared memory allowed for each thread block, then some reads in the second loop will trigger cache misses. But such misses are in fact cold misses, not misses caused after the texture L1 cache is completely filled up. One solution is increase iteration to a much larger number so that the first iteration will always fill up the texture L1 cache. Note that if you move the data preheat out such as\nfor (int cnt=0; cnt  it; cnt++) { tmp=tex1Dfetch(tex_ref, tmp); } The compiler can optimize this whole step out and thus nothing actually gets executed.\nAfter we run the modified code, the result shows that the we the cache missed starts when we set our array size to 6145, indicating the texture L1 cache can hold 6144 ints, which is equivalent to 24 kb. We also notice that each miss is followed by 7 consecutive hits. This means the cache line size is 8 words(b = 32 bytes). The structure of the L1 TLB is shown below, notice there are 192 lines in each set:\n   Set1 Set2 Set3 Set4     1-8 33-40 65-72 97-104   9-16 41-48 … …   17-24 46-56 … …   25-32 57-64 89-96 121-128   129-136      … … … …   2969-2976 3001-3008 3033-3040 3065-3072    According to CUDA documentation, GK110 adds the ability for read-only data in global memory to be loaded through the same cache used by the texture pipeline via a standard pointer without the need to bind a texture beforehand and without the sizing limitations of standard textures. The read-only data cache is loaded by calling __ldg(const restricted * address). We modified the code used to test texture L1 cache. The basic logic remains the same. When the arrays size is set to 6144 integers no cache misses occur with stride set as 32 (s=32 bytes). As soon as we increased one more element in the array cache misses start occurring. This shows the read-only cache is 24kb. We then noticed that the misses occur in a group of either 4 or 8. We infer the cache line to be 32 bytes and the replacement policy is LRU, same as Maxwell. We we increase the array to include 6248 elements(6144+323+8, 6144 is the max capacity of the cache, 32 consecutive number in a set, 323 to cause cache miss in set1, set2, and set3, only need to include 8 more to cause cache miss in set4 since s=32bytes), no caches hits occur. Therefore, we infer the caches set number to be 4, each cache line is 32 bytes, and each set contains 192 cache lines, the same as the texture L1 cache. The memory mapping seems arbitrary because the hit and miss patterns didn’t follow that of the texture L1 cache.\n","wordCount":"2111","inLanguage":"en","datePublished":"2019-01-15T00:00:00Z","dateModified":"2019-01-15T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.bodunhu.com/blog/posts/gpumemoryhierarchy/"},"publisher":{"@type":"Organization","name":"std::bodun::blog","logo":{"@type":"ImageObject","url":"https://www.cs.utexas.edu/sites/default/files/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://www.bodunhu.com/blog/ accesskey=h title="std::bodun::blog (Alt + H)">std::bodun::blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.bodunhu.com/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://www.bodunhu.com/blog/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://www.bodunhu.com/blog/about/ title=About><span>About</span></a></li><li><a href=https://www.bodunhu.com/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.bodunhu.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://www.bodunhu.com/blog/posts/>Posts</a></div><h1 class=post-title>Pascal GPU memory and cache hierarchy</h1><div class=post-meta>January 15, 2019&nbsp;·&nbsp;10 min</div></header><div class=post-content><p>Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program&rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation. Turing architecture might have different results, but the method we used here can be applied as well with slight modification. The method we use in this guide is inspired by the research paper: <a href=https://ieeexplore.ieee.org/document/7445236>Dissecting GPU Memory Hierarchy through Microbenchmarking</a>. Here we will explain how P-Chase works and walk through a small example.</p><h2 id=memory-hierarchy-overview>Memory Hierarchy Overview<a hidden class=anchor aria-hidden=true href=#memory-hierarchy-overview>#</a></h2><p>GPU memory hierarchy is different compared to CPU memory hierarchy. Using the terminologies of CUDA, GPU memory space can be categorized in these groups: register, constant memory, shared memory, texture memory, local memory, and global memory. Each different memory space have its own properties. Since we are interested the
cache systems, here is a picture demonstrating the memory hierarchy of a NVIDIA GPU:</p><p align=center><img src=https://gistbok.ucgis.org/sites/default/files/1000px-Memory.svg_.png width=450></p><p align=center><a href=https://gistbok.ucgis.org/bok-topics/graphics-processing-units-gpus>Image source</a></p><p>The characteristics of each memory space can be found in <a href=https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA_C_Programming_Guide.pdf>NVIDIA CUDA C Programming Guide
</a>. Here we will focus on some target memory space we are interested in. The paper lists some properties of our target memory space:</p><table><thead><tr><th>Memory</th><th style=text-align:center>Type</th><th style=text-align:right>Cached</th><th style=text-align:right>Scope</th></tr></thead><tbody><tr><td>Global</td><td style=text-align:center>R/W</td><td style=text-align:right>Yes</td><td style=text-align:right>All Threads</td></tr><tr><td>Shared</td><td style=text-align:center>R/W</td><td style=text-align:right>N/A</td><td style=text-align:right>Thread Blocks</td></tr><tr><td>Texture</td><td style=text-align:center>R</td><td style=text-align:right>Yes</td><td style=text-align:right>All Threads</td></tr></tbody></table><p>Even though the paper targets Fermi, Kepler and Maxwell generations of GPU, the properties of the table still holds for Pascal GPU and possibly Turing as well. The cached global/texture memory uses a two-level caching system. The L1 cache is located in each stream multiprocessor (SM), while the L2 cache is off-chip and shared among all SMs. It is unified for instruction, data and page table access. According to CUDA documentation, like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. This function previously was served by the separate L1 cache in Fermi and Kepler. Page table is used by GPU to map virtual addresses to physical addresses, and is usually stored in the global memory. The page table is cached in TLB to reduce memory access latency. Once a thread cannot ﬁnd the page entry in the TLB, it would access the global memory to search in the page table, which introduced significant memory access latency. The GPU-specific shared memory is located in the SMs. On the Fermi and Kepler devices, it shares memory space with the L1 data cache. On Maxwell and Pascal devices, it has a dedicated space, since the functionality of the L1 and texture caches have been merged. One thing to note here is that shared memory is accessed by the thread blocks. Thread-blocks remain limited to 48 KB of shared memory in Pascal. Therefore, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block. This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM.</p><p>However, we should be careful that by default, GP100 caches global loads in the L1/Texture cache. In contrast, GP104 follows Kepler and Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler. As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to nvcc at compile time. Even though both GP100 and GP104 belongs to Pascal family, we only focus on GP100 here because that&rsquo;s the GPU we use. Another thing to notice is that unlike Maxwell but similar to Kepler, Pascal caches thread-local memory in the L1 cache. This can mitigate the cost of register spills compared to Maxwell. To illustrate our point, we checked both cudaDevAttrGlobalL1CacheSupported and cudaDevAttrLocalL1CacheSupported on Tesla P100 and GTX 1080 and find both attributes to be 1.</p><p>In addition to the L2 data cache, global memory data that is read-only for the entire lifetime of a kernel can be cached in the read-only data cache with a compute capability of 3.5 or above. We will also explore the size of this read-only cache using __ldg() intrinsic.</p><h2 id=p-chase>P-Chase<a hidden class=anchor aria-hidden=true href=#p-chase>#</a></h2><p>Most existing GPU microbenchmark studies on cache architecture assume a classical set-associative cache model with the least recently used (LRU) replacement policy, the same as the conventional CPU cache. So here we will use this assumption and proceed with our experiments. Here are some notations we will use throughout this post.</p><table><thead><tr><th>Notation</th><th style=text-align:center>Description</th><th style=text-align:right>Notation</th><th style=text-align:right>Description</th></tr></thead><tbody><tr><td>C</td><td style=text-align:center>Cache Size</td><td style=text-align:right>N</td><td style=text-align:right>array size</td></tr><tr><td>b</td><td style=text-align:center>cache line size</td><td style=text-align:right>s</td><td style=text-align:right>stride size</td></tr><tr><td>a</td><td style=text-align:center>cache associativity</td><td style=text-align:right>k</td><td style=text-align:right>iterations</td></tr><tr><td>T</td><td style=text-align:center>number of cache set</td><td style=text-align:right>r</td><td style=text-align:right>cache miss rate</td></tr></tbody></table><p>Under our assumptions, data is loaded from main memory to lower cache in the basic unit of a cache line. The number of words in a cache line is referred to as the line size (b). For the LRU set-associative cache, the cache memory is divided into T cache sets, each of which consists of \(a\) cache lines. It is essential to have these three assumptions using this kind of cache model:</p><ul><li><p><strong>Assumption 1</strong> All cache sets have the same size. The cache parameter should satisfy \(T \cdot a \cdot b = C\).</p></li><li><p><strong>Assumption 2</strong> In the memory address, the bits representing the cache set are immediately followed by the bits representing the offset.</p></li><li><p><strong>Assumption 3</strong> Cache replacement policy should be LRU.</p></li></ul><p>We will later see why these assumptions are essential as we proceed with the experiment. We won&rsquo;t go through how P-Chase work exactly. To find more information, this <a href=https://arxiv.org/pdf/1509.02308.pdf>paper</a> does a good job illustrating how P-Chase work. The takeaway is, we need to brute force an array with one element more than a cache can hold so that cache miss will start to occur periodically whereas such array with less or equal elements to the cache capacity will always result in cache hit and thus no access overhead will be introduced after all data is loaded into the cache. This is the algorithm the paper proposed and we will use it to do the experiment:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>__global__ <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>KernelFunction</span> ( . . . ) {
    <span style=color:#75715e>//declare shared memory space
</span><span style=color:#75715e></span>    __shared__ <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> s tvalue [ ] ;
    __shared__ <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> s index [ ] ;
    preheat the data ; <span style=color:#75715e>// implementation varies
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (it <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; it <span style=color:#f92672>&lt;</span> iter ; it<span style=color:#f92672>++</span>) {
        start_time<span style=color:#f92672>=</span>clock();
        j <span style=color:#f92672>=</span> my_array[j];
        <span style=color:#75715e>//store the array index
</span><span style=color:#75715e></span>
        <span style=color:#75715e>// This following line is essential because due to
</span><span style=color:#75715e></span>        <span style=color:#75715e>// instruction-level parallelism (ILP), function clock() may
</span><span style=color:#75715e></span>        <span style=color:#75715e>// overlap with its previous instruction and even return before
</span><span style=color:#75715e></span>        <span style=color:#75715e>// the previous instruction finishes. For example,
</span><span style=color:#75715e></span>        <span style=color:#75715e>// end_time=clock() can return before j = my_array[j] returns.
</span><span style=color:#75715e></span>        <span style=color:#75715e>// adding s_index [it]= j since it have data dependency on the
</span><span style=color:#75715e></span>        <span style=color:#75715e>// previous line. Thus the memory access will be over before
</span><span style=color:#75715e></span>        <span style=color:#75715e>// end_time=clock() started.
</span><span style=color:#75715e></span>        s_index [it]<span style=color:#f92672>=</span> j;
        end_time<span style=color:#f92672>=</span>clock();
        <span style=color:#75715e>//store the access latency
</span><span style=color:#75715e></span>        s_tvalue[it]<span style=color:#f92672>=</span> end_time<span style=color:#960050;background-color:#1e0010>−</span>start_time ;
    }
}
</code></pre></div><p>The steps is the same as the paper proposes, so here we show the paper&rsquo;s method:</p><ol><li><p>Determine cache size C . We set s to 1. We then initialize N with a small value and increase it gradually until the ﬁrst cache miss appears. C equals the maximum N where all memory accesses are cache hits.</p></li><li><p>Determine cache line size b. We set s to 1. We begin with N = C + 1 and increase N gradually again. When N &lt; C + b + 1, the numbers of cache misses are close. When N is increased to C + b + 1, there is a sudden increase on the number of cache misses, despite that we only increase N by 1. Accordingly we can ﬁnd b. Based on the memory access patterns, we can also have a general idea on the cache replacement policy.</p></li><li><p>Determine number of cache sets T . We set s to b. We then start with N = C and increase N at the granularity of b. Every increment causes cache misses of a new cache set. When N > C + (T − 1)b, all cache sets are missed. We can then deduce T from cache miss patterns accordingly.</p></li><li><p>Determine cache replacement policy. As mentioned before, if the cache replacement policy is LRU, then the memory access process should be periodic and all the cache ways in the cache set are missed. If memory access process is aperiodic, then the replacement policy cannot be LRU. Under this circumstance, we set N = C + b, s = b with a considerable large k (k &#187; N/s) so that we can traverse the array multiple times. All cache misses are from one cache set. Every cache miss is caused by its former cache replacement because we overﬂow the cache by only one cache line. We have the accessed data indices thus we can reproduce the full memory access process and ﬁnd how the cache lines are updated.</p></li></ol><h2 id=texture-l1-cache-and-read-only-data-cache>Texture L1 Cache and Read-only Data Cache<a hidden class=anchor aria-hidden=true href=#texture-l1-cache-and-read-only-data-cache>#</a></h2><p>When use the <a href=http://www.comp.hkbu.edu.hk/~chxw/Code/fine_grain_Maxwell_texture_L1.cu>code</a> with increased our own data preheat implementation because the texture L1 cache can potentially be greater than the shared memory. The original code uses the first iteration of the loop in the algorithm as a way to preheat data:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>const</span> <span style=color:#66d9ef>int</span> it <span style=color:#f92672>=</span>  <span style=color:#ae81ff>6144</span> <span style=color:#75715e>// texture L1 may hold more elements,
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// So the first iteration may not cold
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// hit all elements, some cold hits can
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// be moved to the second iteration,
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// causing confusion
</span><span style=color:#75715e></span><span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> cnt<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>; cnt <span style=color:#f92672>&lt;</span> it; cnt<span style=color:#f92672>++</span>) {
	start<span style=color:#f92672>=</span>clock();
        j<span style=color:#f92672>=</span>tex1Dfetch(tex_ref, j);
	s_value[cnt] <span style=color:#f92672>=</span> j;
			
	end<span style=color:#f92672>=</span>clock();			
	s_tvalue[cnt] <span style=color:#f92672>=</span> (end <span style=color:#f92672>-</span>start);
}
</code></pre></div><p>However, if texture L1 cache is greater than the shared memory allowed for each thread block, then some reads in the second loop will trigger cache misses. But such misses are in fact cold misses, not misses caused after the texture L1 cache is completely filled up. One solution is increase iteration to a much larger number so that the first iteration will always fill up the texture L1 cache. Note that if you move the data
preheat out such as</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> cnt<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>; cnt <span style=color:#f92672>&lt;</span> it; cnt<span style=color:#f92672>++</span>) {
        tmp<span style=color:#f92672>=</span>tex1Dfetch(tex_ref, tmp);
}
</code></pre></div><p>The compiler can optimize this whole step out and thus nothing actually gets executed.</p><p>After we run the modified code, the result shows that the we the cache missed starts when we set our array size to 6145, indicating the texture L1 cache can hold 6144 ints, which is equivalent to 24 kb. We also notice that each miss is followed by 7 consecutive hits. This means the cache line size is 8 words(b = 32 bytes). The structure of the L1 TLB is shown below, notice there are 192 lines in each set:</p><table><thead><tr><th>Set1</th><th style=text-align:center>Set2</th><th style=text-align:right>Set3</th><th style=text-align:right>Set4</th></tr></thead><tbody><tr><td>1-8</td><td style=text-align:center>33-40</td><td style=text-align:right>65-72</td><td style=text-align:right>97-104</td></tr><tr><td>9-16</td><td style=text-align:center>41-48</td><td style=text-align:right>&mldr;</td><td style=text-align:right>&mldr;</td></tr><tr><td>17-24</td><td style=text-align:center>46-56</td><td style=text-align:right>&mldr;</td><td style=text-align:right>&mldr;</td></tr><tr><td>25-32</td><td style=text-align:center>57-64</td><td style=text-align:right>89-96</td><td style=text-align:right>121-128</td></tr><tr><td>129-136</td><td></td><td></td><td></td></tr><tr><td>&mldr;</td><td style=text-align:center>&mldr;</td><td style=text-align:right>&mldr;</td><td style=text-align:right>&mldr;</td></tr><tr><td>2969-2976</td><td style=text-align:center>3001-3008</td><td style=text-align:right>3033-3040</td><td style=text-align:right>3065-3072</td></tr></tbody></table><p>According to CUDA documentation, GK110 adds the ability for read-only data in global memory to be loaded through the same cache used by the texture pipeline via a standard pointer without the need to bind a texture beforehand and without the sizing limitations of standard textures. The read-only data cache is loaded by calling __ldg(const restricted * address). We modified the code used to test texture L1 cache. The basic logic remains the same. When the arrays size is set to 6144 integers no cache misses occur with stride set as 32 (s=32 bytes). As soon as we increased one more element in the array cache misses start occurring. This shows the read-only cache is 24kb. We then noticed that the misses occur in a group of either 4 or 8. We infer the cache line to be 32 bytes and the replacement policy is LRU, same as Maxwell. We we increase the array to include 6248 elements(6144+32<em>3+8, 6144 is the max capacity of the cache, 32 consecutive number in a set, 32</em>3 to cause cache miss in set1, set2, and set3, only need to include 8 more to cause cache miss in set4 since s=32bytes), no caches hits occur. Therefore, we infer the caches set number to be 4, each cache line is 32 bytes, and each set contains 192 cache lines, the same as the texture L1 cache. The memory mapping seems arbitrary because the hit and miss patterns didn&rsquo;t follow that of the texture L1 cache.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.bodunhu.com/blog/tags/gpu/>gpu</a></li><li><a href=https://www.bodunhu.com/blog/tags/os/>os</a></li><li><a href=https://www.bodunhu.com/blog/tags/mm/>mm</a></li></ul><nav class=paginav><a class=prev href=https://www.bodunhu.com/blog/posts/operatingsystemmemorymanagement/><span class=title>« Prev Page</span><br><span>A Little Review on Barrelfish Memory Management</span></a>
<a class=next href=https://www.bodunhu.com/blog/posts/mapreduce/><span class=title>Next Page »</span><br><span>Map Reduce</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Pascal GPU memory and cache hierarchy on twitter" href="https://twitter.com/intent/tweet/?text=Pascal%20GPU%20memory%20and%20cache%20hierarchy&url=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2fgpumemoryhierarchy%2f&hashtags=gpu%2cos%2cmm"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Pascal GPU memory and cache hierarchy on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2fgpumemoryhierarchy%2f&title=Pascal%20GPU%20memory%20and%20cache%20hierarchy&summary=Pascal%20GPU%20memory%20and%20cache%20hierarchy&source=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2fgpumemoryhierarchy%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Pascal GPU memory and cache hierarchy on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2fgpumemoryhierarchy%2f&title=Pascal%20GPU%20memory%20and%20cache%20hierarchy"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Pascal GPU memory and cache hierarchy on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2fgpumemoryhierarchy%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Pascal GPU memory and cache hierarchy on whatsapp" href="https://api.whatsapp.com/send?text=Pascal%20GPU%20memory%20and%20cache%20hierarchy%20-%20https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2fgpumemoryhierarchy%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Pascal GPU memory and cache hierarchy on telegram" href="https://telegram.me/share/url?text=Pascal%20GPU%20memory%20and%20cache%20hierarchy&url=https%3a%2f%2fwww.bodunhu.com%2fblog%2fposts%2fgpumemoryhierarchy%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=BDHU/blog issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2021 <a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>