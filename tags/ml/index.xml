<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ml on std::bodun::blog</title><link>https://www.bodunhu.com/blog/tags/ml/</link><description>Recent content in ml on std::bodun::blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 13 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.bodunhu.com/blog/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Cross Entropy Loss</title><link>https://www.bodunhu.com/blog/posts/cross-entropy-loss/</link><pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/cross-entropy-loss/</guid><description>Many deep learning tasks involve classification, where a model outputs a series of probabilities for their corresponding labels. The goal is to correctly predict a given input&amp;rsquo;s label. Mathematically, it means generating max probabilities for the correct label. The probabilities are generated through a process called softmax.
The softmax function outputs a vector \(\hat{y}\), which represents estimated conditional probabilities of each class given an input \(x\), For example, \(\hat{y}_1 = P(y=\textrm{car}\ |\ x)\).</description></item><item><title>Maximum Likelihood for Classification</title><link>https://www.bodunhu.com/blog/posts/maximum-likelihood-for-classification/</link><pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/maximum-likelihood-for-classification/</guid><description>Let&amp;rsquo;s say we want to classify an input text \(y\) and give it a label \(x\). Formally, we want to find:
\[ \textrm{argmax} P(x | y) \]
By Bayes&amp;rsquo; rule this is the same as
\[ \textrm{argmax} \frac{P(y|x)P(y)}{P(x)} \]
Suppose we have five documents as training data and one document as the input as testing data. Our objective is to give a label to the test sentence.
Credit: Eunsol Choi Let&amp;rsquo;s define the probability of class as (\(N\) is the total number of classes)</description></item><item><title>Megatron with FastMoE</title><link>https://www.bodunhu.com/blog/posts/megatron-with-fastmoe/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/megatron-with-fastmoe/</guid><description>This is a guide on setting up Megatron-LM with FastMoE. Megatron is a transformer developed by the Applied Deep Learning Research team at NVIDIA. FastMoE enables PyTorch support for the Mixture of Experts (MoE) models. We use the FastMoE layer to replace the MLP layers in the transformer language model.
Prerequisites Docker We recommend using one of NGC&amp;rsquo;s recent PyTorch containers. The Megatron-LM repo uses pytorch:20.12-py3. We pull the image with:</description></item><item><title>Paper Review - Dynamic Tensor Rematerialization</title><link>https://www.bodunhu.com/blog/posts/paper-review-dynamic-tensor-rematerialization/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/paper-review-dynamic-tensor-rematerialization/</guid><description>Dynamic Tensor Rematerialization (DTR) treats GPU memory as a large cache, where tensors can be evicted to save memory, and recomputed if needed later.
DTR&amp;rsquo;s eviction policy relies on the heuristic \(h\). The heuristic assigns a value \(h(t)\) to each resident tensor \(t\), approximating the cost of evicting the tensor. DTR evicts the tensor with the lowest cost based on the value of \(h\). \(h\) can factor in arbitrary metadata.</description></item><item><title>Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning</title><link>https://www.bodunhu.com/blog/posts/paper-review-capuchin-tensor-based-gpu-memory-management-for-deep-learning/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/paper-review-capuchin-tensor-based-gpu-memory-management-for-deep-learning/</guid><description>This paper aims to reduce GPU memory usage during DNN training. Capuchin achieves this goal though swapping and recomputation, using tensor as unit of operation. The major question is how to balance between swapping and recomputation to achieve max resource utilization.
Swap and Recomputation Benefit The ultimate goal of swapping and recomputation is to hide the overhead as much as possible to minimize the wait time of back-access (a tensor evicted earlier being accessed again).</description></item></channel></rss>