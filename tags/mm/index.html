<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>mm | std::bodun::blog</title><meta name=keywords content><meta name=description content="Personal Blog for Bodun Hu. Longhorn at UT Austin"><meta name=author content><link rel=canonical href=https://www.bodunhu.com/blog/tags/mm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.e5f3c4ceaebb5646fc1ba414004c06d9e00115f24843fefe9f9cd9552d00dad7.css integrity="sha256-5fPEzq67Vkb8G6QUAEwG2eABFfJIQ/7+n5zZVS0A2tc=" rel="preload stylesheet" as=style><link rel=icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.utexas.edu/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.utexas.edu/favicon-32x32.png><link rel=apple-touch-icon href=https://www.utexas.edu/apple-touch-icon.png><link rel=mask-icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><link rel=alternate type=application/rss+xml href=https://www.bodunhu.com/blog/tags/mm/index.xml><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-108144808-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="mm"><meta property="og:description" content="Personal Blog for Bodun Hu. Longhorn at UT Austin"><meta property="og:type" content="website"><meta property="og:url" content="https://www.bodunhu.com/blog/tags/mm/"><meta property="og:image" content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Bodun's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="mm"><meta name=twitter:description content="Personal Blog for Bodun Hu. Longhorn at UT Austin"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://www.bodunhu.com/blog/ accesskey=h title="std::bodun::blog (Alt + H)">std::bodun::blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.bodunhu.com/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://www.bodunhu.com/blog/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://www.bodunhu.com/blog/about/ title=About><span>About</span></a></li><li><a href=https://www.bodunhu.com/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://www.bodunhu.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://www.bodunhu.com/blog/tags/>Tags</a></div><h1>mm</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/programloadingandmemorymappinginlinux/>Program Loading and Memory Mapping in Linux</a></h2></header><footer class=entry-footer>November 3, 2020&nbsp;·&nbsp;5 min</footer><section class=entry-content><p><p>The goal here is to familiarize yourself with how programs are loaded, dynamically paged, and some of the mechanics of signal handling and memory mapping in Linux.</p><h2 id=execve-syscall>execve Syscall</h2><p>The operating system, as one of itsd basic services, loads programs into memory for them to execute. Programs rely on <code>execve</code> syscall to get the OS to load the program into memory and start it executing as a process. The kernel version we used to testing is 5.4.0. Doing a quick search inside <a href=https://elixir.bootlin.com/linux/v5.4/source/fs/exec.c#L1956>Elixir</a> gives us:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>SYSCALL_DEFINE3(execve,
		<span style=color:#66d9ef>const</span> <span style=color:#66d9ef>char</span> __user <span style=color:#f92672>*</span>, filename,
		<span style=color:#66d9ef>const</span> <span style=color:#66d9ef>char</span> __user <span style=color:#f92672>*</span><span style=color:#66d9ef>const</span> __user <span style=color:#f92672>*</span>, argv,
		<span style=color:#66d9ef>const</span> <span style=color:#66d9ef>char</span> __user <span style=color:#f92672>*</span><span style=color:#66d9ef>const</span> __user <span style=color:#f92672>*</span>, envp)
{
	<span style=color:#66d9ef>return</span> do_execve(getname(filename), argv, envp);
}
</code></pre></div><p>Follow the function call, we will eventually reach the call to <code>__do_execve_file</code>, the comment of this function says &ldquo;sys_execve() executes a new program&rdquo;, which is pretty straighforward. This function first checks the <code>filename</code> pointer. Then it checks the flags of the current process that limit of running processes is not exceeded:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>if</span> (IS_ERR(filename))
		<span style=color:#66d9ef>return</span> PTR_ERR(filename);

<span style=color:#75715e>/*
</span><span style=color:#75715e> * We move the actual failure in case of RLIMIT_NPROC excess from
</span><span style=color:#75715e> * set*uid() to execve() because too many poorly written programs
</span><span style=color:#75715e> * don&#39;t check setuid() return code.  Here we additionally recheck
</span><span style=color:#75715e> * whether NPROC limit is still exceeded.
</span><span style=color:#75715e> */</span>
<span style=color:#66d9ef>if</span> ((current<span style=color:#f92672>-&gt;</span>flags <span style=color:#f92672>&amp;</span> PF_NPROC_EXCEEDED) <span style=color:#f92672>&amp;&amp;</span>
    atomic_read(<span style=color:#f92672>&amp;</span>current_user()<span style=color:#f92672>-&gt;</span>processes) <span style=color:#f92672>&gt;</span> rlimit(RLIMIT_NPROC)) {
    retval <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>EAGAIN;
    <span style=color:#66d9ef>goto</span> out_ret;
}

<span style=color:#75715e>/* We&#39;re below the limit (still or again), so we don&#39;t want to make
</span><span style=color:#75715e>    * further execve() calls fail. */</span>
current<span style=color:#f92672>-&gt;</span>flags <span style=color:#f92672>&amp;=</span> <span style=color:#f92672>~</span>PF_NPROC_EXCEEDED;
</code></pre></div><p>The next important task is to allocate the <code>struct linux_binprm</code> structure defined <a href=https://elixir.bootlin.com/linux/v5.4/source/include/linux/binfmts.h#L17>here</a>. This structure is used to hold the arguments that are used when loading binaries.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>bprm <span style=color:#f92672>=</span> kzalloc(<span style=color:#66d9ef>sizeof</span>(<span style=color:#f92672>*</span>bprm), GFP_KERNEL);
	<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>bprm)
		<span style=color:#66d9ef>goto</span> out_files;
</code></pre></div><p>Next, the function performs a seireis of tasks to prepare the <code>bprm</code> struct. Refer to the <a href=https://0xax.gitbooks.io/linux-insides/content/SysCall/linux-syscall-4.html>linux-insides</a> book to find more information on how exactly the <code>bprm</code> structure is filled up.</p><p>The most important function called by <code>__do_execve_file</code> is <code>search_binary_handler</code>. Based on the <a href=https://elixir.bootlin.com/linux/v5.4/source/fs/exec.c>comment</a>, this function cycles the list of binary formats handler, until one recognizes the image. We can find one section of the code surrounded by <code>binfmt_lock</code>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>list_for_each_entry(fmt, <span style=color:#f92672>&amp;</span>formats, lh) {
    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>try_module_get(fmt<span style=color:#f92672>-&gt;</span>module))
        <span style=color:#66d9ef>continue</span>;
    read_unlock(<span style=color:#f92672>&amp;</span>binfmt_lock);

    bprm<span style=color:#f92672>-&gt;</span>recursion_depth<span style=color:#f92672>++</span>;
    retval <span style=color:#f92672>=</span> fmt<span style=color:#f92672>-&gt;</span>load_binary(bprm);
    bprm<span style=color:#f92672>-&gt;</span>recursion_depth<span style=color:#f92672>--</span>;

    read_lock(<span style=color:#f92672>&amp;</span>binfmt_lock);
    put_binfmt(fmt);
    <span style=color:#66d9ef>if</span> (retval <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>&amp;&amp;</span> <span style=color:#f92672>!</span>bprm<span style=color:#f92672>-&gt;</span>mm) {
        <span style=color:#75715e>/* we got to flush_old_exec() and failed after it */</span>
        read_unlock(<span style=color:#f92672>&amp;</span>binfmt_lock);
        force_sigsegv(SIGSEGV);
        <span style=color:#66d9ef>return</span> retval;
    }
    <span style=color:#66d9ef>if</span> (retval <span style=color:#f92672>!=</span> <span style=color:#f92672>-</span>ENOEXEC <span style=color:#f92672>||</span> <span style=color:#f92672>!</span>bprm<span style=color:#f92672>-&gt;</span>file) {
        read_unlock(<span style=color:#f92672>&amp;</span>binfmt_lock);
        <span style=color:#66d9ef>return</span> retval;
    }
}
</code></pre></div><p>We can see it calls into <code>load_binary</code>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>retval <span style=color:#f92672>=</span> fmt<span style=color:#f92672>-&gt;</span>load_binary(bprm);
</code></pre></div><p>Here, the <code>load_binary</code> is a pointer in a <code>linux_binfmt</code> struct. For elf format, it can be found <a href=https://elixir.bootlin.com/linux/v5.4/source/fs/binfmt_elf.c#L94>here</a>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>struct</span> linux_binfmt elf_format <span style=color:#f92672>=</span> {
	.module		<span style=color:#f92672>=</span> THIS_MODULE,
	.load_binary	<span style=color:#f92672>=</span> load_elf_binary,
	.load_shlib	<span style=color:#f92672>=</span> load_elf_library,
	.core_dump	<span style=color:#f92672>=</span> elf_core_dump,
	.min_coredump	<span style=color:#f92672>=</span> ELF_EXEC_PAGESIZE,
};
</code></pre></div><p>We can find the <code>load_elf_binary</code> function defined in the <a href=https://elixir.bootlin.com/linux/v5.4/source/fs/binfmt_elf.c#L673><code>fs/binfmt_elf.c</code></a> file. Then the function will check the magic number in the ELF file header. You can find the ELF format from <a href=https://en.wikipedia.org/wiki/Executable_and_Linkable_Format>wiki</a>.
We can see for both 32-bit and 64-bit systems, the e-ident field should contain the magic number for ELF format files.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>/* Get the exec-header */</span>
loc<span style=color:#f92672>-&gt;</span>elf_ex <span style=color:#f92672>=</span> <span style=color:#f92672>*</span>((<span style=color:#66d9ef>struct</span> elfhdr <span style=color:#f92672>*</span>)bprm<span style=color:#f92672>-&gt;</span>buf);

retval <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>ENOEXEC;
<span style=color:#75715e>/* First of all, some simple consistency checks */</span>
<span style=color:#66d9ef>if</span> (memcmp(loc<span style=color:#f92672>-&gt;</span>elf_ex.e_ident, ELFMAG, SELFMAG) <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>)
    <span style=color:#66d9ef>goto</span> out;
</code></pre></div><p>Then, <code>load_elf_binary</code> will do some tasks to prepare for the executable file. After that, it will try to load the program header table:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>elf_phdata <span style=color:#f92672>=</span> load_elf_phdrs(<span style=color:#f92672>&amp;</span>loc<span style=color:#f92672>-&gt;</span>elf_ex, bprm<span style=color:#f92672>-&gt;</span>file);
<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>elf_phdata)
    <span style=color:#66d9ef>goto</span> out;
</code></pre></div><p>Then it will traverse the program header table and find the interpreter which is responsible of setting up the stack and map elf binary into the correct location in memory. After the interpreter is obtained, the function will perform simple consistency checks on the interpreter. It will load the interpreter program headers:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>/* Load the interpreter program headers */</span>
interp_elf_phdata <span style=color:#f92672>=</span> load_elf_phdrs(<span style=color:#f92672>&amp;</span>loc<span style=color:#f92672>-&gt;</span>interp_elf_ex,
                    interpreter);
<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>interp_elf_phdata)
    <span style=color:#66d9ef>goto</span> out_free_dentry;
</code></pre></div><p>This function will call <code>setup_arg_pages</code> to finalize the stack vm_area_struct:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>/* Do this so that we can load the interpreter, if need be.  We will
</span><span style=color:#75715e>    change some of these later */</span>
retval <span style=color:#f92672>=</span> setup_arg_pages(bprm, randomize_stack_top(STACK_TOP),
                executable_stack);
<span style=color:#66d9ef>if</span> (retval <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>)
    <span style=color:#66d9ef>goto</span> out_free_dentry;
</code></pre></div><p>It will also mmap the elf image into the correct location in memory. The bss and brk sections are prepared for the executable file:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>/* Now we do a little grungy work by mmapping the ELF image into
</span><span style=color:#75715e>    the correct location in memory. */</span>
<span style=color:#66d9ef>for</span>(i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, elf_ppnt <span style=color:#f92672>=</span> elf_phdata;
    i <span style=color:#f92672>&lt;</span> loc<span style=color:#f92672>-&gt;</span>elf_ex.e_phnum; i<span style=color:#f92672>++</span>, elf_ppnt<span style=color:#f92672>++</span>) {
        
        ...

        <span style=color:#75715e>/* There was a PT_LOAD segment with p_memsz &gt; p_filesz
</span><span style=color:#75715e>           before this one. Map anonymous pages, if needed,
</span><span style=color:#75715e>           and clear the area.  */</span>
        retval <span style=color:#f92672>=</span> set_brk(elf_bss <span style=color:#f92672>+</span> load_bias,
                    elf_brk <span style=color:#f92672>+</span> load_bias,
                    bss_prot);
        <span style=color:#66d9ef>if</span> (retval)
            <span style=color:#66d9ef>goto</span> out_free_dentry;
        nbyte <span style=color:#f92672>=</span> ELF_PAGEOFFSET(elf_bss);
        <span style=color:#66d9ef>if</span> (nbyte) {
            nbyte <span style=color:#f92672>=</span> ELF_MIN_ALIGN <span style=color:#f92672>-</span> nbyte;
            <span style=color:#66d9ef>if</span> (nbyte <span style=color:#f92672>&gt;</span> elf_brk <span style=color:#f92672>-</span> elf_bss)
                nbyte <span style=color:#f92672>=</span> elf_brk <span style=color:#f92672>-</span> elf_bss;
            <span style=color:#66d9ef>if</span> (clear_user((<span style=color:#66d9ef>void</span> __user <span style=color:#f92672>*</span>)elf_bss <span style=color:#f92672>+</span>
                        load_bias, nbyte)) {
            }
</code></pre></div><p>It will also call <code>elf_map</code> to map the segment to [vaddr, vaddr + file size] and align and then perform some checks:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>error <span style=color:#f92672>=</span> elf_map(bprm<span style=color:#f92672>-&gt;</span>file, load_bias <span style=color:#f92672>+</span> vaddr, elf_ppnt,
				elf_prot, elf_flags, total_size);
</code></pre></div><p>The interpreter is then loaded:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>elf_entry <span style=color:#f92672>=</span> load_elf_interp(<span style=color:#f92672>&amp;</span>loc<span style=color:#f92672>-&gt;</span>interp_elf_ex,
                interpreter,
                <span style=color:#f92672>&amp;</span>interp_map_addr,
                load_bias, interp_elf_phdata);
</code></pre></div><p>Finally, the elf talbe is created:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>retval <span style=color:#f92672>=</span> create_elf_tables(bprm, <span style=color:#f92672>&amp;</span>loc<span style=color:#f92672>-&gt;</span>elf_ex,
            load_addr, interp_load_addr);
</code></pre></div><p>After everything is prepared, we can call the <code>start_thread</code> function, which prepares the new task&rsquo;s registers and segments for execution. We will pass the set of registers for the new task, the address of the entry point of the new task, and the address of the top of of the statck for the new task to this function.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>start_thread(regs, elf_entry, bprm<span style=color:#f92672>-&gt;</span>p);
</code></pre></div><p>A lot of the information here can also be found at the <a href=https://0xax.gitbooks.io/linux-insides/content/SysCall/linux-syscall-4.html>linux-insides</a> book. I found it very helpful clearing my confusion.</p><p>In our own implementations, we will not call the loaded program&rsquo;s <code>main</code> function. Instead, our loader will transfer control to the entry point of the loaded program via the <code>jmp</code> instruction. It has two major differences:</p><ul><li>Jumping to the entry point indicates we are going to execute the glibc start up functions before main is called. This includes setting up thread local storge. <code>main</code> simply jump to the main with the loader&rsquo;s TLS, no other setups are involved.</li><li><code>jmp</code> doesn&rsquo;t push return address on stack. When the loaded prgoram finishes execution, it exits the loader program, instead of giving cntrol back to the caller.</li></ul></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/linuxkernelmeasurementandmmap/>Linux Program Measurement and mmap</a></h2></header><footer class=entry-footer>September 23, 2020&nbsp;·&nbsp;23 min</footer><section class=entry-content><p><p>This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to <a href=http://www.cpu-world.com/CPUs/Core_i7/Intel-Core%20i7%20i7-6800K.html>cpu world</a>. This is the system spec:</p><table><thead><tr><th>Attribute</th><th style=text-align:center>Value</th></tr></thead><tbody><tr><td>Processor name (BIOS)</td><td style=text-align:center>Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz</td></tr><tr><td>Cores</td><td style=text-align:center>6</td></tr><tr><td>Logical processors</td><td style=text-align:center>12</td></tr><tr><td>TLB/Cache details</td><td style=text-align:center>64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries. Plus, 1-GB pages, 4-way, 16 entries</td></tr><tr><td>RAM</td><td style=text-align:center>32GB</td></tr><tr><td>Operating System</td><td style=text-align:center>Ubuntu 20.04.1 LTS</td></tr><tr><td>Kernel Version</td><td style=text-align:center>5.4.0-47-generic</td></tr></tbody></table><p><br></p><blockquote><p>8-way set associative means the CPU cache is made up of sets that can fit 8 blocks each.</p></blockquote><p>Here are the details for the CPU cache, which we will need later:</p><table><thead><tr><th>Cache</th><th>L1 data</th><th>L1 instruction</th><th>L2</th><th>L3</th></tr></thead><tbody><tr><td>Size</td><td>6 x 32 KB</td><td>6 x 32 KB</td><td>6 x 256 KB</td><td>15 MB</td></tr><tr><td>Associativity</td><td>8-way set associative</td><td>8-way set associative</td><td>8-way set associative</td><td>20-way set associative</td></tr><tr><td>Line size:</td><td>64 bytes</td><td>64 bytes</td><td>64 bytes</td><td>64 bytes</td></tr><tr><td>Comments:</td><td>Direct-mapped</td><td>Direct-mapped</td><td>Non-inclusive Direct-mapped</td><td>Inclusive Shared between all cores</td></tr></tbody></table><p><br></p><h2 id=memory-map>Memory Map</h2><p>To print the <code>/proc/self/maps</code> file for a process, we use the <code>sprintf</code> to construct the file name and then use the <code>system</code> from stdlib to cat the contents of the running process&rsquo;s address space. If we execute the program, it shows (also available on <a href=https://gist.github.com/BDHU/9ad2f0b6353b789cfb7c29c804a6088a#file-proc_mem_map>gist</a>)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>address                   perms offset  dev   inode                      pathname
559e3e51f000-559e3e520000 r--p 00000000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e520000-559e3e521000 r-xp 00001000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e521000-559e3e522000 r--p 00002000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e522000-559e3e523000 r--p 00002000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e523000-559e3e524000 rw-p 00003000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
7faf5c477000-7faf5c49c000 r--p 00000000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c49c000-7faf5c614000 r-xp 00025000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c614000-7faf5c65e000 r--p 0019d000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c65e000-7faf5c65f000 ---p 001e7000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c65f000-7faf5c662000 r--p 001e7000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c662000-7faf5c665000 rw-p 001ea000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c665000-7faf5c66b000 rw-p 00000000 00:00 0 
7faf5c685000-7faf5c686000 r--p 00000000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c686000-7faf5c6a9000 r-xp 00001000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6a9000-7faf5c6b1000 r--p 00024000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6b2000-7faf5c6b3000 r--p 0002c000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6b3000-7faf5c6b4000 rw-p 0002d000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6b4000-7faf5c6b5000 rw-p 00000000 00:00 0 
7ffcddb8d000-7ffcddbae000 rw-p 00000000 00:00 0                          [stack]
7ffcddbe0000-7ffcddbe3000 r--p 00000000 00:00 0                          [vvar]
7ffcddbe3000-7ffcddbe4000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
</code></pre></div><p>Based on the <a href=https://man7.org/linux/man-pages/man5/proc.5.html>linux man page</a>, we can see each column has different definition. The <em>address</em> field is the address space in the process that the mapping occupies. The <em>perms</em> field is a set of permissions:</p><ul><li>r = read</li><li>w = write</li><li>x = execute</li><li>s = shared</li><li>p = private (copy on write)</li></ul><p>The <em>offset</em> field is the offset into the file/whatever; <em>dev</em> is the device (major:minor); <em>inode</em> is the inode on that device. 0 indicates that no inode is associated with the memory region, as would be the case with BSS (uninitialized data).</p><p>The <em>pathname</em> field will usually be the file that is backing the mapping. For ELF files, you can easily coordinate with the <em>offset</em> field by looking at the Offset field in the ELF program headers (readelf -l). In addition, we can see a few other pseudo-paths:</p><ul><li><p><em>[stack]</em>: the initial process&rsquo;s (also known as the main thread&rsquo;s) stack.</p></li><li><p><em>[vdso]</em>: The virtual dynamically linked shared object. More detailed descriptions can be found on <a href=https://lwn.net/Articles/615809/>lwn</a>.</p></li><li><p><em>[vvar]</em>: location of kernel space variables mapped in user space needed by virtual system calls. Essentially, a kernel-space physical address is mapped into the userspace.</p></li><li><p><em>[vsyscall]</em>: similar to vDSO, vsyscall is another segment used to accelerate certain system calls in Linux. Vsyscall has some limitations; among other things, there is only space for a handful of virtual system calls. More detailed descriptions can be found on <a href=https://lwn.net/Articles/446528/>lwn</a>.</p></li></ul><p>One thing interesting here is that when we execute the same program twice, we can see after the first run, the output is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>7fffbc92f000-7fffbc930000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
</code></pre></div><p>Type the same command again:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>7ffd6a94d000-7ffd6a94e000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
</code></pre></div><p>Note that the vDSO area has moved, while the vsyscall page remains at the same location. The location of the vsyscall page is nailed down in the kernel ABI, but the vDSO area - like most other areas in the user-space memory layout - has its location randomized every time it is mapped. The vsyscall is legacy implementation of user-space sys call acceleration. Since it has fixed addresses, it is vulnerable to security issues. Because applications depend on the existence and exact address of that page, most functions are simply removed and replaced by a special trap instruction. More detailed explanation can be found on <a href=https://lwn.net/Articles/446528/>lwn.net</a>.</p><p>Another interesting thing we observed is the base address of the executable (the start of the text section) and the start address of libc is rather different. This is also the result of using ASLR which is used to prevent return-to-libc attack.</p><h2 id=getrusage>getrusage</h2><p>Then, we call <code>getrusage</code> at the end of our program and print out the fields. We will need <code>getrusage</code> later. Here is a sample output for some fields inside <code>struct rusage</code>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>utime: 1306
stime: 0
maxrss: 2692
minflt: 76
majflt: 0
inblock: 0
oublock: 0
nvcsw: 2
nivcsw: 0
</code></pre></div><p>Here is a short list of descriptions for each of these fields. More detailed information can be found on <a href=https://www.gnu.org/software/libc/manual/html_node/Resource-Usage.html>gnu website</a></p><ul><li><strong>utime</strong>: time spent executing user instructions.</li><li><strong>stime</strong>: time spent in operating system code on behalf of processes.</li><li><strong>maxrss</strong>: the maximum resident set size used, in kilobytes. That is, the maximum number of kilobytes of physical memory that processes used simultaneously.</li><li><strong>minflt</strong>: the number of page faults which were serviced without requiring any I/O.</li><li><strong>majflt</strong>: the number of page faults which were serviced by doing I/O.</li><li><strong>inblock</strong>: the number of times the file system had to read from the disk on behalf of processes.</li><li><strong>oublock</strong>: the number of times the file system had to write to the disk on behalf of processes.</li><li><strong>nvcsw</strong>: the number of times processes voluntarily invoked a context switch (usually to wait for some service).</li><li><strong>nivcsw</strong>: the number of times an involuntary context switch took place (because a time slice expired, or another process of higher priority was scheduled).</li></ul><h2 id=perf_event_open>perf_event_open</h2><p><code>perf_event_open</code> interface is useful to measurement numerous system events. However, glibc doesn&rsquo;t provide wrapper for this system call. Instead, we need to use <code>syscall</code> directly.</p><p>To use <code>perf_event_open</code>, we call create a function wrapper that does the actual syscall for us. Take the example from the <a href=https://man7.org/linux/man-pages/man2/perf_event_open.2.html>Linux man page</a></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span>
<span style=color:#a6e22e>perf_event_open</span>(<span style=color:#66d9ef>struct</span> perf_event_attr <span style=color:#f92672>*</span>hw_event, pid_t pid,
                <span style=color:#66d9ef>int</span> cpu, <span style=color:#66d9ef>int</span> group_fd, <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>long</span> flags)
{
    <span style=color:#66d9ef>int</span> ret;

    ret <span style=color:#f92672>=</span> syscall(__NR_perf_event_open, hw_event, pid, cpu,
                    group_fd, flags);
    <span style=color:#66d9ef>return</span> ret;
}
</code></pre></div><p>Here the <code>__NR_perf_event_open</code> specifies the syscall number. On our local machine, we can go to <code>/usr/include/x86_64-linux-gnu/sys/syscall.h</code>, which specifies the location of <code>__NR_perf_event_open</code>. In our case, it is located at <code>/usr/include/x86_64-linux-gnu/asm/unistd_64.h</code>.</p><p>If we call <code>objdump -d</code> on the binary file, we will see something like this</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>000000000000119a &lt;perf_event_open&gt;:
    119a:	55                   	push   %rbp
    119b:	48 89 e5             	mov    %rsp,%rbp
    119e:	48 83 ec 30          	sub    $0x30,%rsp
    11a2:	48 89 7d e8          	mov    %rdi,-0x18(%rbp)
    11a6:	89 75 e4             	mov    %esi,-0x1c(%rbp)
    11a9:	89 55 e0             	mov    %edx,-0x20(%rbp)
    11ac:	89 4d dc             	mov    %ecx,-0x24(%rbp)
    11af:	4c 89 45 d0          	mov    %r8,-0x30(%rbp)
    11b3:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    11b7:	8b 75 dc             	mov    -0x24(%rbp),%esi
    11ba:	8b 4d e0             	mov    -0x20(%rbp),%ecx
    11bd:	8b 55 e4             	mov    -0x1c(%rbp),%edx
    11c0:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
    11c4:	49 89 f9             	mov    %rdi,%r9
    11c7:	41 89 f0             	mov    %esi,%r8d
    11ca:	48 89 c6             	mov    %rax,%rsi
    11cd:	bf 2a 01 00 00       	mov    $0x12a,%edi
    11d2:	b8 00 00 00 00       	mov    $0x0,%eax
    11d7:	e8 84 fe ff ff       	callq  1060 &lt;syscall@plt&gt;
    11dc:	89 45 fc             	mov    %eax,-0x4(%rbp)
    11df:	8b 45 fc             	mov    -0x4(%rbp),%eax
    11e2:	48 98                	cltq   
    11e4:	c9                   	leaveq 
    11e5:	c3                   	retq   
</code></pre></div><p>We notice there&rsquo;s one interesting line</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>callq  1060 &lt;syscall@plt&gt;
</code></pre></div><p>The <code>plt</code> stands for Procedure Linkage Table. This lines indicates a call to the <code>syscall</code> in the procedure linking table. The PLT allows us to resolve the absolute addresses of shared libraries at runtime.</p><p>Take a look at the <code>&lt;syscall@plt></code> section of the disassembly of section .plt, we see</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>0000000000001060 &lt;syscall@plt&gt;:
    1060:	ff 25 62 2f 00 00    	jmpq   *0x2f62(%rip)    #3fc8&lt;syscall@GLIBC_2.2.5&gt;
    1066:	68 03 00 00 00       	pushq  $0x3
    106b:	e9 b0 ff ff ff       	jmpq   1020 &lt;.plt&gt;
</code></pre></div><p>Notice this jump is a pointer to an address. The address lies inside the GOT (Global Offset Table). The GOT will eventually hold the absolute address call to <code>syscall</code>. On the first call the address will point back to the instruction after the jump in the PLT - <code>0x1066</code>. Then we see another jump instruction. This jump is a jump into the eventual runtime linker code that will load the shared library which has syscall.</p><p>We also see the comment for the first jump instruction</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>#3fc8&lt;syscall@GLIBC_2.2.5&gt;
</code></pre></div><p>Use <code>objdump -R</code>, we see the dynamic relocation entries in the file</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>DYNAMIC RELOCATION RECORDS
OFFSET           TYPE              VALUE 
0000000000003d98 R_X86_64_RELATIVE  *ABS*+0x0000000000001190
0000000000003da0 R_X86_64_RELATIVE  *ABS*+0x0000000000001150
0000000000004008 R_X86_64_RELATIVE  *ABS*+0x0000000000004008
0000000000003fd8 R_X86_64_GLOB_DAT  _ITM_deregisterTMCloneTable
0000000000003fe0 R_X86_64_GLOB_DAT  __libc_start_main@GLIBC_2.2.5
0000000000003fe8 R_X86_64_GLOB_DAT  __gmon_start__
0000000000003ff0 R_X86_64_GLOB_DAT  _ITM_registerTMCloneTable
0000000000003ff8 R_X86_64_GLOB_DAT  __cxa_finalize@GLIBC_2.2.5
0000000000003fb0 R_X86_64_JUMP_SLOT  getpid@GLIBC_2.2.5
0000000000003fb8 R_X86_64_JUMP_SLOT  __stack_chk_fail@GLIBC_2.4
0000000000003fc0 R_X86_64_JUMP_SLOT  system@GLIBC_2.2.5
0000000000003fc8 R_X86_64_JUMP_SLOT  syscall@GLIBC_2.2.5
0000000000003fd0 R_X86_64_JUMP_SLOT  sprintf@GLIBC_2.2.5
</code></pre></div><h2 id=monitor-events>Monitor Events</h2><p>Next, we are going to look at L1 data cache metrics. We are interested in L1 data cache accesses, misses, and data TLB misses. We will measure this code in our experiment. CACHE_LINE_SIZE is defined as 64 to match our CPU specs.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>// p points to a region that is 1GB (ideally)
</span><span style=color:#75715e></span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>do_mem_access</span>(<span style=color:#66d9ef>char</span><span style=color:#f92672>*</span> p, <span style=color:#66d9ef>int</span> size) {
    <span style=color:#66d9ef>int</span> i, j, count, outer, locality;
    <span style=color:#66d9ef>int</span> ws_base <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
    <span style=color:#66d9ef>int</span> max_base <span style=color:#f92672>=</span> ((size <span style=color:#f92672>/</span> CACHE_LINE_SIZE) <span style=color:#f92672>-</span> <span style=color:#ae81ff>512</span>);
    <span style=color:#66d9ef>for</span>(outer <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; outer <span style=color:#f92672>&lt;</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>&lt;&lt;</span><span style=color:#ae81ff>20</span>); <span style=color:#f92672>++</span>outer) {
      <span style=color:#66d9ef>long</span> r <span style=color:#f92672>=</span> simplerand() <span style=color:#f92672>%</span> max_base;
      <span style=color:#75715e>// Pick a starting offset
</span><span style=color:#75715e></span>      <span style=color:#66d9ef>if</span>( opt_random_access ) {
         ws_base <span style=color:#f92672>=</span> r;
      } <span style=color:#66d9ef>else</span> {
         ws_base <span style=color:#f92672>+=</span> <span style=color:#ae81ff>512</span>;
         <span style=color:#66d9ef>if</span>( ws_base <span style=color:#f92672>&gt;=</span> max_base ) {
            ws_base <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
         }
      }
      <span style=color:#66d9ef>for</span>(locality <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; locality <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>16</span>; locality<span style=color:#f92672>++</span>) {
         <span style=color:#66d9ef>volatile</span> <span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>a;
         <span style=color:#66d9ef>char</span> c;
         <span style=color:#66d9ef>for</span>(i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>512</span>; i<span style=color:#f92672>++</span>) {
            <span style=color:#75715e>// Working set of 512 cache lines, 32KB
</span><span style=color:#75715e></span>            a <span style=color:#f92672>=</span> p <span style=color:#f92672>+</span> (ws_base <span style=color:#f92672>+</span> i) <span style=color:#f92672>*</span> CACHE_LINE_SIZE;
            <span style=color:#66d9ef>if</span>((i<span style=color:#f92672>%</span><span style=color:#ae81ff>8</span>) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
               <span style=color:#f92672>*</span>a <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
            } <span style=color:#66d9ef>else</span> {
               c <span style=color:#f92672>=</span> <span style=color:#f92672>*</span>a;
            }
         }
      }
   }
}
</code></pre></div><p>What this routine does is essentially pick a working set of 512 cache lines, periodically perform a write or otherwise read operation. This process is repeated 16 times during each interaction. Each read or write access will operate on a new cache line. The innermost loop will perform this set of operations for the entire L1 data cache.</p><p>When opt_random_access is true, the starting base address of the cache line is randomly picked. Otherwise, it is incremented by 512 cache lines (or one working set) during each outer iteration. The main difference is that with opt_random_access set to true, the starting base address of the cache line can&rsquo;t be precomputed by the hardware, thus likely increase miss rate.</p><p>To measure L1 data cache metrics, we will use the <code>perf_event_open</code> interface we discussed above. To measure L1 data cache read misses, we will configure our <code>struct perf_event_attr</code> as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>#define CALC_CONFIG(perf_hw_cache_id, perf_hw_cache_op_id, perf_hw_cache_op_result_id) \
</span><span style=color:#75715e>((perf_hw_cache_id) | (perf_hw_cache_op_id &lt;&lt; 8) | (perf_hw_cache_op_result_id &lt;&lt; 16))
</span><span style=color:#75715e></span>
hw_event.type <span style=color:#f92672>=</span> PERF_TYPE_HW_CACHE; 
hw_event.size <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>struct</span> perf_event_attr);
hw_event.disabled <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>; <span style=color:#75715e>// disable at init time
</span><span style=color:#75715e></span>hw_event.exclude_kernel <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
hw_event.config <span style=color:#f92672>=</span> CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS);
</code></pre></div><p>The exact details can be found in <a href=https://man7.org/linux/man-pages/man2/perf_event_open.2.html>linux man page</a>. The important part is:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>hw_event.config <span style=color:#f92672>=</span> CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS);
</code></pre></div><p>These configurations allows us to measure the L1 data cahe read misses. The arguments passed to <code>perf_event_open</code> is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>pid_t pid <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
<span style=color:#66d9ef>int</span> cpu <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>;
<span style=color:#66d9ef>int</span> group_fd <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>;
<span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>long</span> flags <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</code></pre></div><p>The choice of these parameters can also be found on the <a href=https://man7.org/linux/man-pages/man2/perf_event_open.2.html>linux man page</a>. After <code>perf_event_open</code> is called, we will re-enable event measurements by calling</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>ioctl(fd, PERF_EVENT_IOC_RESET, <span style=color:#ae81ff>0</span>);
ioctl(fd, PERF_EVENT_IOC_ENABLE, <span style=color:#ae81ff>0</span>);
</code></pre></div><p>What it does is resetting the event count specified by the file descriptor argument to zero, then enables the individual event specified by the file descriptor argument. After <code>do_mem_access(p, size)</code> is executed, we call <code>ioctl(fd, PERF_EVENT_IOC_DISABLE, 0)</code> to disable the event and then read the result by <code>read(fd, &result, sizeof(long long))</code>. How result is defined is up to how <code>PERF_FORMAT_*</code> was specified. You can also check <a href=https://elixir.bootlin.com/linux/latest/source/kernel/events/core.c#L1833>lxr</a> to see how <code>__perf_event_read_size</code> calculates the size of event that is read. In our case, it&rsquo;s simple a <code>u64</code>.</p><blockquote><p>Be aware that simply executing the binary might cause <code>perf_event_open</code> to fail (in which case will always return -1). Using <code>sudo</code> is one workaround. Execute <code>cat /proc/sys/kernel/perf_event_paranoid</code> and see what returns. <code>-1</code> means you have raw access to kernel tracepoints. Otherwise, you might have trouble accessing the performance counter without root privilege. Check this <a href=https://unix.stackexchange.com/questions/14227/do-i-need-root-admin-permissions-to-run-userspace-perf-tool-perf-events-ar>stackexchange post</a> for more details.</p></blockquote><p>To be even more careful about generating repeatable results we should flush the level 1 data cache before enabling the performance counters. We will do this by reading a memory buffer larger than per-core L1 data cache size</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>size_t buffer_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>1024</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1024</span>;
<span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>buff <span style=color:#f92672>=</span> malloc(buffer_size);
<span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> buffer_size; i<span style=color:#f92672>++</span>) {
    buff[i] <span style=color:#f92672>=</span> rand();
}
</code></pre></div><p>We will also lock the process onto a single processor by using the <code>sched_setaffinity</code> function. Our example is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>cpu_set_t set;
CPU_ZERO(<span style=color:#f92672>&amp;</span>set);
CPU_SET(<span style=color:#ae81ff>7</span>, <span style=color:#f92672>&amp;</span>set);
<span style=color:#66d9ef>int</span> aff <span style=color:#f92672>=</span> sched_setaffinity(<span style=color:#ae81ff>0</span>, <span style=color:#66d9ef>sizeof</span>(cpu_set_t), <span style=color:#f92672>&amp;</span>set);
</code></pre></div><p>We perform the each of the above experiments 5 times. First, we turn on random cache line base address generation. On average, we have around 1010665367 L1 data cache read misses wtih standard deviation to be 61010967 misses. When random access is disabled, we have on average 964420324 read misses with standard deviation of 65787193 misses. We can also measure the number L1 data cache write misses by using the <code>PERF_COUNT_HW_CACHE_OP_WRITE</code> config instead. Use <code>PERF_COUNT_HW_CACHE_OP_PREFETCH</code> gives us prefetch misses, in our case, both of these metrics are unavailable. We can check the <code>/arch/x86/events/intel/core.c</code> in <a href=https://elixir.bootlin.com/linux/v5.6/source/arch/x86/events/intel/core.c>lxr</a> and we can see these metrics are not available.</p><p>We can also use the <code>PERF_COUNT_HW_CACHE_DTLB</code> config option for data TLB measurement. For read access we have on average 3390719 misses with std dev being 17579, while write access has 1486451 misses with std dev being 13455. The prefetch metrics for TLB are unavailable in our case. To find out more about available metrics supported, please check the constant <code>static __initconst const u64 skl_hw_cache_event_ids</code> for specific kernel version.</p><p>With random cache line access turned off, we have 517335 read misses data TLB with standard deviation of 3820 misses. For write we have on average 809671 misses with standard deviation being 9580 misses. It is a significant reduction compared to the random access implementation.</p><p>To calculate the L1 cache miss rate and data TLB miss rate, we can use 100.0 * cache misses / cache_accesses and 100.0 * tlb misses / cache_accesses to calculate the results. With random access turned off, we get L1 read access miss rate to be $$miss_{cache} = 1.5%$$ and TLB read miss rate $$miss_{tlb} \approx 0$$. When random access is turned on, we have $$miss_{cache} = 1.4%$$ and $$miss_{tlb} \approx 0$$. We can see the miss rate in all scenarios is really low. This is mainly because the inner most loop in our routine is performing operations on working set already presented in L1 cache and TLB. The read/write operations use continous cache lines, which means there will almost be no faults while we access the 512 cache lines. If one fault causes the entire new working set to be cached, then there would be no subsequent faults until the entire working set is iterated.</p><p>If we use <code>getrusage</code> we can see the metrics listed below:</p><table><thead><tr><th>Metrics</th><th>Mean</th><th>std dev</th></tr></thead><tbody><tr><td>utime</td><td>868629</td><td>126044</td></tr><tr><td>stime</td><td>253586</td><td>20112</td></tr><tr><td>maxrss</td><td>1049691</td><td>43</td></tr><tr><td>minflt</td><td>262214</td><td>1</td></tr><tr><td>majflt</td><td>0</td><td>0</td></tr><tr><td>inblock</td><td>0</td><td>0</td></tr><tr><td>oublock</td><td>0</td><td>0</td></tr><tr><td>nvcsw</td><td>0.4</td><td>0.54</td></tr><tr><td>nivcsw</td><td>47</td><td>7</td></tr></tbody></table><p><br></p><h2 id=mmap>mmap</h2><p>Next we are going to explore the behavior of mmap. Previously, we used <code>malloc</code> for data allocation. Next, we are going to instead use <code>mmap</code> and see what happens. Here we will only use read access for benchmark metrics since it&rsquo;s available in both L1 and TLB metrics.</p><p>First, we use the <code>MAP_ANONYMOUS</code> as a flag passed to <code>mmap</code>. This flag means the mapping is not backed by any file; its contents are initialized to zero. The complete call is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>mmap(NULL, length, PROT_READ <span style=color:#f92672>|</span> PROT_WRITE,
                MAP_PRIVATE <span style=color:#f92672>|</span> MAP_ANONYMOUS, fd_ignore, offset);
</code></pre></div><p>For more details, refer to <a href=https://man7.org/linux/man-pages/man2/mmap.2.html>mmap man page</a> for information.</p><p>When we turn on the random access and use <code>perf_event_open</code> interface to collect metrics, we see the L1 data cache read misses are 956148031 (std dev 84631843). The TLB data cache read misses are 3370309 (std dev 17792). We see it is not really different to the malloc approach we used before. Doing a simple <code>strace</code> shows <code>malloc</code> calls <code>mmap</code>. The memory that backs <code>malloc()</code> allocations is handled by the kernel in much the same way as the memory that backs private anonymous mappings created with <code>mmap()</code>.</p><p>Then, we try to use <code>mmap()</code> to create mapping in the virtual address space backed by a file instead of using <code>MAP_ANONYMOUS</code>.</p><p>We first test <code>mmap</code> with <code>MAP_PRIVATE</code>. According to the man page, this flags means creating a private copy-on-write mapping. Updates to the mapping are not visible to other processes mapping the same file, and are not carried through to the underlying file. It is unspecified whether changes made to the file after the mmap() call are visible in the mapped region.</p><blockquote><p>Note we should call <code>fallocate()</code> for the newly created file, otherwise mmap is gonna throw bur error.</p></blockquote><p>When we measure the L1 data cache miss, it&rsquo;s around 946128512 (std dev 956148031), nothing special happens. When we use <code>MAP_SHARED</code> flag, the result was similar. The result seems to fluctuates as time passes, but overall they are not much different. After all, it&rsquo;s just reading from the memory, whether the address is backed by a file or not doesn&rsquo;t play a big role in affecting the cache miss rate. The L1 data cache misses is shown below:</p><table><thead><tr><th>Flag</th><th>PRIVATE</th><th>PRIVATE+POPULATE</th><th>SHARED</th><th>SHARED+POPULATE</th></tr></thead><tbody><tr><td>Mean</td><td>783864673</td><td>769314361</td><td>842915231</td><td>816749524</td></tr><tr><td>Std dev</td><td>77816766</td><td>53913082</td><td>54613278</td><td>60580595</td></tr></tbody></table><p><br></p><p>If we take a look at TLB data cache, the result is</p><table><thead><tr><th>Flag</th><th>PRIVATE</th><th>PRIVATE+POPULATE</th><th>SHARED</th><th>SHARED+POPULATE</th></tr></thead><tbody><tr><td>Mean</td><td>3372303</td><td>3370740</td><td>3381755</td><td>3377370</td></tr><tr><td>Std dev</td><td>9884</td><td>13567</td><td>17626</td><td>11776</td></tr></tbody></table><p><br></p><p>Still, there doesn&rsquo;t seem have any significant fluctuation in the number of misses in data TLB. This pattern also applies to sequential access, except the TLB data cache misses is alot lower in sequentual access.</p><p>Now If we instead use <code>getrusage()</code>, we will get something like this</p><table><thead><tr><th>Flag</th><th>PRIVATE</th><th>PRIVATE+POPULATE</th><th>SHARED</th><th>SHARED+POPULATE</th></tr></thead><tbody><tr><td>Usec/std dev</td><td>20/0</td><td>20/0</td><td>20/0</td><td>20/0</td></tr><tr><td>usec/std dev</td><td>801512/ 78346</td><td>793452/ 143556</td><td>872342/ 124124</td><td>671957/ 229314</td></tr><tr><td>Ssec/std dev</td><td>0/0</td><td>0/0</td><td>0/0</td><td>0/0</td></tr><tr><td>ssec/std dev</td><td>475977/ 54355</td><td>475678/ 134253</td><td>445467/ 99345</td><td>536041/ 98797</td></tr><tr><td>oublock/std dev</td><td>0/0</td><td>0/0</td><td>2997152/ 82256</td><td>2097152/ 19760</td></tr></tbody></table><p><br></p><p>The most interesting part here is when <code>MAP_SHARED</code> is enabled, the <code>oublock</code> immediately changes. As we mentioned previously, <code>oublock</code> specifies the number of times the file system had to write to the disk on behalf of processes. Because the address is now backed by a file, all write operations will cause the file system to write the contents back to the file.</p><p><code>mmap()</code> creates a new mapping in the virtual address space of the
calling process. However, it doesn&rsquo;t allocate RAM. If we call <code>memset()</code> then followed by <code>msync()</code> with <code>MS_SYNC</code> flag, we can get some interesting results in <code>getrusage</code>, these observations are summarized here:</p><ul><li>kernel space time is much higher. It usually take 1 sec (no std dev) as opposed to 0. Synchronizing to files on disk will require more kernel participation.</li><li>minflt (the number of page faults which were serviced without requiring any I/O) was muich higher, the value is around 540782(std dev 3). More memory mapped means the faults by I/O will be less likely.</li><li>oublock is much higher, the value is around 4196512(std dev 1). The sync operating means there will be approximatly double amount of writes to disk.</li><li>nvcsw was higher, there are more voluntary context switches. Writing results to disk has delay, and thus the process likely need to context switch while waiting for I/O to be finished.</li></ul><p>We may notice the number data TLB misses is lower than the total number of page the application uses. One obvious answer the use of huge page. One huge page can cover many small pages. Also, because we have prefetching TLB and the working set access pattern is contiguous, TLB hit rate will be high. Because we have a set-associative TLB cache, and we access the memory in a fairly deterministic way, it&rsquo;s easy to predict where the next access is pointing to. For example, if the replacement policy is FIFO, then each cache line will remain untouched for exact same clock cycle before replaced. This also applies to other policies. One way to determine the replacement algorithm is using P-Chase.</p><h2 id=strace>strace</h2><p>We then use <code>strace</code> to trace syscalls of our application. The output contains some interesting information, one is</p><pre><code>access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)
...
arch_prctl(ARCH_SET_FS, 0x7fdc6ad83540) = 0
</code></pre><p>According to <a href=https://man7.org/linux/man-pages/man2/arch_prctl.2.html>arch_prctl man page</a>, <code>arch_prctl()</code> sets architecture-specific process or thread state. The <code>ARCH_SET_FS</code> option sets the 64-bit base for the FS register to addr, in our case it&rsquo;s 0x7fdc6ad83540. Let&rsquo;s set a break point at <code>arch_prctl</code> and backtrace from there</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>#0  0x00007ffff7febb55 in ?? () from /lib64/ld-linux-x86-64.so.2
#1  0x00007ffff7fd104c in ?? () from /lib64/ld-linux-x86-64.so.2
#2  0x00007ffff7fd0108 in ?? () from /lib64/ld-linux-x86-64.so.2
#3  0x0000000000000001 in ?? ()
#4  0x00007fffffffe2fa in ?? ()
#5  0x0000000000000000 in ?? ()
</code></pre></div><p>We can see the FS segment base is set by the <code>ld-linux</code>, which is a part of glibc, during the program loading. A simple google seach tells us <code>/lib64/ld-linux-x86-64.so.2</code> is a dynamic linker. A more detailed description can be found on this <a href=https://unix.stackexchange.com/questions/400621/what-is-lib64-ld-linux-x86-64-so-2-and-why-can-it-be-used-to-execute-file>post</a> and <a href=https://lwn.net/Articles/631631/>lwn.net</a>. During the startup, the loader initalizes TLS. The includes memory allocation and setting FS base value to point to the TLS beignning, which is done via the <code>arch_prctl</code> syscall. More can be found <a href=https://unix.stackexchange.com/questions/453749/what-sets-fs0x28-stack-canary/453772>here</a>. This <code>init_tls()</code> is called <a href="https://git.launchpad.net/glibc/tree/elf/rtld.c?id=916124ed841745b7a1e0fbc43f9909340b47d373#n1397">here</a>, which subsequently calls the actuall <a href=https://git.launchpad.net/glibc/tree/sysdeps/x86_64/nptl/tls.h#n153>syscall</a> in <code>tls.h</code>.</p><p>The /etc/ld.so.preload has similarities to LD_PRELOAD, in addition, it doesn&rsquo;t suffer security limitation posed by LD_PRELOAD (<a href=https://superuser.com/questions/1183037/what-is-does-ld-so-preload-do>explanation here</a>). This a feature of <em>glibc</em>.</p><h2 id=competing-for-memory>Competing for Memory</h2><p>Next we are going to fork another process that will compete for memory with our process under test. We will use this code snippet which is going to be executed by both the parent and the child process</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>compete_for_memory</span>(<span style=color:#66d9ef>void</span><span style=color:#f92672>*</span> unused) {
   <span style=color:#66d9ef>long</span> mem_size <span style=color:#f92672>=</span> get_mem_size();
   <span style=color:#66d9ef>int</span> page_sz <span style=color:#f92672>=</span> sysconf(_SC_PAGE_SIZE);
   printf(<span style=color:#e6db74>&#34;Total memsize is %3.2f GBs</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>,
         (<span style=color:#66d9ef>double</span>)mem_size<span style=color:#f92672>/</span>(<span style=color:#ae81ff>1024</span><span style=color:#f92672>*</span><span style=color:#ae81ff>1024</span><span style=color:#f92672>*</span><span style=color:#ae81ff>1024</span>));
   fflush(stdout);
   <span style=color:#66d9ef>char</span><span style=color:#f92672>*</span> p <span style=color:#f92672>=</span> mmap(NULL, mem_size, PROT_READ <span style=color:#f92672>|</span> PROT_WRITE,
                  MAP_NORESERVE<span style=color:#f92672>|</span>MAP_PRIVATE<span style=color:#f92672>|</span>MAP_ANONYMOUS, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, (off_t) <span style=color:#ae81ff>0</span>);
   <span style=color:#66d9ef>if</span> (p <span style=color:#f92672>==</span> MAP_FAILED)
      perror(<span style=color:#e6db74>&#34;Failed anon MMAP competition&#34;</span>);

   <span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
   <span style=color:#66d9ef>while</span>(<span style=color:#ae81ff>1</span>) {
      <span style=color:#66d9ef>volatile</span> <span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>a;
      <span style=color:#66d9ef>long</span> r <span style=color:#f92672>=</span> simplerand() <span style=color:#f92672>%</span> (mem_size<span style=color:#f92672>/</span>page_sz);
      <span style=color:#66d9ef>char</span> c;
      <span style=color:#66d9ef>if</span>( i <span style=color:#f92672>&gt;=</span> mem_size<span style=color:#f92672>/</span>page_sz ) {
         i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
      }
      <span style=color:#75715e>// One read and write per page
</span><span style=color:#75715e></span>      <span style=color:#75715e>//a = p + i * page_sz; // sequential access
</span><span style=color:#75715e></span>      a <span style=color:#f92672>=</span> p <span style=color:#f92672>+</span> r <span style=color:#f92672>*</span> page_sz;
      c <span style=color:#f92672>+=</span> <span style=color:#f92672>*</span>a;
      <span style=color:#66d9ef>if</span>((i<span style=color:#f92672>%</span><span style=color:#ae81ff>8</span>) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
         <span style=color:#f92672>*</span>a <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
      }
      i<span style=color:#f92672>++</span>;
   }
   <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
}
</code></pre></div><p>The <code>get_mem_size()</code> is implemented using this <a href=https://stackoverflow.com/questions/22670257/getting-ram-size-in-c-linux-non-precise-result>portable code</a></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>#if defined(_WIN32)
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;Windows.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#75715e>#elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) &amp;&amp; defined(__MACH__))
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;unistd.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;sys/types.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;sys/param.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#if defined(BSD)
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;sys/sysctl.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#endif
</span><span style=color:#75715e></span>
<span style=color:#75715e>#else
</span><span style=color:#75715e>#error &#34;Unable to define getMemorySize( ) for an unknown OS.&#34;
</span><span style=color:#75715e>#endif
</span><span style=color:#75715e></span>
<span style=color:#75715e>/**
</span><span style=color:#75715e> * Returns the size of physical memory (RAM) in bytes.
</span><span style=color:#75715e> */</span>
size_t <span style=color:#a6e22e>getMemorySize</span>( )
{
<span style=color:#75715e>#if defined(_WIN32) &amp;&amp; (defined(__CYGWIN__) || defined(__CYGWIN32__))
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* Cygwin under Windows. ------------------------------------ */</span>
    <span style=color:#75715e>/* New 64-bit MEMORYSTATUSEX isn&#39;t available.  Use old 32.bit */</span>
    MEMORYSTATUS status;
    status.dwLength <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(status);
    GlobalMemoryStatus( <span style=color:#f92672>&amp;</span>status );
    <span style=color:#66d9ef>return</span> (size_t)status.dwTotalPhys;

<span style=color:#75715e>#elif defined(_WIN32)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* Windows. ------------------------------------------------- */</span>
    <span style=color:#75715e>/* Use new 64-bit MEMORYSTATUSEX, not old 32-bit MEMORYSTATUS */</span>
    MEMORYSTATUSEX status;
    status.dwLength <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(status);
    GlobalMemoryStatusEx( <span style=color:#f92672>&amp;</span>status );
    <span style=color:#66d9ef>return</span> (size_t)status.ullTotalPhys;

<span style=color:#75715e>#elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) &amp;&amp; defined(__MACH__))
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* UNIX variants. ------------------------------------------- */</span>
    <span style=color:#75715e>/* Prefer sysctl() over sysconf() except sysctl() HW_REALMEM and HW_PHYSMEM */</span>

<span style=color:#75715e>#if defined(CTL_HW) &amp;&amp; (defined(HW_MEMSIZE) || defined(HW_PHYSMEM64))
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>int</span> mib[<span style=color:#ae81ff>2</span>];
    mib[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> CTL_HW;
<span style=color:#75715e>#if defined(HW_MEMSIZE)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_MEMSIZE;            <span style=color:#75715e>/* OSX. --------------------- */</span>
<span style=color:#75715e>#elif defined(HW_PHYSMEM64)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_PHYSMEM64;          <span style=color:#75715e>/* NetBSD, OpenBSD. --------- */</span>
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>    int64_t size <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;               <span style=color:#75715e>/* 64-bit */</span>
    size_t len <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>( size );
    <span style=color:#66d9ef>if</span> ( sysctl( mib, <span style=color:#ae81ff>2</span>, <span style=color:#f92672>&amp;</span>size, <span style=color:#f92672>&amp;</span>len, NULL, <span style=color:#ae81ff>0</span> ) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> )
        <span style=color:#66d9ef>return</span> (size_t)size;
    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0L</span>;          <span style=color:#75715e>/* Failed? */</span>

<span style=color:#75715e>#elif defined(_SC_AIX_REALMEM)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* AIX. ----------------------------------------------------- */</span>
    <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_AIX_REALMEM ) <span style=color:#f92672>*</span> (size_t)<span style=color:#ae81ff>1024L</span>;

<span style=color:#75715e>#elif defined(_SC_PHYS_PAGES) &amp;&amp; defined(_SC_PAGESIZE)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* FreeBSD, Linux, OpenBSD, and Solaris. -------------------- */</span>
    <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_PHYS_PAGES ) <span style=color:#f92672>*</span>
        (size_t)sysconf( _SC_PAGESIZE );

<span style=color:#75715e>#elif defined(_SC_PHYS_PAGES) &amp;&amp; defined(_SC_PAGE_SIZE)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* Legacy. -------------------------------------------------- */</span>
    <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_PHYS_PAGES ) <span style=color:#f92672>*</span>
        (size_t)sysconf( _SC_PAGE_SIZE );

<span style=color:#75715e>#elif defined(CTL_HW) &amp;&amp; (defined(HW_PHYSMEM) || defined(HW_REALMEM))
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* DragonFly BSD, FreeBSD, NetBSD, OpenBSD, and OSX. -------- */</span>
    <span style=color:#66d9ef>int</span> mib[<span style=color:#ae81ff>2</span>];
    mib[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> CTL_HW;
<span style=color:#75715e>#if defined(HW_REALMEM)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_REALMEM;        <span style=color:#75715e>/* FreeBSD. ----------------- */</span>
<span style=color:#75715e>#elif defined(HW_PYSMEM)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_PHYSMEM;        <span style=color:#75715e>/* Others. ------------------ */</span>
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> size <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;      <span style=color:#75715e>/* 32-bit */</span>
    size_t len <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>( size );
    <span style=color:#66d9ef>if</span> ( sysctl( mib, <span style=color:#ae81ff>2</span>, <span style=color:#f92672>&amp;</span>size, <span style=color:#f92672>&amp;</span>len, NULL, <span style=color:#ae81ff>0</span> ) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> )
        <span style=color:#66d9ef>return</span> (size_t)size;
    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0L</span>;          <span style=color:#75715e>/* Failed? */</span>
<span style=color:#75715e>#endif </span><span style=color:#75715e>/* sysctl and sysconf variants */</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#75715e>#else
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0L</span>;          <span style=color:#75715e>/* Unknown OS. */</span>
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>}
</code></pre></div><p>The important line is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c> <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_PHYS_PAGES ) <span style=color:#f92672>*</span>
        (size_t)sysconf( _SC_PAGESIZE );
</code></pre></div><p>One thing to notice in the routine for competing for memory is we used <code>fflush</code> after the <code>printf</code>. The purpose of <code>fflush(stream)</code> is to make the operating system flush any buffers to the underlying file. This mainly because stdout is buffered. The buffer is not flushed until newline. <code>fflush</code> will cause this process to happen with the absense of newline. stderr is unbuffered and thus fflush would not be necessary.</p><p>For this experiment, we tested it on a VM. The reason is because the contending process will take all RAM and completely hault the mahcine if tested on the host. To ensure our VM has enough swap space, we follow this <a href=https://wiki.crowncloud.net/?adding_swap_kvm>tutorial</a> to create 4GB of swap area (we allocated 2GB RAM for VM).</p><p>One thing we observe is that the execution time of the program become significantly longer to run. In our experiement we need to limit the number of iterations from 1 &#171; 20 to 1 &#171; 8 to get some sensible results without running for days.</p><p>When we use PRIVATE and ANONYMUS option and random access turned on, the misses in data TLB is 335009(std dev 7298). We can&rsquo;t get access to L1 cache data because it will cause the session to be automatically logged out whenever L1D is used. here are some interesting things to notice:</p><ul><li><em>MAP_PRIVATE + MAP_ANONYMOUS</em>: TLB misses:335009(std dev 17298)<br>minflt: 4220(std dev 231)<br>oublock: 8(std dev 4)<br>nivcsw: 19(10)</li><li><em>MAP_SHARED</em>: TLB misses:251284std dev 103292)<br>minflt: 2784(std dev 231)<br>majflt: 247(std 65)<br>oublock: 18200(std dev 2987)<br>nivcsw: 8(7)</li></ul><p>The most important difference here is that the oublock is much easier to trigger because the constant swapping. When file backed memory is used we also notice that majflt is much higher. Because pages are constantly traveling between swap area and memory, the page fault rate becomes a lot higher. The oublock also follows previous patterns as the file backed memory requires filesystem involvement.</p><p>Finally, we also modify the kernel&rsquo;s head (or more precisely its LRU page replacement algorithm). Look in <code>mm/vmscan.c</code> there&rsquo;s a function calleed <code>shrink_page_list</code>. In it, you will see a switch statement with a PAGEREF_ACTIVATE case, which is the case where the kernel sees the page has been recently accessed. In this case the kernel gotos activate_locked, but you will change it to to do the same thing as the PAGEREF_RECLAIM case. We can simply move the case down and change its default behavior to direct to the PAGEREF_RECLAIM case. After that, we need to recompile the kernel for VM. We also summarize the most interesting results:</p><ul><li><em>MAP_PRIVATE + MAP_ANONYMOUS</em>: TLB misses:308031(std dev 17298)<br>minflt: 4223(std dev 791)<br>oublock: 8(std dev 1)<br>nivcsw: 11(5)</li><li><em>MAP_SHARED</em>: TLB misses: 251284std dev 103292)<br>minflt: 2724(std dev 231)<br>majflt: 0(std 0)<br>oublock: 18200(std dev 2987)<br>nivcsw: 8(7)</li></ul><p>We can see that the most of the pattern follow the previous result after the modified kernel is installed. One main difference is majflt value is reduced back down.</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/vmwareesxserver/>Memory Resource Management in VMware ESX Server</a></h2></header><footer class=entry-footer>September 21, 2020&nbsp;·&nbsp;3 min</footer><section class=entry-content><p><p><a href=https://www.vmware.com/products/esxi-and-esx.html>VMWare ESX Server</a> is a software layer designed to multiplex hardware resources among virtual machines running unmodified commodity operating systems. ESX Server, different to <a href=https://www.vmware.com/products/workstation-pro.html>VMware Workstation</a>, is a type 1 hypervisor, which means it runs directly on bare metal. ESX Server focuses on running guest VMs without modifying the guest OSes at all, which is challenging.</p><blockquote><p>Memory Virtualization is done by interposing an extra abstraction layer between a <code>physical address</code> from the VM&rsquo;s point of view, and a <code>machine address</code> which represents the actual hardware memory. ESX Server maintains a <em>pmap</em> data structure for each VM to translate PPMs to MPNs. A separate <em>shadow page table</em>, consistent with the physical-to-machine mappings, is used to map virtual-to-machine page mappings. This avoids additional overheads as the hardware TLB will cache direct virtual-to-machine address translations read from the shadow page table.</p></blockquote><h2 id=key-features>Key features</h2><p align=center><a href=https://www.vmware.com/pdf/usenix_resource_mgmt.pdf><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/ESXServer/ballooning.png width=70%></a></p><p><strong>Ballooning</strong> is a technique used by the server to achieve memory reclamation. As its name suggests, the hypervisor inflates the balloon by instructing the balloon driver module to allocate pinned physical pages and deflates it by instructing it to deallocate previously-allocated pages. The idea behind this technique is that the hypervisor is unaware of the specific usage patterns of policies of its guests, therefore the making page replacement decisions is best done in the guest VM. When the hypervisor over commits memory, it needs some way to claim memories from the VMs. By consuming some of the memory that the guest OS believes is physically present in the virtual machine. The guest OS will then swap memory to disk reducing the load on the host&rsquo;s physical memory. The host will them reallocate that memory to other VMs. A details description of ballooning can be found in this <a href=https://www.vladan.fr/what-is-vmware-memory-ballooning/>post</a>.</p><p align=center><a href=https://www.vmware.com/pdf/usenix_resource_mgmt.pdf><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/ESXServer/pghash.png width=70%></a></p><blockquote><p>Page Coloring can be used to reduce cache misses or partition resources. But it might complicates memory management, especially with the presence of huge pages. Because coloring enforces ownership, thus might result in distinct L2 cache entries.</p></blockquote><p><strong>Sharing memory</strong> is achieved by comparing the content of each page, since modifying guest operating system internals is not possible. Because comparing each page would be \(O(n^2)\), hashing is used to identify pages to make the progress more efficiently. By letting VMs share pages based the contents, the host can potentially save spaces dramatically. For example, the presence of zero pages is a great opportunity for page sharing by mapping one zero page to multiple VMs. Hint is hash hit, but it doesn&rsquo;t guarantee the content of the page doesn&rsquo;t change at that moment.</p><p><strong>Idle Memory</strong> presents a problem in pure proportional-share algorithms because they do not incorporate any information about active memory usage. More specifically, the memory demand might change dynamically. ESX Server collects <em>idle memory tax</em> from VMs to mitigate this issue. A client is charged more for an idle page than the active one. The cost of idle memory is inflated by tax rate. The metrics of idles pages in guests is collected by hypervisor without guests' involvement. The idle page information in virtual page table inside VMs is periodically sampled on random bases.</p><h2 id=questions>Questions</h2><p>a. What is the overhead of ballooning? Triggering memory management in the VM by &ldquo;tricking&rdquo; it into thinking the the memory resource is scarce/plentiful may have unexpected behaviors.<br>b. Do content-based sharing pose security vulnerabilities?<br>c. Remapping hot I/O pages to low memory can be a bottleneck if the page number is high. How does modern hypervisor solution cope with this issue?</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/operatingsystemmemorymanagement/>A Little Review on Barrelfish Memory Management</a></h2></header><footer class=entry-footer>February 18, 2019&nbsp;·&nbsp;9 min</footer><section class=entry-content><p><p>The memory management has been mentioned numerous times and still remains huge topic. virtual vs. physical memory, physical frame allocation, MMUs, page faults, address space layout, and
demand paging and swapping are familiar terms for every undergrad in college.</p><p>In monolithic kernels such as Linux, much of the functionality is handled in kernel. However, there are OSes that push these functionalities to user space such as Barrelfish. Many concept here will thus be borrowed from the <a href=http://www.barrelfish.org/>Barrelfish OS</a>. I will also borrow some materials from the main pdf from Barrelfish course materials provided by Professor Simon Peter.</p><h2 id=memory-management-in-general>Memory Management in General</h2><p>Microkernels like L4, Mach, Chorus, and Spring trapped page faults in the kernel but then reflected them up to other processes which carried out the
actual page fault handling. This was done on a per-region basis, so each area of
virtual memory was associated with some paging server. Memory objects could be shared between different processes and mapped differently in different address spaces.</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/Barrelfish/os.png width=400></p><p>Such abstraction means that what happens when a page fault happens is entirely dependent on the code in the user-level pager. This design is highly extensible since it&rsquo;s all user code and thus isolated, which means that if a user-level pager crashes, there&rsquo;s a good chance the rest of the OS can continue quite happily since much of the functionality is moved away from the kernel.</p><p>However, moving functionality out of the kernel an important question: if user-space processes can manipulate virtual address spaces, how can
we make sure that one user&rsquo;s program can&rsquo;t manipulate another address space and memory? Here we will introduce the concept of capabilities.</p><h2 id=capabilities>Capabilities</h2><p>Capabilities are introduced to solve the access control problem in operating systems. Access control is the problem of specifying, and enforcing, which subjects (or principals) can perform particular actions on particular objects in an operating system.</p><p>The Barrelfish documentation does a good job illustrating capabilities: abstractly, access control can be thought of as a matrix, which represents all possible combinations of operations in the system. Each row of the matrix represents a
different subject, and each column represents a different object. Each entry in the
matrix contains a list of permissible actions.</p><p>Thus, we have two targets to emphasis: the subject and the object. The ACL(access control list) focuses on the object being operated on.</p><p>A good example will be whenever you enter <em>ls -a</em> in a Linux terminal, you will get list of entries specifies the attributes of a file. Here the attributes represent how a object (in this case, a file) may be accessed.</p><p>On the other hand, a capability can be thought of as a &ldquo;key&rdquo; or &ldquo;licence&rdquo;. It is an unforgettable token which grants authority. Possession of a capability for an object gives the holder the right to perform certain operations on the object.</p><p>A good example will be the file descriptor in Linux. A file is accessed through its file descriptor. Here the file descriptor serves as the &ldquo;key&rdquo; to gain access to the file itself. Capabilities provide fine-grained access control: it is easy to provide access to specific subjects, and it is easy to delegate permissions to others in a controlled manner.</p><p>Note that to be correct, any capability representation must protect capabilities
against forgery. Capabilities can be implemented in various ways such as tagged capabilities, sparse capabilities, or partitioned capabilities. In Barrelfish we used the partitioned capabilities.</p><p>In partitioned capabilities, the kernel ensures that memory used to store capabilities is always separated from that used by user processes to store data and code, for example by using the MMU or ensuring that capability memory is only accessible in kernel mode. The OS maintains the list of capabilities each user principal holds (the clist), and explicitly validates access when performing any privileged operation. Thus, whenever the user accesses memory, the operation can only be done through the resources' corresponding capability. For example, one can map a page frame in the page table page through functions calls with only capabilities.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>Caprefa<span style=color:#f92672>-&gt;</span>install(Caprefb, slot, flags)
</code></pre></div><h2 id=capabilities-in-barrelfish>Capabilities in Barrelfish</h2><p>According to Barrelfish documentation, all memory in Barrelfish (and some other system resources which do not occupy memory) is described using capabilities. Capabilities are typed, and capabilities can be retyped by users holding them according to certain rules and restrictions. The official documentation has very good explanation on the capability management in Barrelfish. Here is the permissible types for the retype invocation capability retyping:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/Barrelfish/cap.png width=400></p><p align=center><a href=http://www.barrelfish.org/publications/TN-013-CapabilityManagement.pdf>Image source</a></p><p>Capabilities referring to memory regions. Capabilities can also be split, resulting 2 new capabilities of the same type, one for each half of the region. Some of the more important capability types in Barrelfish are shown in figure below. The picture is from the Barrelfish manual provided in CS378 Mutlicore class by Simon Peter:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/Barrelfish/cap_aos.png width=400></p><p>Allocation and management of physical memory is achieved by retyping and splitting operations on capabilities. For most kernels, the implementation is to constantly allocate and deallocate memory for a wide variety of purposes, much as any large C program relies heavily on malloc and free.</p><p>The problem is what the kernel should do when this runs out. The current solution in Linux is little more than &ldquo;kill a random process and reclaim its memory&rdquo;, which can be a problem for system stability. In Barrelfish, all kernel objects are actually allocated by user programs. If a user process wants to create another process (or dispatcher in Barrelfish parlance), it has to get a capability to a DRAM area of the right size, retype this capability to type Dispatcher, and hand this to the kernel. This will be covered in later posts. To access different types of memory resources, the corresponding capability has to be retyped to the right type.</p><h2 id=more-on-implementation>More On Implementation</h2><p>In Barrelfish, every capability resides in a slot in a CNode, so a pair (CNode, slot) would identify a capability. It is important to point out that the CNode is another capability itself. Each process in Barrelfish has a CSpace which is structured as a two-level table. So there are actually two different CNode capability types - one for the first level of the table, and one for the second. Every process has, within its &ldquo;dispatcher control block&rdquo;, a pointer to the top-level or root CNode which the kernel can traverse.</p><p>A capability reference in Barrelfish is very similar to VA: the first few bits can represent an index into the first level L1CNode, while the next few bits refer to a slot in a CNode referred to by the capability in the L1CNode slot. Here is a picture from the main pdf showing how the the CSpace is represented in Barrelfish:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/Barrelfish/Cspace.png width=400></p><h2 id=thoughts-on-design-decisions>Thoughts on Design Decisions</h2><p>Even though it is pretty straight forward to understand the CSpace structure, the actual implementation is a lot more complicated than that. Since the CSpace is not directly accessible by user space program, there are additional data structures used to keep track of available memory resources.</p><p>In our implementation, the user process keeps a doubly linked list of <em>struct mmnode</em> to indicate the memory available for allocation. Each element in the free list tracks the information corresponding to one capability. However, there is a big problem with this seemingly simple implementation. Every time we allocate a practical memory space from the memory region, a new capability is created while the old capability still remain in the physical memory pointing to a memory range before the allocation happens. Therefore, the old capability would cover extra memory spaces that are already allocated and managed by other capabilities.</p><p>To solve this problem, we maintain the allocation information in the <em>struct mmnode</em> each time an allocation occurs. If a capability covering physical address space from 0 to 100 is requested for 20 units of memory space, then the memory available for the next allocation would be from 20 to 100 even though the capability itself still manages 0 to 100. By restricting subsequent accesses only to the new memory range, the old capability can still be kept around and used later for retyping.</p><p>Another Problem emerges when we try to free a memory. Since everything is managed by capabilities, freeing a piece of memory also involves managing the capability responsible for the memory. So an intuitive thought could be whenever a memory space is freed, the corresponding capability is merged back to a piece of memory adjacent to it, managed by a different capability.</p><p>However, since capabilities can not be merged, an alternative choice would be to simple destroy it during free. However, this is even a bigger problem in Barrelfish.</p><p>Imagine the scenario where capability A is partially allocate from memory space 30 to 100. Later on another memory is freed and that piece of memory is managed by capability with base 100 and size 20, so the memory range covers 100 to 120, which indicates these the two capability could be &ldquo;merged&rdquo;.</p><p>In this case, if the first capability is destroyed, all children of the first capability will also be destroyed, thus the already allocated memory from 0 to 20 will be thrown away, which is not desired. If the second capability is destroyed, the first one will also be destroyed to create a new capability covering 20 to 120, which will still results in the destruction of capability A.</p><p>Our assumption here is that the parent or root capability is never destroyed when added to the free list. Whenever a capability needs to be freed, the memory manager is responsible to make sure the capability is only merged with another capability from the same parent capability.</p><p>This is done by creating another list of nodes that tracks all parent capabilities. It is only added when the memory manager adds new capabilities to the free list. After the user initializes free, the memory manager actually creates a new free struct mmnode first, then it find the node&rsquo;s parent node, copying the parent&rsquo;s capability and attributes to the newly created node with updated offset to indicate that the memory hasn&rsquo;t been freed yet.</p><p>After that, the memory manager insert the node into the free list. If the memory manager finds out that there are capabilities adjacent to the just-added node, then we simply need to update the attributes of the corresponding mmnode to indicate that merging succeeds. The old mmnode is simply thrown away.</p><p>The advantage of this implementation is that root or parent capabilities are kept around and the next retype will be fairly simple. The implementation is also very straightforward.</p><p>There is of course more efficient solution than a linked list. For example, Linux uses both linked list and red-black tree to store thread information. The redundant data structures can be used in different scenarios when appropriate. However, we only use this simplified version to prove our concepts. Optimizations vary but the general concept still works pretty well.</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/gpumemoryhierarchy/>Pascal GPU memory and cache hierarchy</a></h2></header><footer class=entry-footer>January 15, 2019&nbsp;·&nbsp;10 min</footer><section class=entry-content><p><p>Memory access efﬁciency is an important factor in fully utilizing the computational power of graphics processing units (GPUs). However, many GPU vendors like NVIDIA kept the GPU memory hierarchy as a secret. Therefore it becomes hard to measure GPUs performance and sets barriers to understand memory access patterns, which is a key component to improve program&rsquo;s performance. Here we introduce a novel fine-grained microbenchmark approach and apply to the Pascal generation. Turing architecture might have different results, but the method we used here can be applied as well with slight modification. The method we use in this guide is inspired by the research paper: <a href=https://ieeexplore.ieee.org/document/7445236>Dissecting GPU Memory Hierarchy through Microbenchmarking</a>. Here we will explain how P-Chase works and walk through a small example.</p><h2 id=memory-hierarchy-overview>Memory Hierarchy Overview</h2><p>GPU memory hierarchy is different compared to CPU memory hierarchy. Using the terminologies of CUDA, GPU memory space can be categorized in these groups: register, constant memory, shared memory, texture memory, local memory, and global memory. Each different memory space have its own properties. Since we are interested the
cache systems, here is a picture demonstrating the memory hierarchy of a NVIDIA GPU:</p><p align=center><img src=https://gistbok.ucgis.org/sites/default/files/1000px-Memory.svg_.png width=450></p><p align=center><a href=https://gistbok.ucgis.org/bok-topics/graphics-processing-units-gpus>Image source</a></p><p>The characteristics of each memory space can be found in <a href=https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA_C_Programming_Guide.pdf>NVIDIA CUDA C Programming Guide
</a>. Here we will focus on some target memory space we are interested in. The paper lists some properties of our target memory space:</p><table><thead><tr><th>Memory</th><th style=text-align:center>Type</th><th style=text-align:right>Cached</th><th style=text-align:right>Scope</th></tr></thead><tbody><tr><td>Global</td><td style=text-align:center>R/W</td><td style=text-align:right>Yes</td><td style=text-align:right>All Threads</td></tr><tr><td>Shared</td><td style=text-align:center>R/W</td><td style=text-align:right>N/A</td><td style=text-align:right>Thread Blocks</td></tr><tr><td>Texture</td><td style=text-align:center>R</td><td style=text-align:right>Yes</td><td style=text-align:right>All Threads</td></tr></tbody></table><p>Even though the paper targets Fermi, Kepler and Maxwell generations of GPU, the properties of the table still holds for Pascal GPU and possibly Turing as well. The cached global/texture memory uses a two-level caching system. The L1 cache is located in each stream multiprocessor (SM), while the L2 cache is off-chip and shared among all SMs. It is unified for instruction, data and page table access. According to CUDA documentation, like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. This function previously was served by the separate L1 cache in Fermi and Kepler. Page table is used by GPU to map virtual addresses to physical addresses, and is usually stored in the global memory. The page table is cached in TLB to reduce memory access latency. Once a thread cannot ﬁnd the page entry in the TLB, it would access the global memory to search in the page table, which introduced significant memory access latency. The GPU-specific shared memory is located in the SMs. On the Fermi and Kepler devices, it shares memory space with the L1 data cache. On Maxwell and Pascal devices, it has a dedicated space, since the functionality of the L1 and texture caches have been merged. One thing to note here is that shared memory is accessed by the thread blocks. Thread-blocks remain limited to 48 KB of shared memory in Pascal. Therefore, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block. This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM.</p><p>However, we should be careful that by default, GP100 caches global loads in the L1/Texture cache. In contrast, GP104 follows Kepler and Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler. As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to nvcc at compile time. Even though both GP100 and GP104 belongs to Pascal family, we only focus on GP100 here because that&rsquo;s the GPU we use. Another thing to notice is that unlike Maxwell but similar to Kepler, Pascal caches thread-local memory in the L1 cache. This can mitigate the cost of register spills compared to Maxwell. To illustrate our point, we checked both cudaDevAttrGlobalL1CacheSupported and cudaDevAttrLocalL1CacheSupported on Tesla P100 and GTX 1080 and find both attributes to be 1.</p><p>In addition to the L2 data cache, global memory data that is read-only for the entire lifetime of a kernel can be cached in the read-only data cache with a compute capability of 3.5 or above. We will also explore the size of this read-only cache using __ldg() intrinsic.</p><h2 id=p-chase>P-Chase</h2><p>Most existing GPU microbenchmark studies on cache architecture assume a classical set-associative cache model with the least recently used (LRU) replacement policy, the same as the conventional CPU cache. So here we will use this assumption and proceed with our experiments. Here are some notations we will use throughout this post.</p><table><thead><tr><th>Notation</th><th style=text-align:center>Description</th><th style=text-align:right>Notation</th><th style=text-align:right>Description</th></tr></thead><tbody><tr><td>C</td><td style=text-align:center>Cache Size</td><td style=text-align:right>N</td><td style=text-align:right>array size</td></tr><tr><td>b</td><td style=text-align:center>cache line size</td><td style=text-align:right>s</td><td style=text-align:right>stride size</td></tr><tr><td>a</td><td style=text-align:center>cache associativity</td><td style=text-align:right>k</td><td style=text-align:right>iterations</td></tr><tr><td>T</td><td style=text-align:center>number of cache set</td><td style=text-align:right>r</td><td style=text-align:right>cache miss rate</td></tr></tbody></table><p>Under our assumptions, data is loaded from main memory to lower cache in the basic unit of a cache line. The number of words in a cache line is referred to as the line size (b). For the LRU set-associative cache, the cache memory is divided into T cache sets, each of which consists of \(a\) cache lines. It is essential to have these three assumptions using this kind of cache model:</p><ul><li><p><strong>Assumption 1</strong> All cache sets have the same size. The cache parameter should satisfy \(T \cdot a \cdot b = C\).</p></li><li><p><strong>Assumption 2</strong> In the memory address, the bits representing the cache set are immediately followed by the bits representing the offset.</p></li><li><p><strong>Assumption 3</strong> Cache replacement policy should be LRU.</p></li></ul><p>We will later see why these assumptions are essential as we proceed with the experiment. We won&rsquo;t go through how P-Chase work exactly. To find more information, this <a href=https://arxiv.org/pdf/1509.02308.pdf>paper</a> does a good job illustrating how P-Chase work. The takeaway is, we need to brute force an array with one element more than a cache can hold so that cache miss will start to occur periodically whereas such array with less or equal elements to the cache capacity will always result in cache hit and thus no access overhead will be introduced after all data is loaded into the cache. This is the algorithm the paper proposed and we will use it to do the experiment:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>__global__ <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>KernelFunction</span> ( . . . ) {
    <span style=color:#75715e>//declare shared memory space
</span><span style=color:#75715e></span>    __shared__ <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> s tvalue [ ] ;
    __shared__ <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> s index [ ] ;
    preheat the data ; <span style=color:#75715e>// implementation varies
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (it <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; it <span style=color:#f92672>&lt;</span> iter ; it<span style=color:#f92672>++</span>) {
        start_time<span style=color:#f92672>=</span>clock();
        j <span style=color:#f92672>=</span> my_array[j];
        <span style=color:#75715e>//store the array index
</span><span style=color:#75715e></span>
        <span style=color:#75715e>// This following line is essential because due to
</span><span style=color:#75715e></span>        <span style=color:#75715e>// instruction-level parallelism (ILP), function clock() may
</span><span style=color:#75715e></span>        <span style=color:#75715e>// overlap with its previous instruction and even return before
</span><span style=color:#75715e></span>        <span style=color:#75715e>// the previous instruction finishes. For example,
</span><span style=color:#75715e></span>        <span style=color:#75715e>// end_time=clock() can return before j = my_array[j] returns.
</span><span style=color:#75715e></span>        <span style=color:#75715e>// adding s_index [it]= j since it have data dependency on the
</span><span style=color:#75715e></span>        <span style=color:#75715e>// previous line. Thus the memory access will be over before
</span><span style=color:#75715e></span>        <span style=color:#75715e>// end_time=clock() started.
</span><span style=color:#75715e></span>        s_index [it]<span style=color:#f92672>=</span> j;
        end_time<span style=color:#f92672>=</span>clock();
        <span style=color:#75715e>//store the access latency
</span><span style=color:#75715e></span>        s_tvalue[it]<span style=color:#f92672>=</span> end_time<span style=color:#960050;background-color:#1e0010>−</span>start_time ;
    }
}
</code></pre></div><p>The steps is the same as the paper proposes, so here we show the paper&rsquo;s method:</p><ol><li><p>Determine cache size C . We set s to 1. We then initialize N with a small value and increase it gradually until the ﬁrst cache miss appears. C equals the maximum N where all memory accesses are cache hits.</p></li><li><p>Determine cache line size b. We set s to 1. We begin with N = C + 1 and increase N gradually again. When N &lt; C + b + 1, the numbers of cache misses are close. When N is increased to C + b + 1, there is a sudden increase on the number of cache misses, despite that we only increase N by 1. Accordingly we can ﬁnd b. Based on the memory access patterns, we can also have a general idea on the cache replacement policy.</p></li><li><p>Determine number of cache sets T . We set s to b. We then start with N = C and increase N at the granularity of b. Every increment causes cache misses of a new cache set. When N > C + (T − 1)b, all cache sets are missed. We can then deduce T from cache miss patterns accordingly.</p></li><li><p>Determine cache replacement policy. As mentioned before, if the cache replacement policy is LRU, then the memory access process should be periodic and all the cache ways in the cache set are missed. If memory access process is aperiodic, then the replacement policy cannot be LRU. Under this circumstance, we set N = C + b, s = b with a considerable large k (k &#187; N/s) so that we can traverse the array multiple times. All cache misses are from one cache set. Every cache miss is caused by its former cache replacement because we overﬂow the cache by only one cache line. We have the accessed data indices thus we can reproduce the full memory access process and ﬁnd how the cache lines are updated.</p></li></ol><h2 id=texture-l1-cache-and-read-only-data-cache>Texture L1 Cache and Read-only Data Cache</h2><p>When use the <a href=http://www.comp.hkbu.edu.hk/~chxw/Code/fine_grain_Maxwell_texture_L1.cu>code</a> with increased our own data preheat implementation because the texture L1 cache can potentially be greater than the shared memory. The original code uses the first iteration of the loop in the algorithm as a way to preheat data:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>const</span> <span style=color:#66d9ef>int</span> it <span style=color:#f92672>=</span>  <span style=color:#ae81ff>6144</span> <span style=color:#75715e>// texture L1 may hold more elements,
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// So the first iteration may not cold
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// hit all elements, some cold hits can
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// be moved to the second iteration,
</span><span style=color:#75715e></span>                     <span style=color:#75715e>// causing confusion
</span><span style=color:#75715e></span><span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> cnt<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>; cnt <span style=color:#f92672>&lt;</span> it; cnt<span style=color:#f92672>++</span>) {
	start<span style=color:#f92672>=</span>clock();
        j<span style=color:#f92672>=</span>tex1Dfetch(tex_ref, j);
	s_value[cnt] <span style=color:#f92672>=</span> j;
			
	end<span style=color:#f92672>=</span>clock();			
	s_tvalue[cnt] <span style=color:#f92672>=</span> (end <span style=color:#f92672>-</span>start);
}
</code></pre></div><p>However, if texture L1 cache is greater than the shared memory allowed for each thread block, then some reads in the second loop will trigger cache misses. But such misses are in fact cold misses, not misses caused after the texture L1 cache is completely filled up. One solution is increase iteration to a much larger number so that the first iteration will always fill up the texture L1 cache. Note that if you move the data
preheat out such as</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> cnt<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>; cnt <span style=color:#f92672>&lt;</span> it; cnt<span style=color:#f92672>++</span>) {
        tmp<span style=color:#f92672>=</span>tex1Dfetch(tex_ref, tmp);
}
</code></pre></div><p>The compiler can optimize this whole step out and thus nothing actually gets executed.</p><p>After we run the modified code, the result shows that the we the cache missed starts when we set our array size to 6145, indicating the texture L1 cache can hold 6144 ints, which is equivalent to 24 kb. We also notice that each miss is followed by 7 consecutive hits. This means the cache line size is 8 words(b = 32 bytes). The structure of the L1 TLB is shown below, notice there are 192 lines in each set:</p><table><thead><tr><th>Set1</th><th style=text-align:center>Set2</th><th style=text-align:right>Set3</th><th style=text-align:right>Set4</th></tr></thead><tbody><tr><td>1-8</td><td style=text-align:center>33-40</td><td style=text-align:right>65-72</td><td style=text-align:right>97-104</td></tr><tr><td>9-16</td><td style=text-align:center>41-48</td><td style=text-align:right>&mldr;</td><td style=text-align:right>&mldr;</td></tr><tr><td>17-24</td><td style=text-align:center>46-56</td><td style=text-align:right>&mldr;</td><td style=text-align:right>&mldr;</td></tr><tr><td>25-32</td><td style=text-align:center>57-64</td><td style=text-align:right>89-96</td><td style=text-align:right>121-128</td></tr><tr><td>129-136</td><td></td><td></td><td></td></tr><tr><td>&mldr;</td><td style=text-align:center>&mldr;</td><td style=text-align:right>&mldr;</td><td style=text-align:right>&mldr;</td></tr><tr><td>2969-2976</td><td style=text-align:center>3001-3008</td><td style=text-align:right>3033-3040</td><td style=text-align:right>3065-3072</td></tr></tbody></table><p>According to CUDA documentation, GK110 adds the ability for read-only data in global memory to be loaded through the same cache used by the texture pipeline via a standard pointer without the need to bind a texture beforehand and without the sizing limitations of standard textures. The read-only data cache is loaded by calling __ldg(const restricted * address). We modified the code used to test texture L1 cache. The basic logic remains the same. When the arrays size is set to 6144 integers no cache misses occur with stride set as 32 (s=32 bytes). As soon as we increased one more element in the array cache misses start occurring. This shows the read-only cache is 24kb. We then noticed that the misses occur in a group of either 4 or 8. We infer the cache line to be 32 bytes and the replacement policy is LRU, same as Maxwell. We we increase the array to include 6248 elements(6144+32<em>3+8, 6144 is the max capacity of the cache, 32 consecutive number in a set, 32</em>3 to cause cache miss in set1, set2, and set3, only need to include 8 more to cause cache miss in set4 since s=32bytes), no caches hits occur. Therefore, we infer the caches set number to be 4, each cache line is 32 bytes, and each set contains 192 cache lines, the same as the texture L1 cache. The memory mapping seems arbitrary because the hit and miss patterns didn&rsquo;t follow that of the texture L1 cache.</p></p></section></article><footer class=page-footer><nav class=pagination><a class=next href=https://www.bodunhu.com/blog/tags/mm/page/2/>Next Page »</a></nav></footer></main><footer class=footer><span>&copy; 2021 <a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>