<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>mlsys on std::bodun::blog</title><link>https://www.bodunhu.com/blog/tags/mlsys/</link><description>Recent content in mlsys on std::bodun::blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 30 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.bodunhu.com/blog/tags/mlsys/index.xml" rel="self" type="application/rss+xml"/><item><title>TensorIR Transformation</title><link>https://www.bodunhu.com/blog/posts/tensorir-transformation/</link><pubDate>Tue, 30 Aug 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/tensorir-transformation/</guid><description>In the previous post, we&amp;rsquo;ve explored how to write primitive functions in TensorIR. Here, we will see how to transform TensorIR into other (potentially more performant) variants. The content is drived from the mlc course taught by Tianqi Chen.
Batched BMM ReLu A batched matrix multiplication followed by a ReLu operation can be expressed using numpy as:
def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray): Y = np.empty((16, 128, 128), dtype=&amp;#34;float32&amp;#34;) for n in range(16): for i in range(128): for j in range(128): for k in range(128): if k == 0: Y[n, i, j] = 0 Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j] for n in range(16): for i in range(128): for j in range(128): C[n, i, j] = max(Y[n, i, j], 0) Translating the numpy code into TensorIR we get:</description></item><item><title>Dive into TensorIR</title><link>https://www.bodunhu.com/blog/posts/dive-into-tensorir/</link><pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/dive-into-tensorir/</guid><description>TensorIR is a compiler abstraction for optimizing programs with tensor computation primitives in TVM. Imagine a DNN task as a graph, where each node represents a tensor computation. TensorIR explains how each node/tensor computation primitive in the graph is carried out. This post explains my attempt to implement 2D convolution using TensorIR. It is derived from the Machine Learning Compilation course offered by Tianqi Chen.
Implement 2D Convolution 2D convolution is a common operation in image processing.</description></item><item><title>Pathways: Google's New ML System</title><link>https://www.bodunhu.com/blog/posts/pathways-googles-new-ml-system/</link><pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/pathways-googles-new-ml-system/</guid><description>Google recently released the paper about its new ML system called &lt;a href="https://arxiv.org/pdf/2203.12533.pdf">Pathways&lt;/a>. I&amp;rsquo;m a bit surprised since I expect it to introduce a brand new model architecture. In fact, this paper is not easy to digest at all. I feel like it&amp;rsquo;s written for people who spent many years developing ML frameworks. Anyway, we will try to understand why it is developed and how it works. Also, you should check this &lt;a href="https://zhuanlan.zhihu.com/p/495592456">post&lt;/a> (in Chinese). This post explains many concepts in Pathways much more clearly. Many contents here are credited to this post.</description></item><item><title>FlexFlow</title><link>https://www.bodunhu.com/blog/posts/flexflow/</link><pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/flexflow/</guid><description>FlexFlow is a deep learning framework that discovers a fast parallelization strategy for distributed DNN training. It uses &lt;em>SOAP&lt;/em> (Sample-Operation-Attribute-Parameter) search space of parallelization strategies. in short, FlexFlow automates the parallelization of model training.</description></item><item><title>Maximum Likelihood for Classification</title><link>https://www.bodunhu.com/blog/posts/maximum-likelihood-for-classification/</link><pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/maximum-likelihood-for-classification/</guid><description>Let&amp;rsquo;s say we want to classify an input text \(y\) and give it a label \(x\). Formally, we want to find:
\[ \textrm{argmax} P(x | y) \]
By Bayes&amp;rsquo; rule this is the same as
\[ \textrm{argmax} \frac{P(y|x)P(y)}{P(x)} \]
Suppose we have five documents as training data and one document as the input as testing data. Our objective is to give a label to the test sentence.
Credit: Eunsol Choi Let&amp;rsquo;s define the probability of class as (\(N\) is the total number of classes)</description></item><item><title>Machine Learning System Resources</title><link>https://www.bodunhu.com/blog/posts/machine-learning-system-resources/</link><pubDate>Sat, 08 Jan 2022 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/machine-learning-system-resources/</guid><description>This is my personal list of resources related to machine learning systems. Feel free to drop me an email if you think there&amp;rsquo;s something worth mentioning. I will try to update this page frequently to include the most recent stuffs in mlsys. Resources Facebook&amp;rsquo;s external large-scale work NGC Container Doc Awesome-System-for-Machine-Learning: A curated list of research in machine learning systems (MLSys). Paper notes are also provided. Courses Deep Learning Systems:</description></item><item><title>In-Network Aggregation for Shared Machine Learning Clusters</title><link>https://www.bodunhu.com/blog/posts/in-network-aggregation-for-shared-machine-learning-clusters/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/in-network-aggregation-for-shared-machine-learning-clusters/</guid><description>This &lt;a href="https://proceedings.mlsys.org/paper/2021/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf">paper&lt;/a> by Nadeen appeared in MLSys 2021. It presents an in-network aggregation framework called &lt;em>PANAMA&lt;/em> for distributed ML training tasks.</description></item></channel></rss>