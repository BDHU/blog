<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>mlsys on std::bodun::blog</title><link>https://www.bodunhu.com/blog/tags/mlsys/</link><description>Recent content in mlsys on std::bodun::blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 31 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://www.bodunhu.com/blog/tags/mlsys/index.xml" rel="self" type="application/rss+xml"/><item><title>In-Network Aggregation for Shared Machine Learning Clusters</title><link>https://www.bodunhu.com/blog/posts/in-network-aggregation-for-shared-machine-learning-clusters/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/in-network-aggregation-for-shared-machine-learning-clusters/</guid><description>This paper by Nadeen appeared in MLSys 2021. It presents an in-network aggregation framework called PANAMA for distributed ML training tasks. PANAMA has two components: (1) an in-network hardware accelerator with support for floating-point gradient aggregation; (2) a domain-specific load-balancing and congestion control protocol.
Motivation ðŸ”—The primary motivation behind PANAMA is the data-parallel training (in which the neural network is replicated across \(N\) worker where each worker processes a subset of the training data) demands constant local gradient exchanging at every iteration, thus creating a huge amount of traffic.</description></item></channel></rss>