<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>paper-review on std::bodun::blog</title><link>https://www.bodunhu.com/blog/tags/paper-review/</link><description>Recent content in paper-review on std::bodun::blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 19 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://www.bodunhu.com/blog/tags/paper-review/index.xml" rel="self" type="application/rss+xml"/><item><title>SHEPHERD: Serving DNNs in the Wild</title><link>https://www.bodunhu.com/blog/posts/shepherd-serving-dnns-in-the-wild/</link><pubDate>Wed, 19 Apr 2023 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/shepherd-serving-dnns-in-the-wild/</guid><description>Paper link: SHEPHERD: Serving DNNs in the Wild
Achieving scalability, high system goodput and maximize resource utilization, at the same time is hard for an inference system.
while individual request streams can be highly unpredictable, aggregating request streams into moderately-sized groups greatly improves predictability, permitting high resource utilization as well as scalability
SHEPHERD&amp;rsquo;s main observation is</description></item><item><title>Paper Review - Dynamic Tensor Rematerialization</title><link>https://www.bodunhu.com/blog/posts/paper-review-dynamic-tensor-rematerialization/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/paper-review-dynamic-tensor-rematerialization/</guid><description>Dynamic Tensor Rematerialization (DTR) treats GPU memory as a large cache, where tensors can be evicted to save memory, and recomputed if needed later.
DTR&amp;rsquo;s eviction policy relies on the heuristic \(h\). The heuristic assigns a value \(h(t)\) to each resident tensor \(t\), approximating the cost of evicting the tensor. DTR evicts the tensor with the lowest cost based on the value of \(h\). \(h\) can factor in arbitrary metadata.</description></item><item><title>Paper Review - Capuchin: Tensor-based GPU Memory Management for Deep Learning</title><link>https://www.bodunhu.com/blog/posts/paper-review-capuchin-tensor-based-gpu-memory-management-for-deep-learning/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/paper-review-capuchin-tensor-based-gpu-memory-management-for-deep-learning/</guid><description>This paper aims to reduce GPU memory usage during DNN training. Capuchin achieves this goal though swapping and recomputation, using tensor as unit of operation. The major question is how to balance between swapping and recomputation to achieve max resource utilization.
Swap and Recomputation Benefit The ultimate goal of swapping and recomputation is to hide the overhead as much as possible to minimize the wait time of back-access (a tensor evicted earlier being accessed again).</description></item><item><title>Whiz: Data-Driven Analytics Execution</title><link>https://www.bodunhu.com/blog/posts/whiz-data-driven-analytics-execution/</link><pubDate>Sun, 05 Sep 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/whiz-data-driven-analytics-execution/</guid><description>This paper by &lt;a href="https://utns.cs.utexas.edu/">UTNS&lt;/a> lab appeared in &lt;a href="https://www.usenix.org/conference/nsdi21">NSDI 2021&lt;/a>. It presents a data-analytics framework that decouples intermediate data from computations</description></item><item><title>In-Network Aggregation for Shared Machine Learning Clusters</title><link>https://www.bodunhu.com/blog/posts/in-network-aggregation-for-shared-machine-learning-clusters/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://www.bodunhu.com/blog/posts/in-network-aggregation-for-shared-machine-learning-clusters/</guid><description>This &lt;a href="https://proceedings.mlsys.org/paper/2021/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf">paper&lt;/a> by Nadeen appeared in MLSys 2021. It presents an in-network aggregation framework called &lt;em>PANAMA&lt;/em> for distributed ML training tasks.</description></item></channel></rss>