<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>algorithm | std::bodun::blog</title><meta name=keywords content><meta name=description content="The Blog of Bodun Hu"><meta name=author content><link rel=canonical href=https://www.bodunhu.com/blog/tags/algorithm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="226272F73598D749583CF29B80850392"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.b6b54b0bc92f8d5200fc17cdbbca79c96b40f704117fd12ff3a8e6e0a4cb076f.css integrity="sha256-trVLC8kvjVIA/BfNu8p5yWtA9wQRf9Ev86jm4KTLB28=" rel="preload stylesheet" as=style><link rel=icon href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/favicon-32x32.png><link rel=apple-touch-icon href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/apple-touch-icon.png><link rel=mask-icon href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><link rel=alternate type=application/rss+xml href=https://www.bodunhu.com/blog/tags/algorithm/index.xml><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-108144808-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="algorithm"><meta property="og:description" content="The Blog of Bodun Hu"><meta property="og:type" content="website"><meta property="og:url" content="https://www.bodunhu.com/blog/tags/algorithm/"><meta property="og:image" content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="std::bodun::blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="algorithm"><meta name=twitter:description content="The Blog of Bodun Hu"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://www.bodunhu.com/blog/ accesskey=h title="std::bodun::blog (Alt + H)">std::bodun::blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button>
<a href=/blog/search/ title="Search (Alt + S)" accesskey=s><svg xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg></a></span></div><ul id=menu><li><a href=https://www.bodunhu.com/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://www.bodunhu.com/blog/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://www.bodunhu.com/blog/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://www.bodunhu.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://www.bodunhu.com/blog/tags/>Tags</a></div><h1>algorithm</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/hoarelogic/>Hoare Logic</a></h2></header><footer class=entry-footer>April 17, 2021&nbsp;·&nbsp;12 min</footer><section class=entry-content><p><p>Hoare logic forms the basis of all deductive verification. To illustrate Hoare logic, we will first consider a smaller imperative programming
language <strong>IMP</strong>.</p><p>In IMP, we have three program constructs: expressions, conditionals, and statements:</p><ul><li><p>Expression takes the form \( E := Z\ |\ V\ |\ e_1 + e_2\ |\ e_1 \times e_2 \)</p></li><li><p>Conditional is self-explanatory: \( C := true\ |\ false\ |\ e_1 = e_2\ |\ e_1 \leq e_2 \)</p></li><li><p>Statement consists of several different forms:</p><ul><li>\(S := V := E\) (Assignment)</li><li>\(S_1; S_2\) (Composition)</li><li>if \(C\) then \(S_1\) else \(S_2\) (If)</li><li>while \(C\) do \(S\) (While)</li></ul></li></ul><h2 id=hoare-triple>Hoare Triple</h2><p>In Hoare logic, we specify partial correctness of programs using Hoare triples:</p><p>\[\{P\} S \{Q\}\]</p><p>Here \(P\) is the precondition and \(Q\) is the post-condition. $S$ is a statement in IMP.</p><p>The interpretation of Hoare triple is as follows:</p><ol><li><p>if \(S\) is executed in a state satisfying \(P\)</p></li><li><p>and if execution of \(S\) terminates</p></li><li><p>then the program state after \(S\) terminates satisfies \(Q\)</p></li></ol><p>Here an example, \(\{x = 0 \} while\ true\ do\ x := 0\ \{x = 1 \}\) is a valid Hoare triple because the execution of the statement never terminates, thus satisfying the requirement posed by Hoare triple.</p><p>Thus the specification \(\{P\} S \{Q\}\) is called <em>partial</em> correctness spec, because it doesn&rsquo;t require \(S\) to terminate.</p><p>There is also a stronger requirement called <em>total</em> correctness. The total correctness specification is written as:</p><p>\[ [P] S [Q]\]</p><p>Total correctness requires that if \(P\) is satisfied when executing \(S\), then \(S\) must terminate, and the post-conditional \(Q\) must be satisfied after \(S\) terminates.</p><p>Thus the example \(\{x = 0 \} while\ true\ do\ x := 0\ \{x = 1 \}\) is no longer valid because it never terminates.</p><p>In summary, we can say that Total correctness \(=\) Partial correctness \(+\) termination.</p><h2 id=proving-partial-correctness>Proving Partial Correctness</h2><p>We use \(\vDash \{P\} S \{Q\} \) to say a Hoare triple is valid and we use \(\vdash \{P\} S \{Q\} \) to indicate we can prove validity of a Hoare triple.</p><p>Let&rsquo;s say we are given an assignment \(x := y \) with post-condition \(x > 2\). The question is, what do we need to know before the assignment happens so that the post-condition, \(x > 2\), holds afterwards?</p><p>To prove \(Q\) holds after the assignment \(x := E\), we need to show that <strong>\(Q\) with \(E\) substituting \(x\) holds before the assignment</strong>. Formally, we write it as:</p><p>\[\vdash \{Q[E / x]\}\ x := E \{Q\}\]</p><p>For example, given \( \{ x+1 = n\}\ x := x+1 \ \{x=n\} \), we know this formula is provable because we can take \(Q\), which is \(\{x=n\}\), substituting \(x\) with \(x+1\) given we need to replace it with \(E\), and we will convert \(x=n\) to \(x+1 = n\), which matches the precondition.</p><p>Here is another interesting example, given \( \{z = 2\}y:= x \{y = x\} \), this Hoare triple is valid but not provable. If we use the above substitution procedure, it will result in the precondition being \(x=x\) which is always true but is also different from the original precondition \(z=2\).</p><p>Intuitively, we can prove the post-condition \(y = x\) given the statement \(y = x\) without any assumptions, so even if we do have assumptions like \(z=2\), we should still be able to prove it, and here comes proof rule for precondition strengthening.</p><h2 id=proof-rule-for-precondition-strengthening>Proof Rule for Precondition Strengthening</h2><p>Formally, we define precondition strengthening as:</p><p>\[ \frac{ \vDash \{P'\} S \{Q\}\ \ P \Rightarrow P' }{\vdash \{P\} S \{Q\}} \]</p><p>Now, with the original formula \( \{z = 2\}y:= x \{y = x\} \), we would derive \( x= x \equiv true \). and since \(z=2 \rightarrow true\) is valid, we can now prove the formula!</p><h2 id=a-dual-post-condition-weakening>A Dual: Post-Condition Weakening</h2><p>Formally, we define post-condition weakening as:</p><p>\[ \frac{ \vDash \{P\} S \{Q'\}\ \ Q' \Rightarrow Q }{\vdash \{P\} S \{Q\}} \]</p><p>What this means if that if we can prove a post-condition \(Q'\), we can always relax it to something <strong>weaker</strong>.</p><p>For example, given that \(\vdash \{true\}S\{x=y \land z=2\}\), we can prove \(\{true\}S\{x=y\}\) because \(x=y\) is a weaker condition of \( x=y \land z=2 \).</p><h2 id=proof-rule-for-composition>Proof Rule for Composition</h2><p>For composition, we define the rule as:</p><p>\[ \frac{ \vdash \{P\}S_1\{Q\}\ \ \vdash \{Q\}S_2 \{R\} }{ \vdash \{P\}S_1;S_2\{R\} }\]</p><p>I won&rsquo;t show why this is true, so this will be left as an exercise.</p><h2 id=proof-rule-for-if-statements>Proof Rule for If Statements</h2><p>Naturally, we define the rule for if statement as:</p><p>\[ \frac{_{ \vdash \{P \land C\} S_1 \{Q\} }^{ \vdash \{P \land \neg C\} S_2 \{Q\} }}{ \vdash \{P\}\ if\ C\ then\ S_1\ else \ S_2 \ \{Q\} } \]</p><p>In summary, this means given we know \(P\) is true, no matter what \(C\) evaluates to, we will come to the same post-condition \(Q\). If you still don&rsquo;t understand it, just stare at it for five minutes and you should figure out why this is the case:)</p><h2 id=proof-rule-for-while>Proof Rule for While</h2><p>To understand the proof rule for while statement, we need to first understand a simple concept: loop invariant</p><h3 id=loop-invariant>Loop Invariant</h3><p>Loop invariant \(I\) has two properties:</p><ol><li><p>\(I\) holds initially before the loop</p></li><li><p>\(I\) holds after each loop iteration</p></li></ol><p>For example, given a loop</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>i := 0;
j := 0;
n := 0;
while i &lt; n do
    i := i + 1;
    j := i + j
</code></pre></div><p>Here, \(i \leq n \) is a loop invariant but \(i &lt; n \) isn&rsquo;t.</p><p>Now, we put the properties of loop invariant \(I\) in formal terms. Given that the precondition before a loop executes is \(C\), by definition, \(I\) holds initially before the loop, we know \(I \land C\) holds.</p><p>For the second property of loop invariant, it specifies \(I\) holds after each loop iteration. So that means \(\{ I \land C\ \} S \{I\} \) holds. Formally, we express loop invariant as \( \vdash \{P \land C\} S \{P\} \).</p><p>Now, we know if a loop terminates, it must be that condition \(C\) no longer holds, meaning \( P \land \neg C \) must be true after loop terminates. This is because \(P\) is a loop invariant and always holds after each loop iteration, including termination.</p><p>Putting all this together, we form the proof rule for while loop:</p><p>\[ \frac{ \vdash \{P \land C\} S \{P\} }{ \vdash \{P\} while \ C \ do \ S\{P \land \neg C\} }\]</p><h3 id=inductive-loop-invariant>Inductive Loop Invariant</h3><p>It&rsquo;s not always the case that we can prove loop invariant is valid. Here is a counter example:</p><p>Consider precondition \( I = j \geq 1 \) and the code is:</p><p>\[i := 1; j := 1; while \ i &lt; n\ do\ \{j := j+i; i ;= i + 1\}\]</p><p>We know that the precondition is \(I = j \geq 1\) and \(C\) (loop condition) is \(i \leq n\). So we have a Hoare triple:</p><p>\[ \{ j \geq 1 \land i \leq n \} j =j + i;\ i = i + 1; \ \{j \geq 1\} \]</p><p>We could simply set \(i = -100\), then if we execute the code once we will not be sure if the post-condition \(j \geq 1\) holds.</p><p>However, if we have <strong>strengthened invariant</strong> such as \(j \geq 1 \land i \geq 1\), the new Hoare triple will be valid. Then \(I\) will become inductive invariant because we can prove these invariant.</p><p>To put everything in action, here is an example showing how to find inductive loop invariant to prove the following Hoare triple:</p><p>\[ \{i = o \land j = o \land n = 5\} \]
\[while\ i &lt; do\ i := i + 1; \ j := j + i; \]
\[\{j = 15\} \]</p><p>If we have \( j = \frac{i(i+1)}{2} \), this is a loop invariant because we can prove that:</p><p>\[\{j = \frac{i(i+1)}{2} \land i &lt; n\} i = i + 1;\ j = j+ i\ \{j = \frac{i(i+1)}{2}\} \]</p><p>If we conjoin this condition with \(i \geq n\) as the post-condition, however, we can&rsquo;t really show that \(j = 15\) is true for the given Hoare triple.</p><p>If we also add condition \(n = 5\) and \(i \leq n\), and we conjoin this with the end-loop condition \( i \geq n\), we would realize that \( i = n = 5\), and thus prove that \(j = 15\) for the given Hoare triple.</p><p>How we get \(j = \frac{i(i+1)}{2}\) is, however, not trivial to solve, and requires some human effort in program verification.</p><h2 id=basic-idea-behind-program-verification>Basic Idea behind Program Verification</h2><h3 id=automating-reasoning-in-hoare-logic>Automating Reasoning in Hoare Logic</h3><p>It&rsquo;s reasonable to automate the tedious parts of program verification: proving correctness. The basic idea to assume an oracle (human or another program) gives loop invariants but automate the rest of the reasoning.</p><p>Automating Hoare logic is based on generating verification conditions (VC). Essentially, a verification condition is formula \(\phi\) s.t. program is correct iff \(\phi\) is valid.</p><p>There are two way to generate verification conditions: forwards and backwards.</p><p>As their name suggests, a forwards analysis starts from precondition and generates formulas to prove post-condition. Forwards technique computes <strong>strongest post-conditions (sp)</strong>. In contrast, backwards analysis starts from post-condition and tries to prove precondition. Backwards technique computes <strong>weakest preconditions (wp)</strong>.</p><p>Here, we start from backwards method.</p><h3 id=weakest-preconditions>Weakest Preconditions</h3><p>Formally, we define the weakest precondition of \(Q\) with respect to \(S\) as \(wp(S, Q)\).</p><p>\(wp(S, Q)\) has the property that it is the weakest condition (least amount of information we need to have) that guarantees \(Q\) holds after \(S\) in any execution.</p><p>Thus, Hoare triple \( \{P\}S\{Q\} \) is valid iff \( P\Rightarrow wp(S, Q) \).</p><p>Weakest preconditions are defined inductively and follow Hoare&rsquo;s proof rules:</p><ul><li><p>\(wp(x := E, Q) = Q[E/x]\)</p></li><li><p>\( wp(s_1 ; s_2, Q) = wp(s_1, wp(s_2, Q) ) \)</p></li><li><p>\(wp(if \ C\ then \ s_1\ else \ s_2, Q) =C \rightarrow wp(s_1, Q) \land \neg C \rightarrow wp(s_2, Q) \)</p></li></ul><p>However, for loops, we might not be able to compute the weakest preconditions exactly because there might be cases where we simply don&rsquo;t know the number loops executed.</p><p>Thus, we relax our requirement by computing \(awp(S,Q)\) (\(a\) stands for approximate)) instead, hoping that \(awp(S, Q)\) is weak enough to be implied by \(P\) although it may not be the weakest.</p><p>Now, assume all loops are annotated with invariants \(while \ C \ do \ [I]\ S\), we will just define \(awp(while \ C \ do \ [I]\ S, Q) \equiv I\).</p><p>However, there is another program, since \(awp\) is only an approximated condition, it doesn&rsquo;t necessarily mean that if \(P \Rightarrow awp(S, Q)\), \( \{P\}S\{Q\} \) is valid. There are two reasons:</p><ol><li><p>We don&rsquo;t know if the loop invariant \(I\) provided by the oracle is correct since it might be provided by human and we know human make mistakes.</p></li><li><p>Even if \(I\) is correct, we don&rsquo;t if \(I \land \neg C\) is sufficient to establish \(Q\)!</p></li></ol><p>Thus, for each statement \(S\), we need to generate verification condition (VC) \( VC(S,Q) \) which encodes additional conditions to prove.</p><h3 id=verification-conditions>Verification Conditions</h3><p>So how do formulate VC generation rules for loops?</p><p>\[ VC(while\ C\ do\ [I]\ S,Q) = ?\]</p><p>First, we need to ensure that \(Q\) is satisfied after loop, which means \( I \land \neg C \Rightarrow Q \).</p><p>To show that \(I\) is actually correct, we also need \( \{I \land C\} S \{I\} \).</p><p>This implies that we need to show \( I \land C \Rightarrow awp(S, I) \). In case \(S\) contains nested loops, and also add \(VC(S, I)\)</p><p>In summary, to how that loop invariant \(I\) provided by the oracle is correct, we need to show \( I \land C \Rightarrow awp(S,I) \land VC(S, I) \).</p><p>To show \(I\) is strong enough to establish \(Q\), we need to show \( I \land \neg C \Rightarrow Q \).</p><p>Putting this together, and to answer the two reason why \(P \Rightarrow awp(S, Q)\), \( \{P\}S\{Q\} \) might not be valid, VC for a while loop \( S' = while \ C \ do \ \{I\} \) is expressed as:</p><p>\[ VC(S', Q) = (I \land C \Rightarrow awp(S, I) \land VC(S, I) ) \land (I \land \neg C \Rightarrow Q) \]</p><p>In essence, verification condition simply stands for additional checks we need to verify before we can claim that, if an approximated precondition \(P\) is valid, \( \{P\} S \{Q\} \).</p><p>The verification condition for other statements is as follows:</p><ol><li><p>For assignment, we don&rsquo;t need any additional checks for precondition because if \( P \Rightarrow wp(S, Q) \), it implies that \( \{P\} S \{Q\} \) is valid. Thus, \( VC(x:= E, Q) = true \).</p></li><li><p>For composition, we have \( VC(s_1 ; s_2, Q) = VC(s_2, Q) \land VC(s_1, awp(s_2 , Q)) \).</p></li><li><p>For if statement, we have \( VC(if \ C \ then \ s_1\ else \ s_2, Q) = VC(s_1, Q) \land VC(s_2, Q) \).</p></li></ol><blockquote><p>Quick question: for if statement, why don&rsquo;t we instead use verification condition generation rule: \( C \Rightarrow VC(s_1, Q) \land \neg C \Rightarrow VC(s_2, Q) \)?</p></blockquote><p>Here is a counter example. Suppose we have \( S = if\ (x > 0) \ while (*) x - -; else \ skip\), and we have given loop invariant \(x \geq 0\).</p><p>If we use the original rule \( VC(s_1, Q) \land VC(s_2, Q) \), according to the verification condition generation rule for while loop, we would have to verify the loop invariant \(I\) is correct, and thus \(VC(S, I) \equiv \{ x \geq 0 \} x - - \{ x \geq 0 \} \), obviously, this not true, and we can use this VC.</p><p>However, if we instead use the rule \( C \Rightarrow VC(s_1, Q) \land \neg C \Rightarrow VC(s_2, Q) \). The VC would become \( x > 0 \Rightarrow (\{ x \geq 0 \} x - - \{ x \geq 0 \}) \), which is valid, and we will include the wrong VC. Thus we can&rsquo;t use this VC generation rule.</p><h2 id=verification-of-hoare-triple>Verification of Hoare Triple</h2><p>Thus, to show validity of Hoare triple \( \{P\} S \{ Q \} \), we need to compute:</p><ol><li><p>\( awp(S, Q) \)</p></li><li><p>\( VC(S, Q) \)</p></li></ol><p>Therefore, a Hoare triple is valid if the follow formula holds:</p><p>\begin{equation}\tag{*}
VC(S, Q) \land P \rightarrow awp(S, Q)
\end{equation}</p><p>Thus, if we can prove the validity of the above equation *, we know the program obeys specification.</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/congruence_closure/>Congruence Closure</a></h2></header><footer class=entry-footer>March 27, 2021&nbsp;·&nbsp;4 min</footer><section class=entry-content><p><p>This is a summary of how to compute congruence closure. I implemented the algorithm to compute congruence closure and thought I&rsquo;d never forget it. But my memory starts to get blurry just after two days. So I figured I&rsquo;d put things down so I don&rsquo;t have to watch the entire lecture again the next time I need it.</p><h2 id=equivalence-relation>Equivalence Relation</h2><p>Equivalence relation has three properties: reflexive, symmetric, and transitive.
(E.g. \(\geq\) is not an equivalence relation because it break the symmetric property. $4 \geq 6$ does not imply that $6 \geq 4$)
For example, a binary relation $R$ over a set $S$ meeting these three properties can be expressed as:</p><ul><li>Reflexive: $\forall s \in S.\ sRs$</li><li>Symmetric : $\forall s_1, s_2 \in S.\ s_1 R s_2 \rightarrow s_2 R s_1$</li><li>Transitive: $\forall s_1, s_2, s_3 \in S.\ s_1 R s_2 \land s_2 R s_3 \rightarrow s_1 Rs_3$</li></ul><h2 id=congruence-relation>Congruence Relation</h2><p>Given a set $S$ equipped with functions $F = {f_1, &mldr;, f_n}$, a relation $R$ over $S$ is a congruence relation if $R$ is an equivalence relation and for every $n$&lsquo;ary function $f \in F$ we have:</p><p>\[\forall \overset{\rightarrow}{s}, \overset{\rightarrow}{t}.\ \bigwedge\limits_{i=1}^{n}s_i R t_i \rightarrow f(\overset{\rightarrow}{s}) R f(\overset{\rightarrow}{t})\]</p><p>A counter example would be given $R(x, y)$ defined as $|x| = |y|$ on all integers. If we have $R = {2, 2}$ and $f(x) = x + 1$ (successor function), then we know it violates the equivalence relation we mentioned above</p><h2 id=equivalence-closure>Equivalence Closure</h2><p>In short, the equivalence closure $R^E$ is the smallest equivalence relation that includes $R$.
This is illustrated through an example. Given a set $S = {a, b, c}$ and binary relation $R:{\langle a, b \rangle , \langle b, c \rangle, \langle d, d \rangle}$, $R^E$ would contain all elements extended from $R$ based on the three properties of equivalence relation.</p><h2 id=congruence-closure>Congruence Closure</h2><p>Naturally, congruence closure $R^C$ would be the smallest set that contains congruence relation $R$. What this means is $R^C$ contains $R^E$ (the equivlance closure we derived before), and any element generated from $R^E$ by a given function that produces element which also satisfies equivelance relation. For example, Given $S = {a, b, c}$ and function $f$ such that $f(a) = b$, $f(b) = c$, $f(c) = c$, the congruence closure would contain nine elments in total. First, we
would use the procedure above to generated equivalence closure. Then, because $f(a) = b$ and $f(b) = c$ due to congruence relation, we know $b = c$, now we apply the procure for generating equivalence closure again.</p><h2 id=algorithm-to-compute-congruence-closure>Algorithm to Compute Congruence Closure</h2><p>The high-level description of the algorithm is as following:</p><p>To decide satisfiability of $T_{=}$ (equality theory) formula:</p><p>\[F\ : \ s_1 = t_1 \land &mldr; s_m = t_m \land s_{m+1} \neq t_{m+1} \land &mldr; s_n \neq t_n\]</p><ol><li>Compute subterms and construct initial DAG (each node’s representative is itself)</li><li>For each $i \in [1,m]$, process equality $s_i= t_i$ as described. (Essentially, process all equiv expression first)</li><li>For each $i \in [m + 1,n]$, check if $Rep(s_i) =Rep(t_i)$. (Check if any nequiv expression contradicts any equiv expression)</li><li>If there exists some $i \in [m + 1, n]$, for which $Rep(s_i) =Rep(t_i)$, return UNSAT</li><li>if for all $i$, $Rep(s_i) \neq Rep(t_i)$, return SAT</li></ol><p>This is an example for illustration purpose borrowed from Prof. Dillig&rsquo;s slides:</p><p>Given formula $F\ : \ f^3(a) = a \land f^5(a) = a \land f(a) \neq a$</p><p>The initial DAG would be:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/congruence_algorithm/DAG.png></p><p>Process equality $f^3(a) = a$ gives us:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/congruence_algorithm/DAG_1.png></p><p>Recursively merging the parents results in:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/congruence_algorithm/DAG_2.png></p><p>Process equality $f^5(a) = a$ gives us:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/congruence_algorithm/DAG_3.png></p><p>Now in this step, $f^2(a)$ and $a$ are in the same congruence class, thus we will perform the same operation on their parents, processing equality $f^3(a) = f(a)$:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/congruence_algorithm/DAG_5.png></p><p>We find $f(a) \neq a$ has a conflict because node $a$&rsquo;s representative is $f(a)$, indicating they are in the same congruence class, meeting congruence relation.
Thus the formula is UNSAT.</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/wireless_anomaly/>Performance Anamoly of 802.11b</a></h2></header><footer class=entry-footer>September 13, 2020&nbsp;·&nbsp;5 min</footer><section class=entry-content><p><p>This research is conducted by Martin Heusse, Franck Rousseau, Cilles Berger-Sabbatel, Andrzej Duda on analyzing the performance of the IEEE 802.11b
wireless local area networks. Degraded transmitting rate is caused by CSMA/CA channel access method.</p><h2 id=overview>Overview</h2><p>The performance of the IEEE 802.11b wireless local area networks have degraded performances when some mobile hosts use a lower bit rate than the others, which is caused by CSMA/CA channel access method. When one host changes it modulation type which degrades bit rate, it occupies the channel for a longer time, causing other hosts still using higher bit rate to be penalized. The paper <a href=https://ieeexplore.ieee.org/document/1208921>Performance Anamoly of 802.11b</a> analyzes how such anomaly works.</p><h2 id=transmission-overhead>Transmission Overhead</h2><p>Consider there is only a single host in a 802.11b cell transmitting a single data frame. The overall transmission time is expressed as:</p><p>$$T = t_{tr} + t_{ov}$$</p><p>where the constant overhead</p><p>$$t_{ov} = DIFS + t_{pr} + SIFS + t_{pr} + t_{ack}$$</p><p>The transmission process can be represented by the graph</p><p align=center><a href=https://ieeexplore.ieee.org/document/1208921><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/ieee_anomaly/transmission_single_frame.png width=80%></a></p><p>When there are multiple hosts attempting to transmit, a host will execute the exponential backoff algorithm - it waits for a random interval to avoid saturating the channel, resulting in extra time spent in the contention procedure:</p><p>$$T = t_{tr} + t_{ov} + t_{cont}(N)$$</p><p>Finally, the useful throughput obtained by a host depends on the number of hosts:</p><p>$$p(n) = t_{tr} / T(N)$$</p><p>This indicates the useful throughput is smaller than the nominal bit rate and largely depends on the number of competing hosts.</p><h2 id=anomaly>Anomaly</h2><p>Assume there are \(N\) hosts, \(N-1\) hosts use high transmission rate \(R=11\)Mb/s, one hosts transmits at rate \(r=5.5\), \(2\), or \(1\) Mb/s. We can deduce the transmission time of the fast ones:</p><p>$$T_f = t_{ov}^{R} + \frac{s_d}{R} + t_{cont}$$</p><p>The transmission time of the slow host is:</p><p>$$T_s = t_{ov}^{R} + \frac{s_d}{r} + t_{cont}$$</p><p>The short term behavior of CSMA/CA is shown to be not fair, thus we have</p><p>$$U_f = \frac{T_f}{(N-1)T_f + T_s + P_c(N)\times t_{jam} \times N}$$</p><p>\(t_{jam}\) is the average time spent in collisions, calculated between the all possible pairs between the fast hosts and the slow one:</p><p>$$t_{jam} = \frac{2}{N}T_s + (1 - \frac{2}{N})T_f$$</p><p>The throughput at he MAC layer of each fast hosts is:</p><p>$$X_f = U_f \times p_f(N) \times R$$</p><p>given that:</p><p>$$p_f(N) = \frac{s_d}{RT_f}$$</p><p>We apply the same process for the slow host, given \(p_s(N) = \frac{s_d}{rT_s}\), what we get eventually is:</p><p>$$X_f=X_s = X$$</p><p><em>This key point here is that the fast hosts transmitting at the higher rate R obtain the same throughput as the slow host transmitting at the lower rate.</em></p><h2 id=simulation-and-measurement-results>Simulation and Measurement Results</h2><p>In general, the experimental value of \(P_c(N)\) seems to match the theory model. One thing the paper could illustrates better is to show how experimental value matches the equation as the number of hosts increases. The average and cumulative throughput value also seems reasonable compared to the expression discussed before.</p><p>The throughput is measured using three different tools: <em>netperf</em>, <em>tcpperf</em>, and <em>udpperf</em>. This idea of duplication makes the data collected more reliable and persuasive, which is especially useful in benchmarking since the results can be sensitive to environmental variable changes.</p><p>The presented results justify the statement made in the paper. For example, the measured TCP throughput for two hosts is shown to degrade as time passes:</p><p align=center><a href=https://ieeexplore.ieee.org/document/1208921><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/ieee_anomaly/TCP_degrade.png width=80%></a></p><p>One thing the paper can articulate more is how this seemly periodic pattern is related to the model. Another concern is the number of device used to conduct these experiments. The number of devices used seems to be much smaller than what would be in real-world scenario. It will be interesting to see how the performances are affected with a lot devices competing for a channel. This can be further extended to measuring performances with multiple devices having lower bit rate, which is more likely to capture real-world use cases. The potential performance impact is not clear given the present measurement.</p><p>The paper also claims the useful throughput strongly depends on the number of competing host. More data related to how the number of hosts is related to performance impact will make this paper more interesting. It may be hard to achieve as many papers resort to simulation.</p><p>This paper has made improvements over previous work in that it studies the performance of 802.11 WLANs, with one host having lower bit rate, whereas many other assume that all hosts communicate using the same bit rate. This is a step forward to capture more realistic situations. Overall, the paper does a good job in terms of proving its point. It captures the most critical information and it&rsquo;s easy to follow the concept. However, the neat structure can make readers without sufficient background to spend more time catching up since the background section may not be enough for starters.</p><h2 id=conclusion>Conclusion</h2><p>Overall, this paper brings novel approach to analyze the performance of 802.11 WLANs with varying bit rate. It brings new insights into studying the 802.11 standard. The paper focuses on TCP and UDP protocols. Applying the method discussed in paper to a lesser known protocol such as DCTCP can yield more insights into the different protocols can affect the throughput. Another direction is to generalize this model to multiple bit rate degrading and study their behaviors.</p><p>The bit rate used in the paper also seems to be pretty low compared to modern standards. With the introduction of 5G network, the bit rate becomes a lot higher, it will be interesting to see how extremely high bit rate can affect the performance of 802.11.</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/svd/>Singular Value Decomposition</a></h2></header><footer class=entry-footer>February 10, 2020&nbsp;·&nbsp;2 min</footer><section class=entry-content><p><p>Unitary matrices and the Singular Value Decomposition (SVD) are two important concepts in linear algebra. In order to fully understand these concepts, we will need to first discuss orthogonality. Most materials are converted in Advanced Linear Algebra: Foundations to Frontiers taught by professor <a href=https://www.cs.utexas.edu/~rvdg/>Robert van de Geijn</a>. This is a brief summary over the important concepts covered in Chapter 2.</p><h2 id=components-in-the-direction-of-a-vector>Components in the direction of a vector</h2><p>By Pythagorean theorem, we know that \(b = \chi a + c\) where \(a\) is a unit vector orthogonal to \(c\) and \(\chi\) is a scaler. Then we have</p><p>$$a^T (b-\chi a) = 0$$</p><p>Solving it gives us \(\chi = \frac{a^T b}{a^T a}\). We have \(\frac{a^T b}{a^T a}a = \frac{a a^T}{a^T a}b\). And \(\frac{a a^T}{a^T a}\) can map vector \(b\) in the direction of \(a\). The orthogonal component of \(a\) can thus be calculated as \(I-\frac{a a^T}{a^T a}\).</p><p>The linear transformation can be simplified by letting \(\left\lVert a\right\rVert_{2}=1\) because this will render \(a^T a = 1\).</p><h2 id=unitary-matrix>Unitary Matrix</h2><p>A matrix \(U\) is said to unitary matrix is if \(U\) is a square matrix and satisfies \(U^H U= I\).</p><p>In addition, unitary matrix has some nice properties. First, the product of a sequence of unitary matrix is also unitary matrix. This can be proven by first explore the product of \((U_0 U_1)^H (U_0 U_1)= I\), showing \(U_0 U_1\) is a unitary matrix, and then perform induction.</p><p>Unitary matrix also preserves <strong>length</strong>. This is done by showing \(\left\lVert Ux \right\rVert 2^2 = (Ux)^H (Ux) = x^H x= \left\lVert x \right\rVert _2^2\).</p><h2 id=change-of-orthonormal-basis>Change of orthonormal basis</h2><p>We mentioned we can map a vector \(x\) another vector in the same direction as vector \(a\). Now we extend it to express a vector \(x\) using a set of orthonormal basis \(U\).</p><p>We know that \(x = Ix= UU^Tx=U(U^Tx)=u_0^Hxu_0+&mldr;+u_{m-1}^Hxu_{m-1}\). We notice that \(u_0^Hx\) is a scalar so we can write then equation as \(U(U^Tx)=a_0u_0+&mldr;+a_{m-1}u_{m-1}\). We successfully expressed the vector \(x\) based on the orthonormal basis.</p><h2 id=todo>TODO</h2></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/probabilistic_clock_synchronization/>Understanding Probabilistic Clock Synchronization</a></h2></header><footer class=entry-footer>September 17, 2019&nbsp;·&nbsp;5 min</footer><section class=entry-content><p><p>This post is meant to discuss the probabilistic clock synchronization technique. The main goal of this technique is to bound the difference between systems by setting up an upper bound. In short,
\(|P(t)-Q(t)|\leq \varepsilon\). We will discuss what these symbols represent later.</p><h2 id=perfect-synchronization>Perfect Synchronization</h2><p>The motivation behind this technique is that synchronization always involves overheads. In a perfect environment where network delay and request processing time are both 0, the clocks can be synchronized with ease. A slave P will send &ldquo;Time = ?&rdquo; at global time
t
to master Q and master Q replies &ldquo;Time = Q(t)&rdquo; instantaneously at global time
t
. Then P will adjust its clock P(t) according to Q(t). However, such case only exists in imagination.</p><h2 id=amortization>Amortization</h2><p>Suppose the difference between the clock of P and Q is \(\Delta\) at synchronization, our goal is to adjust P&rsquo;s logical clock C(t) to mitigate the difference. The adjustment is simple:</p><p>\[C(t)=H(t)+A(t)\]</p><p>Here C(t) is P&rsquo;s logical clock, H(t) is P&rsquo;s hardware clock, and A(t) is the adjustment function(can also be A(H(t))).</p><p align=center><img src="https://raw.githubusercontent.com/BDHU/Page_pics/master/clock.png?token=ACKPLVNGE4DFY4GQF55PU7C5QFOGW" width=600></p><p>A naive method will be simply subtract or add
\(\Delta\) to C(t) to mitigate the difference. However, it will create a discontinuity in P&rsquo;s clock, which may disrupt systems services. For example, if
\(\Delta = 2\) seconds,the logical clock will instantly jump ahead 2 seconds and a stopwatch will skip one second.</p><p>So the adjustment function is as follows:</p><p>\[A(t)=m\cdot H(t)+N\]</p><p>Now the logical clock can be derived as follows:</p><p>\[C(t)=(1+m)\cdot H(t)+N\]</p><p>This process is called amortization.</p><p>However, how do we know the value for m and N? Let&rsquo;s take a look at the time when amortization process starts, the logical time of P at this moment is:</p><p>\[L=(1+m)\cdot H+N \qquad (1)\]</p><p>At the end of the amortization (lasts for time period \(\alpha\)) we have reached
\(M=H+\alpha\)
. Here M is the master logical clock sent by master Q. So at the end of the amortization, the slave P should be able to catch up with its master&rsquo;s logical clock after
\(\alpha\)
period of time. Therefore, we have:</p><p>\[M+\alpha = (1+m)(H+\alpha)+N \qquad (2)\]</p><p>Solving (1) and (2) together, we now get:</p><p>\[m = \frac{M-L}{\alpha}\]</p><p>\[N = L - (1+m)H\]</p><p>Thus, at the end of amortization at time t where
\(t > H+\alpha\)
, we would want the following to be true:</p><p>\[C(t)=C(H+\alpha)+(H(t)-H(H+\alpha))=H(t)+M-H\]</p><p>Here is a question, why is N required in this case. Couldn&rsquo;t we simply use m to amortize the time difference? Here&rsquo;s my interpretation(feel free to pin me if you have something else in mind): if N is set to be 0, then at the beginning of amortization, we would have:</p><p>\[L=(1+m)H\]</p><p>Therefore,
\(m = \frac{L-H}{H}\)
. Now, m is settled by L and H. Compared to
\(m=\frac{M-L}{\alpha}\)
, we can see that now m is a constant and not determined by the value of
\(\alpha\)
. We lost control of the amortization rate m, which is not desirable.</p><h2 id=general-case>General Case</h2><p>We now return to the general case where network delay and processing time are both present. The situation is represented below:</p><p align=center><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/general_case.png width=500></p><p>Looking at this graph, we can see slave P takes 2d real time to for a round-trip. Let&rsquo;s also assume that 2D is the round-trip delay measured by P&rsquo;s clock between sending and receiving. Then we can bound the clock time 2D based on the drift rate \(\rho\) of the clock:</p><p>\[2d(1-\rho)\leq 2D \leq2d(1+\rho)\]</p><p>Ignoring higher order terms of \(\rho\), we now have \(2d\leq(1+\rho)2D\).</p><p>When looking at the graph above, one thing to notice is we are not sure of the time \(\alpha\) and \(\beta\). However, if we are going to pick one, \(\beta\) will be more important than \(\alpha\). This is because if we know the value of \(\beta\), then we know the lower bound of the round-trip delay. Here we assume min is the minimum amount of time required for network transfer, \(\beta\) will be the time master Q spends between processing the request and responds the result back to P.</p><p>Now we&rsquo;ve narrowed down our focus to \(min+\beta\). The time interval between \(Q(t)=T\) and the arrival of &lsquo;&lsquo;Time=T&rsquo;&rsquo; at P will be at least \(min(1-\rho)\). This is based on \(\beta=0\) and clock drift rate.</p><p>The upper bound of the interval will be \((min+\beta)(1+\rho)\), assuming no time is wasted for \(Q\) to wait until it starts processing the request from P. The time required will be \(min+\beta\) and we need to take Q&rsquo;s drift rate \(\rho\) into account. We can also see that the total round-trip real time is \(2d=2min+\alpha+\beta\). Thus we get:</p><p>\[\beta=2d-2min-\alpha \leq 2d-2min\]</p><p>With this equation, we can see that the upper bound measured from Q(t)=T is also bounded. Thus, we have:</p><p>\[
\begin{eqnarray}
(min+\beta)(1+\rho) &\leq& (min+2d-2min)(1+\rho) \nonumber \newline
&=& (2d-min)(1+\rho) \nonumber \newline
&=&(1+\rho)2d-min(1+\rho) \nonumber \newline
&\leq&(1+\rho)2D(1+\rho)-min(1+\rho) \nonumber \newline
&=&(1+2\rho +\rho^2)2D-min(1+\rho) \nonumber \newline
&\approx&(1+2\rho )2D-min(1+\rho) \nonumber
\end{eqnarray}
\]</p><p>Now we can see that master Q&rsquo;s clock time when P receives the response is bounded in the interval \([T+min(1-\rho), T+2D(1+2\rho )-min(1+\rho)]\). The take away here is that we can&rsquo;t use real time t in a distributed system because it&rsquo;s merely an abstract concept since all systems in a network essentially rely on their own clock time. We need to find the relationship between T and master&rsquo;s clock cycle because P will rely on T, not real time t.</p></p></section></article></main><footer class=footer><span>&copy; 2021 <a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a> ·
<a href=https://github.com/BDHU rel=noopener>GitHub</a> ·
<a href=/blog/index.xml rel=noopener target=_blank>RSS</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>