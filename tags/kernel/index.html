<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>kernel | std::bodun::blog</title><meta name=keywords content><meta name=description content="The Blog of Bodun Hu"><meta name=author content><link rel=canonical href=https://www.bodunhu.com/blog/tags/kernel/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.758151a3f029732139c5ed917da59127bed78e4d7fce57610c1cd24d4cb52b2a.css integrity="sha256-dYFRo/ApcyE5xe2RfaWRJ77Xjk1/zldhDBzSTUy1Kyo=" rel="preload stylesheet" as=style><link rel=icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.utexas.edu/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.utexas.edu/favicon-32x32.png><link rel=apple-touch-icon href=https://www.utexas.edu/apple-touch-icon.png><link rel=mask-icon href=https://www.cs.utexas.edu/sites/default/files/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><link rel=alternate type=application/rss+xml href=https://www.bodunhu.com/blog/tags/kernel/index.xml><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-108144808-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="kernel"><meta property="og:description" content="The Blog of Bodun Hu"><meta property="og:type" content="website"><meta property="og:url" content="https://www.bodunhu.com/blog/tags/kernel/"><meta property="og:image" content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="std::bodun::blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="kernel"><meta name=twitter:description content="The Blog of Bodun Hu"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://www.bodunhu.com/blog/ accesskey=h title="std::bodun::blog (Alt + H)">std::bodun::blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.bodunhu.com/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://www.bodunhu.com/blog/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://www.bodunhu.com/blog/about/ title=About><span>About</span></a></li><li><a href=https://www.bodunhu.com/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://www.bodunhu.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://www.bodunhu.com/blog/tags/>Tags</a></div><h1>kernel</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/programloadingandmemorymappinginlinux/>Program Loading and Memory Mapping in Linux</a></h2></header><footer class=entry-footer>November 3, 2020&nbsp;·&nbsp;5 min</footer><section class=entry-content><p><p>The goal here is to familiarize yourself with how programs are loaded, dynamically paged, and some of the mechanics of signal handling and memory mapping in Linux.</p><h2 id=execve-syscall>execve Syscall</h2><p>The operating system, as one of itsd basic services, loads programs into memory for them to execute. Programs rely on <code>execve</code> syscall to get the OS to load the program into memory and start it executing as a process. The kernel version we used to testing is 5.4.0. Doing a quick search inside <a href=https://elixir.bootlin.com/linux/v5.4/source/fs/exec.c#L1956>Elixir</a> gives us:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>SYSCALL_DEFINE3(execve,
		<span style=color:#66d9ef>const</span> <span style=color:#66d9ef>char</span> __user <span style=color:#f92672>*</span>, filename,
		<span style=color:#66d9ef>const</span> <span style=color:#66d9ef>char</span> __user <span style=color:#f92672>*</span><span style=color:#66d9ef>const</span> __user <span style=color:#f92672>*</span>, argv,
		<span style=color:#66d9ef>const</span> <span style=color:#66d9ef>char</span> __user <span style=color:#f92672>*</span><span style=color:#66d9ef>const</span> __user <span style=color:#f92672>*</span>, envp)
{
	<span style=color:#66d9ef>return</span> do_execve(getname(filename), argv, envp);
}
</code></pre></div><p>Follow the function call, we will eventually reach the call to <code>__do_execve_file</code>, the comment of this function says &ldquo;sys_execve() executes a new program&rdquo;, which is pretty straighforward. This function first checks the <code>filename</code> pointer. Then it checks the flags of the current process that limit of running processes is not exceeded:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>if</span> (IS_ERR(filename))
		<span style=color:#66d9ef>return</span> PTR_ERR(filename);

<span style=color:#75715e>/*
</span><span style=color:#75715e> * We move the actual failure in case of RLIMIT_NPROC excess from
</span><span style=color:#75715e> * set*uid() to execve() because too many poorly written programs
</span><span style=color:#75715e> * don&#39;t check setuid() return code.  Here we additionally recheck
</span><span style=color:#75715e> * whether NPROC limit is still exceeded.
</span><span style=color:#75715e> */</span>
<span style=color:#66d9ef>if</span> ((current<span style=color:#f92672>-&gt;</span>flags <span style=color:#f92672>&amp;</span> PF_NPROC_EXCEEDED) <span style=color:#f92672>&amp;&amp;</span>
    atomic_read(<span style=color:#f92672>&amp;</span>current_user()<span style=color:#f92672>-&gt;</span>processes) <span style=color:#f92672>&gt;</span> rlimit(RLIMIT_NPROC)) {
    retval <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>EAGAIN;
    <span style=color:#66d9ef>goto</span> out_ret;
}

<span style=color:#75715e>/* We&#39;re below the limit (still or again), so we don&#39;t want to make
</span><span style=color:#75715e>    * further execve() calls fail. */</span>
current<span style=color:#f92672>-&gt;</span>flags <span style=color:#f92672>&amp;=</span> <span style=color:#f92672>~</span>PF_NPROC_EXCEEDED;
</code></pre></div><p>The next important task is to allocate the <code>struct linux_binprm</code> structure defined <a href=https://elixir.bootlin.com/linux/v5.4/source/include/linux/binfmts.h#L17>here</a>. This structure is used to hold the arguments that are used when loading binaries.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>bprm <span style=color:#f92672>=</span> kzalloc(<span style=color:#66d9ef>sizeof</span>(<span style=color:#f92672>*</span>bprm), GFP_KERNEL);
	<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>bprm)
		<span style=color:#66d9ef>goto</span> out_files;
</code></pre></div><p>Next, the function performs a seireis of tasks to prepare the <code>bprm</code> struct. Refer to the <a href=https://0xax.gitbooks.io/linux-insides/content/SysCall/linux-syscall-4.html>linux-insides</a> book to find more information on how exactly the <code>bprm</code> structure is filled up.</p><p>The most important function called by <code>__do_execve_file</code> is <code>search_binary_handler</code>. Based on the <a href=https://elixir.bootlin.com/linux/v5.4/source/fs/exec.c>comment</a>, this function cycles the list of binary formats handler, until one recognizes the image. We can find one section of the code surrounded by <code>binfmt_lock</code>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>list_for_each_entry(fmt, <span style=color:#f92672>&amp;</span>formats, lh) {
    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>try_module_get(fmt<span style=color:#f92672>-&gt;</span>module))
        <span style=color:#66d9ef>continue</span>;
    read_unlock(<span style=color:#f92672>&amp;</span>binfmt_lock);

    bprm<span style=color:#f92672>-&gt;</span>recursion_depth<span style=color:#f92672>++</span>;
    retval <span style=color:#f92672>=</span> fmt<span style=color:#f92672>-&gt;</span>load_binary(bprm);
    bprm<span style=color:#f92672>-&gt;</span>recursion_depth<span style=color:#f92672>--</span>;

    read_lock(<span style=color:#f92672>&amp;</span>binfmt_lock);
    put_binfmt(fmt);
    <span style=color:#66d9ef>if</span> (retval <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>&amp;&amp;</span> <span style=color:#f92672>!</span>bprm<span style=color:#f92672>-&gt;</span>mm) {
        <span style=color:#75715e>/* we got to flush_old_exec() and failed after it */</span>
        read_unlock(<span style=color:#f92672>&amp;</span>binfmt_lock);
        force_sigsegv(SIGSEGV);
        <span style=color:#66d9ef>return</span> retval;
    }
    <span style=color:#66d9ef>if</span> (retval <span style=color:#f92672>!=</span> <span style=color:#f92672>-</span>ENOEXEC <span style=color:#f92672>||</span> <span style=color:#f92672>!</span>bprm<span style=color:#f92672>-&gt;</span>file) {
        read_unlock(<span style=color:#f92672>&amp;</span>binfmt_lock);
        <span style=color:#66d9ef>return</span> retval;
    }
}
</code></pre></div><p>We can see it calls into <code>load_binary</code>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>retval <span style=color:#f92672>=</span> fmt<span style=color:#f92672>-&gt;</span>load_binary(bprm);
</code></pre></div><p>Here, the <code>load_binary</code> is a pointer in a <code>linux_binfmt</code> struct. For elf format, it can be found <a href=https://elixir.bootlin.com/linux/v5.4/source/fs/binfmt_elf.c#L94>here</a>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>struct</span> linux_binfmt elf_format <span style=color:#f92672>=</span> {
	.module		<span style=color:#f92672>=</span> THIS_MODULE,
	.load_binary	<span style=color:#f92672>=</span> load_elf_binary,
	.load_shlib	<span style=color:#f92672>=</span> load_elf_library,
	.core_dump	<span style=color:#f92672>=</span> elf_core_dump,
	.min_coredump	<span style=color:#f92672>=</span> ELF_EXEC_PAGESIZE,
};
</code></pre></div><p>We can find the <code>load_elf_binary</code> function defined in the <a href=https://elixir.bootlin.com/linux/v5.4/source/fs/binfmt_elf.c#L673><code>fs/binfmt_elf.c</code></a> file. Then the function will check the magic number in the ELF file header. You can find the ELF format from <a href=https://en.wikipedia.org/wiki/Executable_and_Linkable_Format>wiki</a>.
We can see for both 32-bit and 64-bit systems, the e-ident field should contain the magic number for ELF format files.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>/* Get the exec-header */</span>
loc<span style=color:#f92672>-&gt;</span>elf_ex <span style=color:#f92672>=</span> <span style=color:#f92672>*</span>((<span style=color:#66d9ef>struct</span> elfhdr <span style=color:#f92672>*</span>)bprm<span style=color:#f92672>-&gt;</span>buf);

retval <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>ENOEXEC;
<span style=color:#75715e>/* First of all, some simple consistency checks */</span>
<span style=color:#66d9ef>if</span> (memcmp(loc<span style=color:#f92672>-&gt;</span>elf_ex.e_ident, ELFMAG, SELFMAG) <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>)
    <span style=color:#66d9ef>goto</span> out;
</code></pre></div><p>Then, <code>load_elf_binary</code> will do some tasks to prepare for the executable file. After that, it will try to load the program header table:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>elf_phdata <span style=color:#f92672>=</span> load_elf_phdrs(<span style=color:#f92672>&amp;</span>loc<span style=color:#f92672>-&gt;</span>elf_ex, bprm<span style=color:#f92672>-&gt;</span>file);
<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>elf_phdata)
    <span style=color:#66d9ef>goto</span> out;
</code></pre></div><p>Then it will traverse the program header table and find the interpreter which is responsible of setting up the stack and map elf binary into the correct location in memory. After the interpreter is obtained, the function will perform simple consistency checks on the interpreter. It will load the interpreter program headers:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>/* Load the interpreter program headers */</span>
interp_elf_phdata <span style=color:#f92672>=</span> load_elf_phdrs(<span style=color:#f92672>&amp;</span>loc<span style=color:#f92672>-&gt;</span>interp_elf_ex,
                    interpreter);
<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>interp_elf_phdata)
    <span style=color:#66d9ef>goto</span> out_free_dentry;
</code></pre></div><p>This function will call <code>setup_arg_pages</code> to finalize the stack vm_area_struct:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>/* Do this so that we can load the interpreter, if need be.  We will
</span><span style=color:#75715e>    change some of these later */</span>
retval <span style=color:#f92672>=</span> setup_arg_pages(bprm, randomize_stack_top(STACK_TOP),
                executable_stack);
<span style=color:#66d9ef>if</span> (retval <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>)
    <span style=color:#66d9ef>goto</span> out_free_dentry;
</code></pre></div><p>It will also mmap the elf image into the correct location in memory. The bss and brk sections are prepared for the executable file:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>/* Now we do a little grungy work by mmapping the ELF image into
</span><span style=color:#75715e>    the correct location in memory. */</span>
<span style=color:#66d9ef>for</span>(i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, elf_ppnt <span style=color:#f92672>=</span> elf_phdata;
    i <span style=color:#f92672>&lt;</span> loc<span style=color:#f92672>-&gt;</span>elf_ex.e_phnum; i<span style=color:#f92672>++</span>, elf_ppnt<span style=color:#f92672>++</span>) {
        
        ...

        <span style=color:#75715e>/* There was a PT_LOAD segment with p_memsz &gt; p_filesz
</span><span style=color:#75715e>           before this one. Map anonymous pages, if needed,
</span><span style=color:#75715e>           and clear the area.  */</span>
        retval <span style=color:#f92672>=</span> set_brk(elf_bss <span style=color:#f92672>+</span> load_bias,
                    elf_brk <span style=color:#f92672>+</span> load_bias,
                    bss_prot);
        <span style=color:#66d9ef>if</span> (retval)
            <span style=color:#66d9ef>goto</span> out_free_dentry;
        nbyte <span style=color:#f92672>=</span> ELF_PAGEOFFSET(elf_bss);
        <span style=color:#66d9ef>if</span> (nbyte) {
            nbyte <span style=color:#f92672>=</span> ELF_MIN_ALIGN <span style=color:#f92672>-</span> nbyte;
            <span style=color:#66d9ef>if</span> (nbyte <span style=color:#f92672>&gt;</span> elf_brk <span style=color:#f92672>-</span> elf_bss)
                nbyte <span style=color:#f92672>=</span> elf_brk <span style=color:#f92672>-</span> elf_bss;
            <span style=color:#66d9ef>if</span> (clear_user((<span style=color:#66d9ef>void</span> __user <span style=color:#f92672>*</span>)elf_bss <span style=color:#f92672>+</span>
                        load_bias, nbyte)) {
            }
</code></pre></div><p>It will also call <code>elf_map</code> to map the segment to [vaddr, vaddr + file size] and align and then perform some checks:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>error <span style=color:#f92672>=</span> elf_map(bprm<span style=color:#f92672>-&gt;</span>file, load_bias <span style=color:#f92672>+</span> vaddr, elf_ppnt,
				elf_prot, elf_flags, total_size);
</code></pre></div><p>The interpreter is then loaded:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>elf_entry <span style=color:#f92672>=</span> load_elf_interp(<span style=color:#f92672>&amp;</span>loc<span style=color:#f92672>-&gt;</span>interp_elf_ex,
                interpreter,
                <span style=color:#f92672>&amp;</span>interp_map_addr,
                load_bias, interp_elf_phdata);
</code></pre></div><p>Finally, the elf talbe is created:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>retval <span style=color:#f92672>=</span> create_elf_tables(bprm, <span style=color:#f92672>&amp;</span>loc<span style=color:#f92672>-&gt;</span>elf_ex,
            load_addr, interp_load_addr);
</code></pre></div><p>After everything is prepared, we can call the <code>start_thread</code> function, which prepares the new task&rsquo;s registers and segments for execution. We will pass the set of registers for the new task, the address of the entry point of the new task, and the address of the top of of the statck for the new task to this function.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>start_thread(regs, elf_entry, bprm<span style=color:#f92672>-&gt;</span>p);
</code></pre></div><p>A lot of the information here can also be found at the <a href=https://0xax.gitbooks.io/linux-insides/content/SysCall/linux-syscall-4.html>linux-insides</a> book. I found it very helpful clearing my confusion.</p><p>In our own implementations, we will not call the loaded program&rsquo;s <code>main</code> function. Instead, our loader will transfer control to the entry point of the loaded program via the <code>jmp</code> instruction. It has two major differences:</p><ul><li>Jumping to the entry point indicates we are going to execute the glibc start up functions before main is called. This includes setting up thread local storge. <code>main</code> simply jump to the main with the loader&rsquo;s TLS, no other setups are involved.</li><li><code>jmp</code> doesn&rsquo;t push return address on stack. When the loaded prgoram finishes execution, it exits the loader program, instead of giving cntrol back to the caller.</li></ul></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/scheduleractivation/>Scheduler Activation</a></h2></header><footer class=entry-footer>October 24, 2020&nbsp;·&nbsp;4 min</footer><section class=entry-content><p><p>What is a thread? A thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler.</p><h2 id=kernel-level-threads-proscons>Kernel Level Threads Pros/Cons</h2><ul><li>Good functionality, system wide integration</li><li>Threads are seen and scheduled only by the kernel. A lot of kernel information should be invisible to user thread and can be useful for scheduling</li><li>Poor performance, every thread_related call traps. This situation is a lot worse in the 1990s than it is now mainly due to clock speed.
The scheduling quanta are roughly the same, but because the clock speeds are much faster today, you can execute orders of magnitude more instructions
per quanta today than you could in 1990. Even if traps, let&rsquo;s say, costs 10 cycles to complete, it would be a much bigger fraction of the quanta in 1990 than
it is today.</li></ul><h2 id=user-level-threads-proscons>User Level Threads Pros/Cons</h2><ul><li>Good performances. (most threads operations don&rsquo;t involve kernel)</li><li>Good scheduling policy flexibility: done by thread lib</li><li>Poor system-wide integration</li><li>Multi-programmed workloads are hard to schedule</li><li>I/O, page faults invisible</li><li>Potetntial for incorrect behavior<ul><li>User level scheduler may not be cooperative. With user threads running on kernel threads, it may be that kernel threads block when a user-thread blocks, thus an application can run out of kernel threads to run their user threads.May be gilding the lily.</li></ul></li></ul><h2 id=some-problems-about-user-level-threads-on-kernel-interface>Some Problems about User-Level Threads on Kernel Interface</h2><ul><li>Insufficient visibility between the kernel and user thread lib</li><li>Kernel event such as pr-emption or I/O are not visible to user lib<ul><li>For example, if user level threads block, then the kernel thread serving it also blocks.</li></ul></li><li>Kernel threads are scheduled with respect to user-level thread library, we can have this interferences between two schedulers.</li><li>Kernel time-slicing of threads<ul><li>For example, user level threads holding a spin-lock can be pre-empted, which can potentially cause all other user threads to wait.</li></ul></li></ul><h2 id=scheduler-activation>Scheduler Activation</h2><p>The basic principle about scheduler actication is to expose revocation: telling me when you take something away. This is basically the same idea as the exokernel. For example, interfaces like</p><ul><li>add_processor()</li><li>has_blocked()</li></ul><p>The basics about scheudler activation are</p><ul><li>Multi-threaded programs are still given an address space</li><li>Facilitate flow of kernel information between user and kernel threads</li><li>Kernel explicily vectors kernel events to the user-level thread<ul><li>via scheduler activation (upcall)</li></ul></li><li>Extended kernel interface for processor allcoation-related events<ul><li>Essentially exchanging information</li></ul></li></ul><h2 id=scheduler-activation-vs-kernel-threads>Scheduler Activation vs Kernel Threads</h2><p>Key differences:</p><ul><li>Pre-empted threads never resumed by the kernel direcly.<ul><li>Essentially, every new SA is a brand new context.</li><li>For example, if you do blocking I/O, the kernel will provide a new scheduling activagtion and vector into that application space. There isn&rsquo;t a notion of &ldquo;resume&rdquo;. The kernel is simply going to find some new schedule activation to notify you that a work has unblocked. In modern kernels, you would do somehting like stack unwinding to get back into user space.</li></ul></li></ul><p>An important problem is what happened if a user thread is forced to be de-scheduler while it&rsquo;s in a scheduler. The user thread will hold a a lock on user level run queue. That means no other user thread can be scheduled to run because none of them can acquire the lock. Because there&rsquo;s no notion of &ldquo;resume&rdquo; in scheduling activation, we can&rsquo;t really resume the execution in the scheduler. Thus, we run into a deadlock situation.</p><p>One solution is to detect whether we are using a lock and keep executing until we leave the locked region. Of course, there are too many gotchas in this solution.</p><p>Another solution is that the kernel can make a copy of the critical section and execute the critical section itself regardless of what the user thread chooses to do. Therefore, we can guarantee by the time you vector back into user space the lock is no longer held. So the kernel is basically executing the user code! Crazy, right?
Now we ran into more gotchas. What if the code is written in Java? How to find a locked region in userspace? What if &mldr;</p><p>Another thing we want to mention is page fault. Page fault indicates that you are missing part of your address. So there will be a notification with a new scheduler activation. Once you do soemthing with it, you will liekly touch that same piece in the space and double fault again.</p><p>What is the solution?</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/linuxkernelmeasurementandmmap/>Linux Program Measurement and mmap</a></h2></header><footer class=entry-footer>September 23, 2020&nbsp;·&nbsp;23 min</footer><section class=entry-content><p><p>This is a summary over Linux kernel program measurement and mmap. The specs of our experiment environment is listed below. For more details regarding the CPU spec please refer to <a href=http://www.cpu-world.com/CPUs/Core_i7/Intel-Core%20i7%20i7-6800K.html>cpu world</a>. This is the system spec:</p><table><thead><tr><th>Attribute</th><th style=text-align:center>Value</th></tr></thead><tbody><tr><td>Processor name (BIOS)</td><td style=text-align:center>Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz</td></tr><tr><td>Cores</td><td style=text-align:center>6</td></tr><tr><td>Logical processors</td><td style=text-align:center>12</td></tr><tr><td>TLB/Cache details</td><td style=text-align:center>64-byte Prefetching Data TLB: 1-GB pages, 4-way set associative, 4 entries Data TLB: 4-KB Pages, 4-way set associative, 64 entries Instruction TLB: 4-KByte pages, 8-way set associative, 64 entries L2 TLB: 1-MB, 4-way set associative, 64-byte line size Shared 2nd-Level TLB: 4-KB / 2-MB pages, 6-way associative, 1536 entries. Plus, 1-GB pages, 4-way, 16 entries</td></tr><tr><td>RAM</td><td style=text-align:center>32GB</td></tr><tr><td>Operating System</td><td style=text-align:center>Ubuntu 20.04.1 LTS</td></tr><tr><td>Kernel Version</td><td style=text-align:center>5.4.0-47-generic</td></tr></tbody></table><p><br></p><blockquote><p>8-way set associative means the CPU cache is made up of sets that can fit 8 blocks each.</p></blockquote><p>Here are the details for the CPU cache, which we will need later:</p><table><thead><tr><th>Cache</th><th>L1 data</th><th>L1 instruction</th><th>L2</th><th>L3</th></tr></thead><tbody><tr><td>Size</td><td>6 x 32 KB</td><td>6 x 32 KB</td><td>6 x 256 KB</td><td>15 MB</td></tr><tr><td>Associativity</td><td>8-way set associative</td><td>8-way set associative</td><td>8-way set associative</td><td>20-way set associative</td></tr><tr><td>Line size:</td><td>64 bytes</td><td>64 bytes</td><td>64 bytes</td><td>64 bytes</td></tr><tr><td>Comments:</td><td>Direct-mapped</td><td>Direct-mapped</td><td>Non-inclusive Direct-mapped</td><td>Inclusive Shared between all cores</td></tr></tbody></table><p><br></p><h2 id=memory-map>Memory Map</h2><p>To print the <code>/proc/self/maps</code> file for a process, we use the <code>sprintf</code> to construct the file name and then use the <code>system</code> from stdlib to cat the contents of the running process&rsquo;s address space. If we execute the program, it shows (also available on <a href=https://gist.github.com/BDHU/9ad2f0b6353b789cfb7c29c804a6088a#file-proc_mem_map>gist</a>)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>address                   perms offset  dev   inode                      pathname
559e3e51f000-559e3e520000 r--p 00000000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e520000-559e3e521000 r-xp 00001000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e521000-559e3e522000 r--p 00002000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e522000-559e3e523000 r--p 00002000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
559e3e523000-559e3e524000 rw-p 00003000 00:31 1199787                    /mnt/hdd1/Desktop/CS/CS380L/Lab1/a.out
7faf5c477000-7faf5c49c000 r--p 00000000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c49c000-7faf5c614000 r-xp 00025000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c614000-7faf5c65e000 r--p 0019d000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c65e000-7faf5c65f000 ---p 001e7000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c65f000-7faf5c662000 r--p 001e7000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c662000-7faf5c665000 rw-p 001ea000 08:22 11932543                   /usr/lib/x86_64-linux-gnu/libc-2.31.so
7faf5c665000-7faf5c66b000 rw-p 00000000 00:00 0 
7faf5c685000-7faf5c686000 r--p 00000000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c686000-7faf5c6a9000 r-xp 00001000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6a9000-7faf5c6b1000 r--p 00024000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6b2000-7faf5c6b3000 r--p 0002c000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6b3000-7faf5c6b4000 rw-p 0002d000 08:22 11932535                   /usr/lib/x86_64-linux-gnu/ld-2.31.so
7faf5c6b4000-7faf5c6b5000 rw-p 00000000 00:00 0 
7ffcddb8d000-7ffcddbae000 rw-p 00000000 00:00 0                          [stack]
7ffcddbe0000-7ffcddbe3000 r--p 00000000 00:00 0                          [vvar]
7ffcddbe3000-7ffcddbe4000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
</code></pre></div><p>Based on the <a href=https://man7.org/linux/man-pages/man5/proc.5.html>linux man page</a>, we can see each column has different definition. The <em>address</em> field is the address space in the process that the mapping occupies. The <em>perms</em> field is a set of permissions:</p><ul><li>r = read</li><li>w = write</li><li>x = execute</li><li>s = shared</li><li>p = private (copy on write)</li></ul><p>The <em>offset</em> field is the offset into the file/whatever; <em>dev</em> is the device (major:minor); <em>inode</em> is the inode on that device. 0 indicates that no inode is associated with the memory region, as would be the case with BSS (uninitialized data).</p><p>The <em>pathname</em> field will usually be the file that is backing the mapping. For ELF files, you can easily coordinate with the <em>offset</em> field by looking at the Offset field in the ELF program headers (readelf -l). In addition, we can see a few other pseudo-paths:</p><ul><li><p><em>[stack]</em>: the initial process&rsquo;s (also known as the main thread&rsquo;s) stack.</p></li><li><p><em>[vdso]</em>: The virtual dynamically linked shared object. More detailed descriptions can be found on <a href=https://lwn.net/Articles/615809/>lwn</a>.</p></li><li><p><em>[vvar]</em>: location of kernel space variables mapped in user space needed by virtual system calls. Essentially, a kernel-space physical address is mapped into the userspace.</p></li><li><p><em>[vsyscall]</em>: similar to vDSO, vsyscall is another segment used to accelerate certain system calls in Linux. Vsyscall has some limitations; among other things, there is only space for a handful of virtual system calls. More detailed descriptions can be found on <a href=https://lwn.net/Articles/446528/>lwn</a>.</p></li></ul><p>One thing interesting here is that when we execute the same program twice, we can see after the first run, the output is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>7fffbc92f000-7fffbc930000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
</code></pre></div><p>Type the same command again:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>7ffd6a94d000-7ffd6a94e000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
</code></pre></div><p>Note that the vDSO area has moved, while the vsyscall page remains at the same location. The location of the vsyscall page is nailed down in the kernel ABI, but the vDSO area - like most other areas in the user-space memory layout - has its location randomized every time it is mapped. The vsyscall is legacy implementation of user-space sys call acceleration. Since it has fixed addresses, it is vulnerable to security issues. Because applications depend on the existence and exact address of that page, most functions are simply removed and replaced by a special trap instruction. More detailed explanation can be found on <a href=https://lwn.net/Articles/446528/>lwn.net</a>.</p><p>Another interesting thing we observed is the base address of the executable (the start of the text section) and the start address of libc is rather different. This is also the result of using ASLR which is used to prevent return-to-libc attack.</p><h2 id=getrusage>getrusage</h2><p>Then, we call <code>getrusage</code> at the end of our program and print out the fields. We will need <code>getrusage</code> later. Here is a sample output for some fields inside <code>struct rusage</code>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>utime: 1306
stime: 0
maxrss: 2692
minflt: 76
majflt: 0
inblock: 0
oublock: 0
nvcsw: 2
nivcsw: 0
</code></pre></div><p>Here is a short list of descriptions for each of these fields. More detailed information can be found on <a href=https://www.gnu.org/software/libc/manual/html_node/Resource-Usage.html>gnu website</a></p><ul><li><strong>utime</strong>: time spent executing user instructions.</li><li><strong>stime</strong>: time spent in operating system code on behalf of processes.</li><li><strong>maxrss</strong>: the maximum resident set size used, in kilobytes. That is, the maximum number of kilobytes of physical memory that processes used simultaneously.</li><li><strong>minflt</strong>: the number of page faults which were serviced without requiring any I/O.</li><li><strong>majflt</strong>: the number of page faults which were serviced by doing I/O.</li><li><strong>inblock</strong>: the number of times the file system had to read from the disk on behalf of processes.</li><li><strong>oublock</strong>: the number of times the file system had to write to the disk on behalf of processes.</li><li><strong>nvcsw</strong>: the number of times processes voluntarily invoked a context switch (usually to wait for some service).</li><li><strong>nivcsw</strong>: the number of times an involuntary context switch took place (because a time slice expired, or another process of higher priority was scheduled).</li></ul><h2 id=perf_event_open>perf_event_open</h2><p><code>perf_event_open</code> interface is useful to measurement numerous system events. However, glibc doesn&rsquo;t provide wrapper for this system call. Instead, we need to use <code>syscall</code> directly.</p><p>To use <code>perf_event_open</code>, we call create a function wrapper that does the actual syscall for us. Take the example from the <a href=https://man7.org/linux/man-pages/man2/perf_event_open.2.html>Linux man page</a></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span>
<span style=color:#a6e22e>perf_event_open</span>(<span style=color:#66d9ef>struct</span> perf_event_attr <span style=color:#f92672>*</span>hw_event, pid_t pid,
                <span style=color:#66d9ef>int</span> cpu, <span style=color:#66d9ef>int</span> group_fd, <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>long</span> flags)
{
    <span style=color:#66d9ef>int</span> ret;

    ret <span style=color:#f92672>=</span> syscall(__NR_perf_event_open, hw_event, pid, cpu,
                    group_fd, flags);
    <span style=color:#66d9ef>return</span> ret;
}
</code></pre></div><p>Here the <code>__NR_perf_event_open</code> specifies the syscall number. On our local machine, we can go to <code>/usr/include/x86_64-linux-gnu/sys/syscall.h</code>, which specifies the location of <code>__NR_perf_event_open</code>. In our case, it is located at <code>/usr/include/x86_64-linux-gnu/asm/unistd_64.h</code>.</p><p>If we call <code>objdump -d</code> on the binary file, we will see something like this</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>000000000000119a &lt;perf_event_open&gt;:
    119a:	55                   	push   %rbp
    119b:	48 89 e5             	mov    %rsp,%rbp
    119e:	48 83 ec 30          	sub    $0x30,%rsp
    11a2:	48 89 7d e8          	mov    %rdi,-0x18(%rbp)
    11a6:	89 75 e4             	mov    %esi,-0x1c(%rbp)
    11a9:	89 55 e0             	mov    %edx,-0x20(%rbp)
    11ac:	89 4d dc             	mov    %ecx,-0x24(%rbp)
    11af:	4c 89 45 d0          	mov    %r8,-0x30(%rbp)
    11b3:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    11b7:	8b 75 dc             	mov    -0x24(%rbp),%esi
    11ba:	8b 4d e0             	mov    -0x20(%rbp),%ecx
    11bd:	8b 55 e4             	mov    -0x1c(%rbp),%edx
    11c0:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
    11c4:	49 89 f9             	mov    %rdi,%r9
    11c7:	41 89 f0             	mov    %esi,%r8d
    11ca:	48 89 c6             	mov    %rax,%rsi
    11cd:	bf 2a 01 00 00       	mov    $0x12a,%edi
    11d2:	b8 00 00 00 00       	mov    $0x0,%eax
    11d7:	e8 84 fe ff ff       	callq  1060 &lt;syscall@plt&gt;
    11dc:	89 45 fc             	mov    %eax,-0x4(%rbp)
    11df:	8b 45 fc             	mov    -0x4(%rbp),%eax
    11e2:	48 98                	cltq   
    11e4:	c9                   	leaveq 
    11e5:	c3                   	retq   
</code></pre></div><p>We notice there&rsquo;s one interesting line</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>callq  1060 &lt;syscall@plt&gt;
</code></pre></div><p>The <code>plt</code> stands for Procedure Linkage Table. This lines indicates a call to the <code>syscall</code> in the procedure linking table. The PLT allows us to resolve the absolute addresses of shared libraries at runtime.</p><p>Take a look at the <code>&lt;syscall@plt></code> section of the disassembly of section .plt, we see</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>0000000000001060 &lt;syscall@plt&gt;:
    1060:	ff 25 62 2f 00 00    	jmpq   *0x2f62(%rip)    #3fc8&lt;syscall@GLIBC_2.2.5&gt;
    1066:	68 03 00 00 00       	pushq  $0x3
    106b:	e9 b0 ff ff ff       	jmpq   1020 &lt;.plt&gt;
</code></pre></div><p>Notice this jump is a pointer to an address. The address lies inside the GOT (Global Offset Table). The GOT will eventually hold the absolute address call to <code>syscall</code>. On the first call the address will point back to the instruction after the jump in the PLT - <code>0x1066</code>. Then we see another jump instruction. This jump is a jump into the eventual runtime linker code that will load the shared library which has syscall.</p><p>We also see the comment for the first jump instruction</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>#3fc8&lt;syscall@GLIBC_2.2.5&gt;
</code></pre></div><p>Use <code>objdump -R</code>, we see the dynamic relocation entries in the file</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>DYNAMIC RELOCATION RECORDS
OFFSET           TYPE              VALUE 
0000000000003d98 R_X86_64_RELATIVE  *ABS*+0x0000000000001190
0000000000003da0 R_X86_64_RELATIVE  *ABS*+0x0000000000001150
0000000000004008 R_X86_64_RELATIVE  *ABS*+0x0000000000004008
0000000000003fd8 R_X86_64_GLOB_DAT  _ITM_deregisterTMCloneTable
0000000000003fe0 R_X86_64_GLOB_DAT  __libc_start_main@GLIBC_2.2.5
0000000000003fe8 R_X86_64_GLOB_DAT  __gmon_start__
0000000000003ff0 R_X86_64_GLOB_DAT  _ITM_registerTMCloneTable
0000000000003ff8 R_X86_64_GLOB_DAT  __cxa_finalize@GLIBC_2.2.5
0000000000003fb0 R_X86_64_JUMP_SLOT  getpid@GLIBC_2.2.5
0000000000003fb8 R_X86_64_JUMP_SLOT  __stack_chk_fail@GLIBC_2.4
0000000000003fc0 R_X86_64_JUMP_SLOT  system@GLIBC_2.2.5
0000000000003fc8 R_X86_64_JUMP_SLOT  syscall@GLIBC_2.2.5
0000000000003fd0 R_X86_64_JUMP_SLOT  sprintf@GLIBC_2.2.5
</code></pre></div><h2 id=monitor-events>Monitor Events</h2><p>Next, we are going to look at L1 data cache metrics. We are interested in L1 data cache accesses, misses, and data TLB misses. We will measure this code in our experiment. CACHE_LINE_SIZE is defined as 64 to match our CPU specs.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>// p points to a region that is 1GB (ideally)
</span><span style=color:#75715e></span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>do_mem_access</span>(<span style=color:#66d9ef>char</span><span style=color:#f92672>*</span> p, <span style=color:#66d9ef>int</span> size) {
    <span style=color:#66d9ef>int</span> i, j, count, outer, locality;
    <span style=color:#66d9ef>int</span> ws_base <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
    <span style=color:#66d9ef>int</span> max_base <span style=color:#f92672>=</span> ((size <span style=color:#f92672>/</span> CACHE_LINE_SIZE) <span style=color:#f92672>-</span> <span style=color:#ae81ff>512</span>);
    <span style=color:#66d9ef>for</span>(outer <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; outer <span style=color:#f92672>&lt;</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>&lt;&lt;</span><span style=color:#ae81ff>20</span>); <span style=color:#f92672>++</span>outer) {
      <span style=color:#66d9ef>long</span> r <span style=color:#f92672>=</span> simplerand() <span style=color:#f92672>%</span> max_base;
      <span style=color:#75715e>// Pick a starting offset
</span><span style=color:#75715e></span>      <span style=color:#66d9ef>if</span>( opt_random_access ) {
         ws_base <span style=color:#f92672>=</span> r;
      } <span style=color:#66d9ef>else</span> {
         ws_base <span style=color:#f92672>+=</span> <span style=color:#ae81ff>512</span>;
         <span style=color:#66d9ef>if</span>( ws_base <span style=color:#f92672>&gt;=</span> max_base ) {
            ws_base <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
         }
      }
      <span style=color:#66d9ef>for</span>(locality <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; locality <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>16</span>; locality<span style=color:#f92672>++</span>) {
         <span style=color:#66d9ef>volatile</span> <span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>a;
         <span style=color:#66d9ef>char</span> c;
         <span style=color:#66d9ef>for</span>(i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>512</span>; i<span style=color:#f92672>++</span>) {
            <span style=color:#75715e>// Working set of 512 cache lines, 32KB
</span><span style=color:#75715e></span>            a <span style=color:#f92672>=</span> p <span style=color:#f92672>+</span> (ws_base <span style=color:#f92672>+</span> i) <span style=color:#f92672>*</span> CACHE_LINE_SIZE;
            <span style=color:#66d9ef>if</span>((i<span style=color:#f92672>%</span><span style=color:#ae81ff>8</span>) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
               <span style=color:#f92672>*</span>a <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
            } <span style=color:#66d9ef>else</span> {
               c <span style=color:#f92672>=</span> <span style=color:#f92672>*</span>a;
            }
         }
      }
   }
}
</code></pre></div><p>What this routine does is essentially pick a working set of 512 cache lines, periodically perform a write or otherwise read operation. This process is repeated 16 times during each interaction. Each read or write access will operate on a new cache line. The innermost loop will perform this set of operations for the entire L1 data cache.</p><p>When opt_random_access is true, the starting base address of the cache line is randomly picked. Otherwise, it is incremented by 512 cache lines (or one working set) during each outer iteration. The main difference is that with opt_random_access set to true, the starting base address of the cache line can&rsquo;t be precomputed by the hardware, thus likely increase miss rate.</p><p>To measure L1 data cache metrics, we will use the <code>perf_event_open</code> interface we discussed above. To measure L1 data cache read misses, we will configure our <code>struct perf_event_attr</code> as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>#define CALC_CONFIG(perf_hw_cache_id, perf_hw_cache_op_id, perf_hw_cache_op_result_id) \
</span><span style=color:#75715e>((perf_hw_cache_id) | (perf_hw_cache_op_id &lt;&lt; 8) | (perf_hw_cache_op_result_id &lt;&lt; 16))
</span><span style=color:#75715e></span>
hw_event.type <span style=color:#f92672>=</span> PERF_TYPE_HW_CACHE; 
hw_event.size <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>struct</span> perf_event_attr);
hw_event.disabled <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>; <span style=color:#75715e>// disable at init time
</span><span style=color:#75715e></span>hw_event.exclude_kernel <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
hw_event.config <span style=color:#f92672>=</span> CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS);
</code></pre></div><p>The exact details can be found in <a href=https://man7.org/linux/man-pages/man2/perf_event_open.2.html>linux man page</a>. The important part is:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>hw_event.config <span style=color:#f92672>=</span> CALC_CONFIG(PERF_COUNT_HW_CACHE_L1D, PERF_COUNT_HW_CACHE_OP_READ, PERF_COUNT_HW_CACHE_RESULT_ACCESS);
</code></pre></div><p>These configurations allows us to measure the L1 data cahe read misses. The arguments passed to <code>perf_event_open</code> is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>pid_t pid <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
<span style=color:#66d9ef>int</span> cpu <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>;
<span style=color:#66d9ef>int</span> group_fd <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>;
<span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>long</span> flags <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</code></pre></div><p>The choice of these parameters can also be found on the <a href=https://man7.org/linux/man-pages/man2/perf_event_open.2.html>linux man page</a>. After <code>perf_event_open</code> is called, we will re-enable event measurements by calling</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>ioctl(fd, PERF_EVENT_IOC_RESET, <span style=color:#ae81ff>0</span>);
ioctl(fd, PERF_EVENT_IOC_ENABLE, <span style=color:#ae81ff>0</span>);
</code></pre></div><p>What it does is resetting the event count specified by the file descriptor argument to zero, then enables the individual event specified by the file descriptor argument. After <code>do_mem_access(p, size)</code> is executed, we call <code>ioctl(fd, PERF_EVENT_IOC_DISABLE, 0)</code> to disable the event and then read the result by <code>read(fd, &result, sizeof(long long))</code>. How result is defined is up to how <code>PERF_FORMAT_*</code> was specified. You can also check <a href=https://elixir.bootlin.com/linux/latest/source/kernel/events/core.c#L1833>lxr</a> to see how <code>__perf_event_read_size</code> calculates the size of event that is read. In our case, it&rsquo;s simple a <code>u64</code>.</p><blockquote><p>Be aware that simply executing the binary might cause <code>perf_event_open</code> to fail (in which case will always return -1). Using <code>sudo</code> is one workaround. Execute <code>cat /proc/sys/kernel/perf_event_paranoid</code> and see what returns. <code>-1</code> means you have raw access to kernel tracepoints. Otherwise, you might have trouble accessing the performance counter without root privilege. Check this <a href=https://unix.stackexchange.com/questions/14227/do-i-need-root-admin-permissions-to-run-userspace-perf-tool-perf-events-ar>stackexchange post</a> for more details.</p></blockquote><p>To be even more careful about generating repeatable results we should flush the level 1 data cache before enabling the performance counters. We will do this by reading a memory buffer larger than per-core L1 data cache size</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>size_t buffer_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>1024</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1024</span>;
<span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>buff <span style=color:#f92672>=</span> malloc(buffer_size);
<span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> buffer_size; i<span style=color:#f92672>++</span>) {
    buff[i] <span style=color:#f92672>=</span> rand();
}
</code></pre></div><p>We will also lock the process onto a single processor by using the <code>sched_setaffinity</code> function. Our example is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>cpu_set_t set;
CPU_ZERO(<span style=color:#f92672>&amp;</span>set);
CPU_SET(<span style=color:#ae81ff>7</span>, <span style=color:#f92672>&amp;</span>set);
<span style=color:#66d9ef>int</span> aff <span style=color:#f92672>=</span> sched_setaffinity(<span style=color:#ae81ff>0</span>, <span style=color:#66d9ef>sizeof</span>(cpu_set_t), <span style=color:#f92672>&amp;</span>set);
</code></pre></div><p>We perform the each of the above experiments 5 times. First, we turn on random cache line base address generation. On average, we have around 1010665367 L1 data cache read misses wtih standard deviation to be 61010967 misses. When random access is disabled, we have on average 964420324 read misses with standard deviation of 65787193 misses. We can also measure the number L1 data cache write misses by using the <code>PERF_COUNT_HW_CACHE_OP_WRITE</code> config instead. Use <code>PERF_COUNT_HW_CACHE_OP_PREFETCH</code> gives us prefetch misses, in our case, both of these metrics are unavailable. We can check the <code>/arch/x86/events/intel/core.c</code> in <a href=https://elixir.bootlin.com/linux/v5.6/source/arch/x86/events/intel/core.c>lxr</a> and we can see these metrics are not available.</p><p>We can also use the <code>PERF_COUNT_HW_CACHE_DTLB</code> config option for data TLB measurement. For read access we have on average 3390719 misses with std dev being 17579, while write access has 1486451 misses with std dev being 13455. The prefetch metrics for TLB are unavailable in our case. To find out more about available metrics supported, please check the constant <code>static __initconst const u64 skl_hw_cache_event_ids</code> for specific kernel version.</p><p>With random cache line access turned off, we have 517335 read misses data TLB with standard deviation of 3820 misses. For write we have on average 809671 misses with standard deviation being 9580 misses. It is a significant reduction compared to the random access implementation.</p><p>To calculate the L1 cache miss rate and data TLB miss rate, we can use 100.0 * cache misses / cache_accesses and 100.0 * tlb misses / cache_accesses to calculate the results. With random access turned off, we get L1 read access miss rate to be $$miss_{cache} = 1.5%$$ and TLB read miss rate $$miss_{tlb} \approx 0$$. When random access is turned on, we have $$miss_{cache} = 1.4%$$ and $$miss_{tlb} \approx 0$$. We can see the miss rate in all scenarios is really low. This is mainly because the inner most loop in our routine is performing operations on working set already presented in L1 cache and TLB. The read/write operations use continous cache lines, which means there will almost be no faults while we access the 512 cache lines. If one fault causes the entire new working set to be cached, then there would be no subsequent faults until the entire working set is iterated.</p><p>If we use <code>getrusage</code> we can see the metrics listed below:</p><table><thead><tr><th>Metrics</th><th>Mean</th><th>std dev</th></tr></thead><tbody><tr><td>utime</td><td>868629</td><td>126044</td></tr><tr><td>stime</td><td>253586</td><td>20112</td></tr><tr><td>maxrss</td><td>1049691</td><td>43</td></tr><tr><td>minflt</td><td>262214</td><td>1</td></tr><tr><td>majflt</td><td>0</td><td>0</td></tr><tr><td>inblock</td><td>0</td><td>0</td></tr><tr><td>oublock</td><td>0</td><td>0</td></tr><tr><td>nvcsw</td><td>0.4</td><td>0.54</td></tr><tr><td>nivcsw</td><td>47</td><td>7</td></tr></tbody></table><p><br></p><h2 id=mmap>mmap</h2><p>Next we are going to explore the behavior of mmap. Previously, we used <code>malloc</code> for data allocation. Next, we are going to instead use <code>mmap</code> and see what happens. Here we will only use read access for benchmark metrics since it&rsquo;s available in both L1 and TLB metrics.</p><p>First, we use the <code>MAP_ANONYMOUS</code> as a flag passed to <code>mmap</code>. This flag means the mapping is not backed by any file; its contents are initialized to zero. The complete call is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>mmap(NULL, length, PROT_READ <span style=color:#f92672>|</span> PROT_WRITE,
                MAP_PRIVATE <span style=color:#f92672>|</span> MAP_ANONYMOUS, fd_ignore, offset);
</code></pre></div><p>For more details, refer to <a href=https://man7.org/linux/man-pages/man2/mmap.2.html>mmap man page</a> for information.</p><p>When we turn on the random access and use <code>perf_event_open</code> interface to collect metrics, we see the L1 data cache read misses are 956148031 (std dev 84631843). The TLB data cache read misses are 3370309 (std dev 17792). We see it is not really different to the malloc approach we used before. Doing a simple <code>strace</code> shows <code>malloc</code> calls <code>mmap</code>. The memory that backs <code>malloc()</code> allocations is handled by the kernel in much the same way as the memory that backs private anonymous mappings created with <code>mmap()</code>.</p><p>Then, we try to use <code>mmap()</code> to create mapping in the virtual address space backed by a file instead of using <code>MAP_ANONYMOUS</code>.</p><p>We first test <code>mmap</code> with <code>MAP_PRIVATE</code>. According to the man page, this flags means creating a private copy-on-write mapping. Updates to the mapping are not visible to other processes mapping the same file, and are not carried through to the underlying file. It is unspecified whether changes made to the file after the mmap() call are visible in the mapped region.</p><blockquote><p>Note we should call <code>fallocate()</code> for the newly created file, otherwise mmap is gonna throw bur error.</p></blockquote><p>When we measure the L1 data cache miss, it&rsquo;s around 946128512 (std dev 956148031), nothing special happens. When we use <code>MAP_SHARED</code> flag, the result was similar. The result seems to fluctuates as time passes, but overall they are not much different. After all, it&rsquo;s just reading from the memory, whether the address is backed by a file or not doesn&rsquo;t play a big role in affecting the cache miss rate. The L1 data cache misses is shown below:</p><table><thead><tr><th>Flag</th><th>PRIVATE</th><th>PRIVATE+POPULATE</th><th>SHARED</th><th>SHARED+POPULATE</th></tr></thead><tbody><tr><td>Mean</td><td>783864673</td><td>769314361</td><td>842915231</td><td>816749524</td></tr><tr><td>Std dev</td><td>77816766</td><td>53913082</td><td>54613278</td><td>60580595</td></tr></tbody></table><p><br></p><p>If we take a look at TLB data cache, the result is</p><table><thead><tr><th>Flag</th><th>PRIVATE</th><th>PRIVATE+POPULATE</th><th>SHARED</th><th>SHARED+POPULATE</th></tr></thead><tbody><tr><td>Mean</td><td>3372303</td><td>3370740</td><td>3381755</td><td>3377370</td></tr><tr><td>Std dev</td><td>9884</td><td>13567</td><td>17626</td><td>11776</td></tr></tbody></table><p><br></p><p>Still, there doesn&rsquo;t seem have any significant fluctuation in the number of misses in data TLB. This pattern also applies to sequential access, except the TLB data cache misses is alot lower in sequentual access.</p><p>Now If we instead use <code>getrusage()</code>, we will get something like this</p><table><thead><tr><th>Flag</th><th>PRIVATE</th><th>PRIVATE+POPULATE</th><th>SHARED</th><th>SHARED+POPULATE</th></tr></thead><tbody><tr><td>Usec/std dev</td><td>20/0</td><td>20/0</td><td>20/0</td><td>20/0</td></tr><tr><td>usec/std dev</td><td>801512/ 78346</td><td>793452/ 143556</td><td>872342/ 124124</td><td>671957/ 229314</td></tr><tr><td>Ssec/std dev</td><td>0/0</td><td>0/0</td><td>0/0</td><td>0/0</td></tr><tr><td>ssec/std dev</td><td>475977/ 54355</td><td>475678/ 134253</td><td>445467/ 99345</td><td>536041/ 98797</td></tr><tr><td>oublock/std dev</td><td>0/0</td><td>0/0</td><td>2997152/ 82256</td><td>2097152/ 19760</td></tr></tbody></table><p><br></p><p>The most interesting part here is when <code>MAP_SHARED</code> is enabled, the <code>oublock</code> immediately changes. As we mentioned previously, <code>oublock</code> specifies the number of times the file system had to write to the disk on behalf of processes. Because the address is now backed by a file, all write operations will cause the file system to write the contents back to the file.</p><p><code>mmap()</code> creates a new mapping in the virtual address space of the
calling process. However, it doesn&rsquo;t allocate RAM. If we call <code>memset()</code> then followed by <code>msync()</code> with <code>MS_SYNC</code> flag, we can get some interesting results in <code>getrusage</code>, these observations are summarized here:</p><ul><li>kernel space time is much higher. It usually take 1 sec (no std dev) as opposed to 0. Synchronizing to files on disk will require more kernel participation.</li><li>minflt (the number of page faults which were serviced without requiring any I/O) was muich higher, the value is around 540782(std dev 3). More memory mapped means the faults by I/O will be less likely.</li><li>oublock is much higher, the value is around 4196512(std dev 1). The sync operating means there will be approximatly double amount of writes to disk.</li><li>nvcsw was higher, there are more voluntary context switches. Writing results to disk has delay, and thus the process likely need to context switch while waiting for I/O to be finished.</li></ul><p>We may notice the number data TLB misses is lower than the total number of page the application uses. One obvious answer the use of huge page. One huge page can cover many small pages. Also, because we have prefetching TLB and the working set access pattern is contiguous, TLB hit rate will be high. Because we have a set-associative TLB cache, and we access the memory in a fairly deterministic way, it&rsquo;s easy to predict where the next access is pointing to. For example, if the replacement policy is FIFO, then each cache line will remain untouched for exact same clock cycle before replaced. This also applies to other policies. One way to determine the replacement algorithm is using P-Chase.</p><h2 id=strace>strace</h2><p>We then use <code>strace</code> to trace syscalls of our application. The output contains some interesting information, one is</p><pre><code>access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)
...
arch_prctl(ARCH_SET_FS, 0x7fdc6ad83540) = 0
</code></pre><p>According to <a href=https://man7.org/linux/man-pages/man2/arch_prctl.2.html>arch_prctl man page</a>, <code>arch_prctl()</code> sets architecture-specific process or thread state. The <code>ARCH_SET_FS</code> option sets the 64-bit base for the FS register to addr, in our case it&rsquo;s 0x7fdc6ad83540. Let&rsquo;s set a break point at <code>arch_prctl</code> and backtrace from there</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>#0  0x00007ffff7febb55 in ?? () from /lib64/ld-linux-x86-64.so.2
#1  0x00007ffff7fd104c in ?? () from /lib64/ld-linux-x86-64.so.2
#2  0x00007ffff7fd0108 in ?? () from /lib64/ld-linux-x86-64.so.2
#3  0x0000000000000001 in ?? ()
#4  0x00007fffffffe2fa in ?? ()
#5  0x0000000000000000 in ?? ()
</code></pre></div><p>We can see the FS segment base is set by the <code>ld-linux</code>, which is a part of glibc, during the program loading. A simple google seach tells us <code>/lib64/ld-linux-x86-64.so.2</code> is a dynamic linker. A more detailed description can be found on this <a href=https://unix.stackexchange.com/questions/400621/what-is-lib64-ld-linux-x86-64-so-2-and-why-can-it-be-used-to-execute-file>post</a> and <a href=https://lwn.net/Articles/631631/>lwn.net</a>. During the startup, the loader initalizes TLS. The includes memory allocation and setting FS base value to point to the TLS beignning, which is done via the <code>arch_prctl</code> syscall. More can be found <a href=https://unix.stackexchange.com/questions/453749/what-sets-fs0x28-stack-canary/453772>here</a>. This <code>init_tls()</code> is called <a href="https://git.launchpad.net/glibc/tree/elf/rtld.c?id=916124ed841745b7a1e0fbc43f9909340b47d373#n1397">here</a>, which subsequently calls the actuall <a href=https://git.launchpad.net/glibc/tree/sysdeps/x86_64/nptl/tls.h#n153>syscall</a> in <code>tls.h</code>.</p><p>The /etc/ld.so.preload has similarities to LD_PRELOAD, in addition, it doesn&rsquo;t suffer security limitation posed by LD_PRELOAD (<a href=https://superuser.com/questions/1183037/what-is-does-ld-so-preload-do>explanation here</a>). This a feature of <em>glibc</em>.</p><h2 id=competing-for-memory>Competing for Memory</h2><p>Next we are going to fork another process that will compete for memory with our process under test. We will use this code snippet which is going to be executed by both the parent and the child process</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>compete_for_memory</span>(<span style=color:#66d9ef>void</span><span style=color:#f92672>*</span> unused) {
   <span style=color:#66d9ef>long</span> mem_size <span style=color:#f92672>=</span> get_mem_size();
   <span style=color:#66d9ef>int</span> page_sz <span style=color:#f92672>=</span> sysconf(_SC_PAGE_SIZE);
   printf(<span style=color:#e6db74>&#34;Total memsize is %3.2f GBs</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>,
         (<span style=color:#66d9ef>double</span>)mem_size<span style=color:#f92672>/</span>(<span style=color:#ae81ff>1024</span><span style=color:#f92672>*</span><span style=color:#ae81ff>1024</span><span style=color:#f92672>*</span><span style=color:#ae81ff>1024</span>));
   fflush(stdout);
   <span style=color:#66d9ef>char</span><span style=color:#f92672>*</span> p <span style=color:#f92672>=</span> mmap(NULL, mem_size, PROT_READ <span style=color:#f92672>|</span> PROT_WRITE,
                  MAP_NORESERVE<span style=color:#f92672>|</span>MAP_PRIVATE<span style=color:#f92672>|</span>MAP_ANONYMOUS, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, (off_t) <span style=color:#ae81ff>0</span>);
   <span style=color:#66d9ef>if</span> (p <span style=color:#f92672>==</span> MAP_FAILED)
      perror(<span style=color:#e6db74>&#34;Failed anon MMAP competition&#34;</span>);

   <span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
   <span style=color:#66d9ef>while</span>(<span style=color:#ae81ff>1</span>) {
      <span style=color:#66d9ef>volatile</span> <span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>a;
      <span style=color:#66d9ef>long</span> r <span style=color:#f92672>=</span> simplerand() <span style=color:#f92672>%</span> (mem_size<span style=color:#f92672>/</span>page_sz);
      <span style=color:#66d9ef>char</span> c;
      <span style=color:#66d9ef>if</span>( i <span style=color:#f92672>&gt;=</span> mem_size<span style=color:#f92672>/</span>page_sz ) {
         i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
      }
      <span style=color:#75715e>// One read and write per page
</span><span style=color:#75715e></span>      <span style=color:#75715e>//a = p + i * page_sz; // sequential access
</span><span style=color:#75715e></span>      a <span style=color:#f92672>=</span> p <span style=color:#f92672>+</span> r <span style=color:#f92672>*</span> page_sz;
      c <span style=color:#f92672>+=</span> <span style=color:#f92672>*</span>a;
      <span style=color:#66d9ef>if</span>((i<span style=color:#f92672>%</span><span style=color:#ae81ff>8</span>) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
         <span style=color:#f92672>*</span>a <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
      }
      i<span style=color:#f92672>++</span>;
   }
   <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
}
</code></pre></div><p>The <code>get_mem_size()</code> is implemented using this <a href=https://stackoverflow.com/questions/22670257/getting-ram-size-in-c-linux-non-precise-result>portable code</a></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>#if defined(_WIN32)
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;Windows.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#75715e>#elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) &amp;&amp; defined(__MACH__))
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;unistd.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;sys/types.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;sys/param.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#if defined(BSD)
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;sys/sysctl.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#endif
</span><span style=color:#75715e></span>
<span style=color:#75715e>#else
</span><span style=color:#75715e>#error &#34;Unable to define getMemorySize( ) for an unknown OS.&#34;
</span><span style=color:#75715e>#endif
</span><span style=color:#75715e></span>
<span style=color:#75715e>/**
</span><span style=color:#75715e> * Returns the size of physical memory (RAM) in bytes.
</span><span style=color:#75715e> */</span>
size_t <span style=color:#a6e22e>getMemorySize</span>( )
{
<span style=color:#75715e>#if defined(_WIN32) &amp;&amp; (defined(__CYGWIN__) || defined(__CYGWIN32__))
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* Cygwin under Windows. ------------------------------------ */</span>
    <span style=color:#75715e>/* New 64-bit MEMORYSTATUSEX isn&#39;t available.  Use old 32.bit */</span>
    MEMORYSTATUS status;
    status.dwLength <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(status);
    GlobalMemoryStatus( <span style=color:#f92672>&amp;</span>status );
    <span style=color:#66d9ef>return</span> (size_t)status.dwTotalPhys;

<span style=color:#75715e>#elif defined(_WIN32)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* Windows. ------------------------------------------------- */</span>
    <span style=color:#75715e>/* Use new 64-bit MEMORYSTATUSEX, not old 32-bit MEMORYSTATUS */</span>
    MEMORYSTATUSEX status;
    status.dwLength <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(status);
    GlobalMemoryStatusEx( <span style=color:#f92672>&amp;</span>status );
    <span style=color:#66d9ef>return</span> (size_t)status.ullTotalPhys;

<span style=color:#75715e>#elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) &amp;&amp; defined(__MACH__))
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* UNIX variants. ------------------------------------------- */</span>
    <span style=color:#75715e>/* Prefer sysctl() over sysconf() except sysctl() HW_REALMEM and HW_PHYSMEM */</span>

<span style=color:#75715e>#if defined(CTL_HW) &amp;&amp; (defined(HW_MEMSIZE) || defined(HW_PHYSMEM64))
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>int</span> mib[<span style=color:#ae81ff>2</span>];
    mib[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> CTL_HW;
<span style=color:#75715e>#if defined(HW_MEMSIZE)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_MEMSIZE;            <span style=color:#75715e>/* OSX. --------------------- */</span>
<span style=color:#75715e>#elif defined(HW_PHYSMEM64)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_PHYSMEM64;          <span style=color:#75715e>/* NetBSD, OpenBSD. --------- */</span>
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>    int64_t size <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;               <span style=color:#75715e>/* 64-bit */</span>
    size_t len <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>( size );
    <span style=color:#66d9ef>if</span> ( sysctl( mib, <span style=color:#ae81ff>2</span>, <span style=color:#f92672>&amp;</span>size, <span style=color:#f92672>&amp;</span>len, NULL, <span style=color:#ae81ff>0</span> ) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> )
        <span style=color:#66d9ef>return</span> (size_t)size;
    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0L</span>;          <span style=color:#75715e>/* Failed? */</span>

<span style=color:#75715e>#elif defined(_SC_AIX_REALMEM)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* AIX. ----------------------------------------------------- */</span>
    <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_AIX_REALMEM ) <span style=color:#f92672>*</span> (size_t)<span style=color:#ae81ff>1024L</span>;

<span style=color:#75715e>#elif defined(_SC_PHYS_PAGES) &amp;&amp; defined(_SC_PAGESIZE)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* FreeBSD, Linux, OpenBSD, and Solaris. -------------------- */</span>
    <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_PHYS_PAGES ) <span style=color:#f92672>*</span>
        (size_t)sysconf( _SC_PAGESIZE );

<span style=color:#75715e>#elif defined(_SC_PHYS_PAGES) &amp;&amp; defined(_SC_PAGE_SIZE)
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* Legacy. -------------------------------------------------- */</span>
    <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_PHYS_PAGES ) <span style=color:#f92672>*</span>
        (size_t)sysconf( _SC_PAGE_SIZE );

<span style=color:#75715e>#elif defined(CTL_HW) &amp;&amp; (defined(HW_PHYSMEM) || defined(HW_REALMEM))
</span><span style=color:#75715e></span>    <span style=color:#75715e>/* DragonFly BSD, FreeBSD, NetBSD, OpenBSD, and OSX. -------- */</span>
    <span style=color:#66d9ef>int</span> mib[<span style=color:#ae81ff>2</span>];
    mib[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> CTL_HW;
<span style=color:#75715e>#if defined(HW_REALMEM)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_REALMEM;        <span style=color:#75715e>/* FreeBSD. ----------------- */</span>
<span style=color:#75715e>#elif defined(HW_PYSMEM)
</span><span style=color:#75715e></span>    mib[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> HW_PHYSMEM;        <span style=color:#75715e>/* Others. ------------------ */</span>
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> size <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;      <span style=color:#75715e>/* 32-bit */</span>
    size_t len <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>( size );
    <span style=color:#66d9ef>if</span> ( sysctl( mib, <span style=color:#ae81ff>2</span>, <span style=color:#f92672>&amp;</span>size, <span style=color:#f92672>&amp;</span>len, NULL, <span style=color:#ae81ff>0</span> ) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> )
        <span style=color:#66d9ef>return</span> (size_t)size;
    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0L</span>;          <span style=color:#75715e>/* Failed? */</span>
<span style=color:#75715e>#endif </span><span style=color:#75715e>/* sysctl and sysconf variants */</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#75715e>#else
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0L</span>;          <span style=color:#75715e>/* Unknown OS. */</span>
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>}
</code></pre></div><p>The important line is</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c> <span style=color:#66d9ef>return</span> (size_t)sysconf( _SC_PHYS_PAGES ) <span style=color:#f92672>*</span>
        (size_t)sysconf( _SC_PAGESIZE );
</code></pre></div><p>One thing to notice in the routine for competing for memory is we used <code>fflush</code> after the <code>printf</code>. The purpose of <code>fflush(stream)</code> is to make the operating system flush any buffers to the underlying file. This mainly because stdout is buffered. The buffer is not flushed until newline. <code>fflush</code> will cause this process to happen with the absense of newline. stderr is unbuffered and thus fflush would not be necessary.</p><p>For this experiment, we tested it on a VM. The reason is because the contending process will take all RAM and completely hault the mahcine if tested on the host. To ensure our VM has enough swap space, we follow this <a href=https://wiki.crowncloud.net/?adding_swap_kvm>tutorial</a> to create 4GB of swap area (we allocated 2GB RAM for VM).</p><p>One thing we observe is that the execution time of the program become significantly longer to run. In our experiement we need to limit the number of iterations from 1 &#171; 20 to 1 &#171; 8 to get some sensible results without running for days.</p><p>When we use PRIVATE and ANONYMUS option and random access turned on, the misses in data TLB is 335009(std dev 7298). We can&rsquo;t get access to L1 cache data because it will cause the session to be automatically logged out whenever L1D is used. here are some interesting things to notice:</p><ul><li><em>MAP_PRIVATE + MAP_ANONYMOUS</em>: TLB misses:335009(std dev 17298)<br>minflt: 4220(std dev 231)<br>oublock: 8(std dev 4)<br>nivcsw: 19(10)</li><li><em>MAP_SHARED</em>: TLB misses:251284std dev 103292)<br>minflt: 2784(std dev 231)<br>majflt: 247(std 65)<br>oublock: 18200(std dev 2987)<br>nivcsw: 8(7)</li></ul><p>The most important difference here is that the oublock is much easier to trigger because the constant swapping. When file backed memory is used we also notice that majflt is much higher. Because pages are constantly traveling between swap area and memory, the page fault rate becomes a lot higher. The oublock also follows previous patterns as the file backed memory requires filesystem involvement.</p><p>Finally, we also modify the kernel&rsquo;s head (or more precisely its LRU page replacement algorithm). Look in <code>mm/vmscan.c</code> there&rsquo;s a function calleed <code>shrink_page_list</code>. In it, you will see a switch statement with a PAGEREF_ACTIVATE case, which is the case where the kernel sees the page has been recently accessed. In this case the kernel gotos activate_locked, but you will change it to to do the same thing as the PAGEREF_RECLAIM case. We can simply move the case down and change its default behavior to direct to the PAGEREF_RECLAIM case. After that, we need to recompile the kernel for VM. We also summarize the most interesting results:</p><ul><li><em>MAP_PRIVATE + MAP_ANONYMOUS</em>: TLB misses:308031(std dev 17298)<br>minflt: 4223(std dev 791)<br>oublock: 8(std dev 1)<br>nivcsw: 11(5)</li><li><em>MAP_SHARED</em>: TLB misses: 251284std dev 103292)<br>minflt: 2724(std dev 231)<br>majflt: 0(std 0)<br>oublock: 18200(std dev 2987)<br>nivcsw: 8(7)</li></ul><p>We can see that the most of the pattern follow the previous result after the modified kernel is installed. One main difference is majflt value is reduced back down.</p></p></section></article><article class="post-entry tag-entry"><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/kernelhacking/>Start Linux Kernel Hacking</a></h2></header><footer class=entry-footer>September 14, 2020&nbsp;·&nbsp;12 min</footer><section class=entry-content><p><p align=center><a href=https://www.redandblack.com/opinion/opinion-make-the-switch-to-a-linux-operating-system/article_0b8bb324-5425-11e9-9d96-ab61c820e566.html><img src=https://raw.githubusercontent.com/BDHU/Page_pics/master/posts/linux_kernel_hacking/linux.jpg width=80%></a></p><p>This is a summary of how to compile and boot the Linux kernel on the KVM-qemu virtual machine. It covers how to get a VM running in KVM, how to build a customized kernel, and how to use GDB with the Linux kernel. The experiment is conducted on an amd64 architecture CPU. We use Ubuntu as our testing environment but the steps covered here should apply to other distros as well.</p><h2 id=getting-a-vm-running-in-kvm>Getting a VM running in KVM</h2><p>The Ubuntu ISO image is downloaded from the <a href=https://ubuntu.com/download/desktop>Canonical website</a>. The kernel is downloaded directly from <a href=https://www.kernel.org/>kernel.org</a>. The specs of our test environment is:</p><ul><li>CPU: Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz</li><li>RAM: 32 GB</li><li>Host and Guest OS: Ubuntu 20.04.1 LTS</li><li>Host Kernel Version: 5.4.0-47-generic</li><li>GCC: 7.5.0</li><li>QEMU emulator version: 4.2.0</li><li>Guest Kernel Version: 5.8.6</li></ul><p>After we obtained the Ubuntu ISO image, we use GUI virt-manager to install the OS. One thing to notice here is the default directory for virtual disks is <code>/var/lib/libvirt/images</code>, since my system partition is located on a separate SSD with limited space, the virtual disk directory is changed to my <code>/home</code> directory instead.</p><p>We also create the new virtual disk inside virt-manager. We chose raw format instead of qcow2. Creating a new image file can also be done in command line using:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>qemu-img create -f raw -o preallocation<span style=color:#f92672>=</span>full vmdisk.img 40G
</code></pre></div><p>The preallocation can be turn either on or off depends on personal choices. After the disk image is created, we proceeds in virt-manager to install Ubuntu on the newly allocated virtual disk. We enabled storage for this virtual machine so that we don&rsquo;t need to repeat the installation process every time we launch the VM. One thing to be noticed here is we don&rsquo;t need swap area inside a virtual machine. We can simply use the whole virtual disk for <code>/</code> partition.</p><p>To start the VM from cmd, you might need to change the owner of the disk image. We add the user to both <code>kvm</code> and <code>libvirt</code>. The image created or accessed by virt-manager seems to change the file owner to libvirt-qemu, which may cause problems when starting from cmd.</p><p>After the installation is finished, we can simply launch the virtual machine inside virt-manager through its GUI interface. We can also use command line to start the VM:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kvm -accel kvm -m 8G -smp <span style=color:#ae81ff>6</span> --snapshot -drive format<span style=color:#f92672>=</span>raw,file<span style=color:#f92672>=</span>/home/ed/virtimg/ubuntu20.04
</code></pre></div><p>The argument <code>-accel kvm</code> enables Kernel-based Virtual Machine full virtualization, which uses hardware acceleration. Without this option the VM will become extremely slow. The <code>-m 8G</code> assigns the given amount of memory to the VM. The <code>-smp 6</code> assigns the given number of cores to the guest if the host has multiple cores. The <code>--snapshot</code> ensures that no changes are made to your image during an execution so you can do something dangerous and have the original image file preserved. The <code>-drive</code> option specifies the location of the virtual disk and its format. We will use some of these options later.</p><p>To confirm the VM has internet access, simply execution <code>apt install pkg-name</code> in the guest terminal. No error message would indicates properly functioning network access from the guest VM. For example, when we execute <code>sudo apt install llvm</code> it shows:</p><pre><code>Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  llvm-runtime
The following NEW packages will be installed:
  llvm llvm-runtime
0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.
Need to get 6,796 B of archives.
After this operation, 128 kB of additional disk space will be used.
Do you want to continue? [Y/n] 
</code></pre><h2 id=building-the-kernel>Building the Kernel</h2><p>We can use out customized kernel for our newly created VM. After we obtain the Linux kernel from <a href=https://www.kernel.org/>kernel.org</a>, we extract the source into &lt;kernel dir> and create a separate build directory &lt;kbuild> (outside &lt;kernel dir>).</p><p>Then we enter the &lt;kbuild> directory, run</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>yes <span style=color:#e6db74>&#34;&#34;</span> | make -C /home/ed/Desktop/linux_kernel/kbuild O<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span> config
</code></pre></div><p>This will create a <code>.config</code> file inside &lt;kbuild> with the default options selected. We then open the configuration file and ensures <code>CONFIG_SATA_AHCI=y</code>, which builds the SATA disk driver into the kernel. That will allow your kernel to boot off a (virtual) SATA drive without having to load a module to do it.</p><p>Next we build the kernel by running <code>make</code> in &lt;kbuild>. We use the -j 6 option speedup the building process using multiple processor cores. This process can take a long time.</p><h2 id=build-and-install-kernel-modules>Build and Install Kernel Modules</h2><p>To build modules locally on host, we create another separate &lt;install_mod_dir> directory for building kernel modules. Then in &lt;kbuild>, execute</p><pre><code>make INSTALL_MOD_PATH=/home/ed/Desktop/linux_kernel/install_mod_dir modules_install 
</code></pre><p>Now there is a <code>lib</code> directory inside <code>/home/ed/Desktop/linux_kernel/install_mod_dir</code>, which holds all the kernel modules we are about to install.</p><p>The complete list of modules can be listed using <code>cat modules.builtin</code> inside <code>lib/moduels/5.8.6</code>. Here is a <a href=https://gist.github.com/BDHU/4d31d18ad106a13caceac4a961d04a44>link</a> to all the modules being built. We didn&rsquo;t modify anything in the configuration.</p><p>Then we use guestmount to mount the virtual disk to a mount point on the host</p><pre><code>guestmount -a /home/ed/virtimg/ubuntu20.04 -i ~/vm/linux/
</code></pre><p>In Ubuntu this step yields the following message:</p><pre><code>libguestfs: error: /usr/bin/supermin exited with error status 1.
To see full error messages you may need to enable debugging.
Do:
  export LIBGUESTFS_DEBUG=1 LIBGUESTFS_TRACE=1
and run the command again.  For further information, read:
  http://libguestfs.org/guestfs-faq.1.html#debugging-libguestfs
You can also run 'libguestfs-test-tool' and post the *complete* output
into a bug report or message to the libguestfs mailing list.
</code></pre><p>The underlying problem is that the kernel cannot be read and according to the <a href=https://askubuntu.com/questions/1046828/how-to-run-libguestfs-tools-tools-such-as-virt-make-fs-without-sudo>post</a> and the <a href=https://bugs.launchpad.net/fuel/+bug/1467579>bug report</a> on Ubuntu Launchpad.</p><p>To fix the issue, we need to run</p><pre><code>sudo chmod +r /boot/vmlinuz-*
</code></pre><p>We can verify the contents inside ~/vm/linux by simply cd into it.</p><p>To install the modules we just built, we can copy the <code>&lt;install_mod_dir>lib/modules</code> into the mounted filesystem <code>&lt;mount_point>/lib/modules</code>.</p><p>Finally, we unmount the filesystem by doing</p><pre><code>fusermount -u /mnt/hdd1/vm/linux
</code></pre><h2 id=booting-kvm-with-new-kernel>Booting KVM with new Kernel</h2><p>To boot up the VM with the new kernel, we will add a few extra command line options to kvm. For convenience, we put the scripts into a file. It&rsquo;s also available on <a href=https://gist.github.com/BDHU/8c6ab518ab37571a1cae132d79ac9a9e>gist</a>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span>
kvm <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -s <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -display gtk <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -cpu host <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -vga qxl <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -accel kvm <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -kernel <span style=color:#e6db74>&#34;/home/ed/Desktop/linux_kernel/kbuild/arch/x86/boot/bzImage&#34;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -append <span style=color:#e6db74>&#34;root=/dev/sda1 console=ttyS0,115200n8 nokaslr&#34;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -drive format<span style=color:#f92672>=</span>raw,file<span style=color:#f92672>=</span>/home/ed/virtimg/ubuntu20.04 <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -m 8G <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -smp <span style=color:#ae81ff>6</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --snapshot <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    -S
</code></pre></div><p>Aside from the command line arguments we discussed before, there are a few new members here. the <code>-s</code> switch is a shorthand for <code>-gdb tcp::1234</code>. The <code>-display gtk</code> is optional. It enables the opengl context in the display device for gtk display output. <code>-cpu host</code> says the guest should emulate the host processor. <code>-vga qxl</code> enables 3D acceleration on the guest system. <code>-vga virtio</code> also offers good performance in our case. <code>-kernel</code> allows bootloader to pickup the new kernel. The <code>-append</code> along with its arguments specifies where the root partition of the hard disk is and the console parameter adds a serial console at boot so you can see boot messages. The <code>--snapshot</code> in QEMU says the images that refer to an original image will use Redirect-on-Write to avoid changing the original image. The <code>-S</code> means the kernel won&rsquo;t start executing unless we attach a debugger to it. We only use it later in the debugging stage.</p><p>Again, we can verify there is internet access using the new kernel using <code>apt update</code>. There are no errors shown, which indicates the network is functioning correctly.</p><h2 id=booting-process>Booting Process</h2><p>Now we are able to boot up the VM successfully, we can first measure how much time the kernel spends in booting. Running <code>dmesg -d</code> shows the timestamp and time delta spent between messages. The final line shows <code>[10.842998]</code>. If we use <code>systemd-analyze</code>, it outputs</p><pre><code>Startup finished in 795ms (kernel) + 5.451s (userspace) = 6.247s
graphical.target reached after 5.439s in userspace
</code></pre><p>The reason why there is a gap between these two measurement is because <code>dmesg</code> is not a reliable test of how long a boot-up process goes. <code>dmesg</code> itself merely collects information. The drivers and other system processes can output messages at any point in time. There may or may not be processes spawning between those messages.</p><p>Next, we are going to look at how PCI device is involved in kernel startup. <code>lspci</code> outputs the follow</p><pre><code>00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)
00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]
00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]
00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)
00:02.0 VGA compatible controller: Red Hat, Inc. Virtio GPU (rev 01)
00:03.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 03)
</code></pre><p>We can use the PCI address here to search for corresponding information in <code>dmesg</code>. For example, if we use the domain value \(0000:\) as query, we get something like:</p><pre><code>[    0.295026] PCI host bridge to bus 0000:00
[    0.299055] pci 0000:00:00.0: [8086:1237] type 00 class 0x060000
[    0.300133] pci 0000:00:01.0: [8086:7000] type 00 class 0x060100
[    0.301163] pci 0000:00:01.1: [8086:7010] type 00 class 0x010180
[    0.311006] pci 0000:00:02.0: [1af4:1050] type 00 class 0x030000
[    0.319650] pci 0000:00:03.0: [8086:100e] type 00 class 0x020000
</code></pre><p>The full result is also available as <a href=https://gist.github.com/BDHU/4d31d18ad106a13caceac4a961d04a44#file-dmesg_output>gist</a>.</p><p>The <code>lspci</code> command specifies the type of device right after the address. For example, the first one is host bridge. We specifically selected the message in the <em>type 00 class</em> format here. The significance here is that the class value actually telss us the type of the corresponding device. We can check the <a href=https://github.com/torvalds/linux/blob/master/include/linux/pci_ids.h>include/linux/pci_ids.h</a> for each macro respectively. For example,</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>#define PCI_CLASS_NETWORK_ETHERNET	0x0200
</span></code></pre></div><p>this line shows the value 0x0200 corresponds to a network PCI device. This aligns with our <code>dmesg</code> output as well as the <code>lspci</code> result.</p><h2 id=debugging-kernel>Debugging Kernel</h2><p>To build KVM+GDB-friendly kernel, we need to have proper CONFIG_DEBUG* options set in the .config file. More specifically, we need to have the following options enabled:</p><ul><li>CONFIG_DEBUG_INFO y: compile the kernel with debug info. The full list of definitions can be found <a href=https://cateee.net/lkddb/web-lkddb/DEBUG_INFO.html>here</a>.</li><li>CONFIG_DEBUG_INFO_DWARF4 y: generate dwarf4 debug info. Definition can be found <a href=https://cateee.net/lkddb/web-lkddb/DEBUG_INFO_DWARF4.html>here</a>.</li><li>CONFIG_GDB_SCRIPTS y: creates the required links to GDB helper scripts in the build directory. Full definition can be found <a href=https://cateee.net/lkddb/web-lkddb/GDB_SCRIPTS.html>here</a>.</li><li>CONFIG_GDB_INFO_REDUCED n: disable reduced gdb info.</li><li>CONFIG_KGDB y: kernel debugging location. Full list of definitions found <a href=https://cateee.net/lkddb/web-lkddb/KGDB.html>here</a>.</li><li>CONFIG_FRAME_POINTER y: compile the kernel with frame pointers. Full list of definitions found <a href=https://cateee.net/lkddb/web-lkddb/FRAME_POINTER.html>here</a>.</li><li>CONFIG_SATA_AHCI y: this option enables support for AHCI Serial ATA. Definition found <a href=https://cateee.net/lkddb/web-lkddb/SATA_AHCI.html>here</a>.</li><li>CONFIG_KVM_GUEST y: this option enables various optimizations for running under the KVM hypervisor. Definition found <a href=https://cateee.net/lkddb/web-lkddb/KVM_GUEST.html>here</a>.</li><li>CONFIG_RANDOMIZE_BASE n: drop support for Kernel Address Space Layout Randomization (KASLR). Definition found <a href=https://cateee.net/lkddb/web-lkddb/RANDOMIZE_BASE.html>here</a>. We also added <code>nokaslr</code> in our qemu arguments.</li><li>CONFIG_SMP y: enable Symmetric multi-processing support. Definition found <a href=https://cateee.net/lkddb/web-lkddb/SMP.html>here</a>.</li></ul><p>Now we can recompile the kernel and attack gdb to it. We simply add <code>-S</code> option to kvm to only start the VM when gdb is attached. Then we enter our &lt;kbuild> directory and execute:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>gdb vmlinux
<span style=color:#f92672>(</span>gdb<span style=color:#f92672>)</span> target remote:1234
</code></pre></div><p>The step is also documented in the kernel community <a href=https://www.kernel.org/doc/html/latest/dev-tools/gdb-kernel-debugging.html>documentation</a>.</p><h2 id=set-breakpoints>Set Breakpoints</h2><p>Spin lock is easy to find in a kernel. Therefore, we will set break points on <code>spin_lock</code>. For kernel 5.8.6, we see that <code>spin_lock</code> is defined in <a href=https://elixir.bootlin.com/linux/v5.8.6/source/include/linux/spinlock.h#L351>https://elixir.bootlin.com/linux/v5.8.6/source/include/linux/spinlock.h#L351</a> as a inline function. If we trace the function, we can see the actual function we should use is <code>_raw_spin_lock</code> defined <a href=https://elixir.bootlin.com/linux/v5.8.6/source/kernel/locking/spinlock.c#L149>here</a>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#75715e>#ifndef CONFIG_INLINE_SPIN_LOCK
</span><span style=color:#75715e></span><span style=color:#66d9ef>void</span> __lockfunc <span style=color:#a6e22e>_raw_spin_lock</span>(raw_spinlock_t <span style=color:#f92672>*</span>lock)
{
	__raw_spin_lock(lock);
}
</code></pre></div><p>If we need to break the execution only when a given program is executed, we can use the program PID to as the condition. The problem is, how do we get the program PID if it doesn&rsquo;t last for long?</p><p>We could instead first set a breakpoint on <code>fork</code>. We can break its kernel call at <code>_do_fork</code> which is defined <a href=https://elixir.bootlin.com/linux/v5.8.6/source/kernel/fork.c#L2416>here</a>. After that, we can simply continue executing the kernel until we run the program.</p><blockquote><p>Note: we need to compile the program and open a new terminal first. Since they both involves forking new processes, which will hit <code>_do_fork</code> before our program runs.</p></blockquote><p>Then we print the process PID using <code>p $lx_current().pid</code>, we then use this value as the condition for <code>b _raw_spin_lock if $lx_current().pid == pid_value</code> inside gdb.</p><p>If we want <code>_raw_spin_lock</code> to break under different contexts, we can simply use PID as different contexts. We can also set break points in functions in different contexts that calls <code>spin_lock</code> and see what they do. For example, we can set break point at <code>expand_downwards</code> defined in <a href=https://elixir.bootlin.com/linux/v5.8.6/source/mm/mmap.c#L2428>here</a>, if we back trace this function, we will get a series of calls, we mention the important ones here</p><pre><code>#1  0xffffffff81284c4e in expand_stack
#3 0xffffffff813843db in load_elf_binary
#8  do_execve
#12 0xffffffff81b1f658 in do_syscall_64
</code></pre><p>We also added a helper script in .gdbinit to print our the name of the function, which is &lsquo;&lsquo;anacron&rsquo;&rsquo; in this case.
In short, this process execute commands periodically, and it performs a sys call which loads elf binary, thus requiring stack expansion.</p><p>Another example is timer interrupt. The <code>get_next_timer_interrupt</code> calls <code>_raw_spin_lock</code>. We select some messages from backtrace:</p><pre><code>#1  0xffffffff8113b224 in get_next_timer_interrupt
#2  0xffffffff8114d52e in tick_nohz_next_event
#4  tick_nohz_idle_stop_tick ()
#5  0xffffffff810df567 in cpuidle_idle_call ()
</code></pre><p>In short, the is a timer interrupt that gets called when CPU is idle.</p><p>The last example is <code>hrtimer_interrupt</code>. The selected messages are:</p><pre><code>#4  0xffffffff8114d80c in tick_sched_timer
#7  0xffffffff8113c8e7 in hrtimer_interrupt
#12 run_on_irqstack_cond
#14 0xffffffff81c00cc2 in asm_sysvec_apic_timer_interrupt
</code></pre><p>In summary, <code>hrtimer_interrupt</code> is called as event handler. This function is responsible to select all timers that have expired and either move them to the expiration list (if they may be processed in softIRQ context) or call the handler function directly.</p><h2 id=syscall>Syscall</h2><p>Essentially, processor switches from the user mode to kernel mode and starts execution of the sys call entry - <code>entry_SYSCALL_64</code>, we can find its definition at <a href=https://elixir.bootlin.com/linux/v5.8.6/source/arch/x86/entry/entry_64.S#L94>here</a>. This is the only entry point used for 64-bit system calls. We can set a break point here. When the break point is hit, we use <code>info registers</code> in gdb to get the value of cr3. In our case, it is 0x22a6d5806. Then we simply step from this breakpoint, and will likely reach <code>SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp</code>. After this call the value in cr3 is changed to 0x22a6d4006. The macro is defined <a href=https://elixir.bootlin.com/linux/v5.8.6/source/arch/x86/entry/entry_32.S#L165>here</a>.</p><p>We can see whenever the processor switch from the user mode to kernel mode the value of cr3 is changed. The root cause the Page <a href=https://www.kernel.org/doc/html/latest/x86/pti.html>Table Isolation (PTI)</a>. It is a countermeasure against attacks on the shared user/kernel address space such as the &lsquo;&lsquo;Meltdown&rsquo;&rsquo; approach. To mitigate this class of attacks, two independent page table copies are created, one in kernel space, one in user space. The cr3 register enables the processor to translate linear addresses into physical addresses by locating the page directory and page tables for the current task. So whenever the process enters kernel mode, the kernel copy requires its page directory address to be loaded into cr3 register.</p><p>If we add <code>nopti</code> in <code>-append</code> in the QEMU cmd argument and perform the same steps. We get 0x231466005 before and after <code>SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp</code> is executed. Based on the desciption in the <a href="https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/Documentation/admin-guide/kernel-parameters.txt?h=v5.1.3#L3656">linux kernel tree</a>, the <code>nopti</code> on X86_64 is equivalent to pti=off, therefore explaining the constant value of cr3.</p></p></section></article></main><footer class=footer><span>&copy; 2021 <a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>