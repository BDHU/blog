<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>std::bodun::blog</title><meta name=keywords content="Blog,Portfolio,Bodun,Edward,Hu,BodunHu,EdwardHu,programming,coding"><meta name=description content="The Blog of Bodun Hu"><meta name=author content><link rel=canonical href=https://www.bodunhu.com/blog/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="226272F73598D749583CF29B80850392"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.b6b54b0bc92f8d5200fc17cdbbca79c96b40f704117fd12ff3a8e6e0a4cb076f.css integrity="sha256-trVLC8kvjVIA/BfNu8p5yWtA9wQRf9Ev86jm4KTLB28=" rel="preload stylesheet" as=style><link rel=icon href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/favicon-32x32.png><link rel=apple-touch-icon href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/apple-touch-icon.png><link rel=mask-icon href=https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><link rel=alternate type=application/rss+xml href=https://www.bodunhu.com/blog/index.xml><link rel=alternate type=application/json href=https://www.bodunhu.com/blog/index.json><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-108144808-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="std::bodun::blog"><meta property="og:description" content="The Blog of Bodun Hu"><meta property="og:type" content="website"><meta property="og:url" content="https://www.bodunhu.com/blog/"><meta property="og:image" content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="std::bodun::blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.bodunhu.com/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="std::bodun::blog"><meta name=twitter:description content="The Blog of Bodun Hu"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"std::bodun::blog","url":"https://www.bodunhu.com/blog/","description":"The Blog of Bodun Hu","thumbnailUrl":"https://raw.githubusercontent.com/BDHU/Page_pics/master/favicon_io/favicon.ico","sameAs":["https://twitter.com/","https://stackoverflow.com","https://github.com/"]}</script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://www.bodunhu.com/blog/ accesskey=h title="std::bodun::blog (Alt + H)">std::bodun::blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button>
<a href=/blog/search/ title="Search (Alt + S)" accesskey=s><svg xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg></a></span></div><ul id=menu><li><a href=https://www.bodunhu.com/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://www.bodunhu.com/blog/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://www.bodunhu.com/blog/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/networks/>Networks</a></h2></header><footer class=entry-footer>November 13, 2017&nbsp;·&nbsp;3 min</footer><section class=entry-content><p><p>The concept of a worldwide of networks of information was introduced long before the technology used to build the internet. The first workable prototype came in the late 1960s with the creation of ARPANET(The Advanced Research Projects Agency Network). The famous TCP/IP, or Transmission Control Protocol and Internet Protocol, was developed by Robert Kahn and Vinton Cerf in the 1970s. In the 1980s, research by Tim Berners-Lee gave birth to the World Wide Web, linking hypertext documents into an information system, making them accessible from any node on the network (<a href=https://en.wikipedia.org/wiki/History_of_the_Internet>History of Internet</a>). The implementation and the evolution of the internet has improved ever since. Today, for most users, the internet feels like smoke and mirrors since requiring everyone to understand the technical implementation will be way too harsh. However, as software developers, they are much more likely to deal with networks sometime in life. This article is meant to unveil technical details of networks mainly from a programmer&rsquo;s perspective so the focus will be put on the software side.</p><h2 id=the-os-view-of-networks>The OS View of Networks</h2><p>For the operating system, the network is perceived as an extra device. The Network Interface Controller(NIC), which is a hardward device used to connect the computer to a computer network, is added to the bus.The data can be transferred to/from memory to NIC through two methods: DMA or memory-mapped I/O. DMA refers to direct memory access. The name suggests that the hardware is able to read or write memory without the involvement of CPU. On the other hand, memory-mapped I/O means the CPU can control the hardware to read or write specific memory addresses, which means the CPU is doing the job of writing/reading to/from memory. DMA is usually used for high-bandwidth operations such as disk I/O while memory-mapped I/O is used in low-bandwidth operations like change control bits.</p><h2 id=layers-of-network>Layers of Network</h2><p>The OSI (Open Source Interconnection) 7 Layer Model devides network communication into seven layers. Here I will discuss each layer and its corresponding function. Starting from the lowest one:</p><p>Layer 1: This layer is the physical layer which is concerned with the transmission of unconstructed raw bit stream over the physical medium. Thus the protocal data unit(PDU) is bit for this layer.</p><p>Layer 2: This is the data link layer. Its function is in charge of reliably transfer the data frames between two nodes connected by a physical layer. The PDU of this layer is frame.</p><p>Layer 3: This is the network layer. This layer is in charge of structuring and managing a multi-node network. Examples includes addressing, routing, and load control. The PDU for this layer is packet.</p><p>Layer 4: This is the transport layer. It is used to deliver messages error-free, in sequence, and without duplications or losses. The PDU of this layer is segment/datagram(segment for TCP, datagram for UDP).</p><p>Layer 5: This is the session layer. It allows session establishment between process running on different stations. Layer 5 is often OS/Library. From here, the PDU will generalized to data.</p><p>Layer 6: This is the presentation layer. As the name suggests, it formats the data and present it to the application layer. It can be viewed as the translator for the network. It is usually OS/Library.</p><p>Layer 7: The final layer is called the application layer. It serves as the window for the users and appliaction processes to access the network.</p><p>Note the Department of Defense Four-Layer Model has only four catagories: the Network Access Layer(layer 1-2), the Internet Layer(layer 3), the Host-to-Host Layer(layer 4), and the Process Layer(layer 5-7).</p><h2 id=more-on-layer-2-network>More on Layer 2 Network</h2><p>There are three types of layer 2 networks: System Area Network(SAN), Local Area Network(LAN), and Wide Area Network(WAN).</p></p></section></article><article class=post-entry><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/filesystem/>File System Design</a></h2></header><footer class=entry-footer>October 30, 2017&nbsp;·&nbsp;5 min</footer><section class=entry-content><p><p>What exactly is a file system? The general concept is that the file system provides naming organization. It manages the physical disk layout such as picking a block constituting a file, balancing locality with expandability, and managing free space. It can translate from file name and offset to the actual data block. In a nutshell, it is a servant that manages all the dirty details of communicating the data between system and the hardware in an optimal way which you aren&rsquo;t required to understand so you can go on and do other things with your life. So what are the functionalities of file systems? In general, it providses file name organizations such as directories. It can manage disk layout by picking blocks that constitute a file, balancing locality with expandability, and manage free space. It can translate from file name and offset to the actual data block.</p><h2 id=file>File</h2><p>Let&rsquo;s start from and bottom-to-top pattern to describe file system by first introducing the most fundamental unit: the file itself. So a file is composed of two parts: the metadata and the actual data. The metadata is a <em>file header</em> that holds information about where the data is stored and attributes of the file such as a permission, access time, owner id, size, and so on. One thing to note is that meta data blocks are stored on a location that is known by the OS and thus it can be accessed without having to check another data structure. Then the actual data is the part users actually care about. There are two kinds of blocks (there can be more than these two data blocks but we will only discuss two here), The directory data block which maps file names to file headers, and file data block that contains information we care about.</p><h2 id=design-file-layout>Design File Layout</h2><p>There are several factors we need to take into consideration when designing file layout:</p><ul><li>Support for sequential and random access. Almost every file operation is either sequential or random.</li><li>Lay out the files on the physical disk.</li><li>Maintain file location information. This makes sense since we need an agent to keep track all files because we users are too lazy to do that.</li><li>In Unix most files are small in size so we need to support small files, which means block size can&rsquo;t be too large due to internal fagmentation.</li><li>Most disk space is consumed by large files so we also need to support large files and accessing them should be effcient as well.</li><li>I/O operations target both types of files.</li></ul><h2 id=block-vs-sector>Block VS Sector</h2><p>Before we dig deeper into file system design, it&rsquo;s important to note the the block size of file system is different from disk blocks size. According to <a href=http://www.nobius.org/~dbg/practical-file-system-design.pdf>Practical File System Design</a>,
block is the smallest unit writable by a disk or ﬁle system. Everything a ﬁle system does is composed of operations done on blocks. A ﬁle system block is always the same size as or larger (in integer multiples) than the disk block size. Also each blocks consists of consecutive sectors so that sequential access becomes possible. A larger block size increases transfer efficiency also because of sequential access since you don&rsquo;t have to move the head too many times, it may be convenient if the block size matches the machine&rsquo;s page size, this is because we don&rsquo;t have to switch pages assuming the block is bigger than the page size. Many systems allows transfer of many sectors between interrupts.</p><h2 id=allocation-methods>Allocation methods</h2><h4 id=contiguous-allocation>Contiguous Allocation</h4><ul><li>OS maintains an ordered list of free disk blocks.</li><li>OS allocates a contiguous chunk of free blocks when it creates a file.</li><li>Placement/allocation policy can be first-fit, best-fit, or worst-fit.</li><li>File header specifies starting block and length.</li><li>Pros:<ul><li>All file data stored contiguously on disk.</li><li>Simple to implement as bump pointer is a common way of implementation.</li><li>Best performance for initial write of a file due to locality resulted from contiguous allocaiton.</li></ul></li><li>Cons:<ul><li>External fragmentation because some allocation for large files are simply impossible, resulting in wasted unallocated space, and hard to grow file in size.</li><li>Later writes may cause the file to grow which would require it to be copied and moved.</li></ul></li></ul><h4 id=linked-allocation>Linked Allocation</h4><ul><li>Files are stored as a linked list of blocks, in each sector, there&rsquo;s a pointer pointing to the next sector. (This is a hardware implementation, we still use blocks fot later discussion.)</li><li>The file header keeps a pointer to the first and last sector/block allocated to that file.</li><li>There are two types of implementations for Linked allocation:<ul><li>Linked list of disk blocks, data blocks point to other blocks</li><li>Linked list in a table (FAT file system)</li></ul></li><li>Pros:<ul><li>Reduce or eliminate external fragmentation since blocks can fit in if there are free blocks available.</li><li>Easy to grow file just like adding elements into a linked list.</li><li>Linear access is somewhat efficient(It&rsquo;s linked list, what do you expect? O(1)?).</li></ul></li><li>Cons:<ul><li>linear random access time in linked list.</li></ul></li></ul><h3 id=fat-file-system-file-allocation-table>FAT File System (File Allocation Table)</h3><p>FAT32 file system is very important file system created by Microsoft. It was introduced the solve the volume problem posed by FAT16. Although named FAT32, only 28 of the 32 bits are actually used and the remaining 4 bits are &ldquo;reserved for future use&rdquo;. As a result, a FAT32 partition has a maximum cluster count of (268,435,455)2^28-1. I found this discription about FAT32 on <a href=https://superuser.com/questions/983139/why-is-fat32-limited-to-just-under-228-clusters>StackExchange</a> that is useful:</p><blockquote><p>Although VFAT was a clever system, it did not address the limitations of With the appearance of the FAT32 file system, the maximum number of clusters per partition went increased from 65535 to 268,435,455 (228-1). FAT32 thus allows much bigger partitions (up to 8 terabytes). Although the maximum theoretical size of a FAT32 partition is 8 TB, Microsoft has voluntarily limited it to 32 GB on Windows 9x systems to promote NTFS</p></blockquote><p>FAT32 is implemented in a completely different way . Unlike FFS in UNIX, each entry in in the MTF merely represents a block of data. Each block is able to point to another block, with multiple entries in the table to represent a file represented multiple blocks. Each file&rsquo;s file number is indicated using the index of the first entry in the MTF. Thus, in order to locate a specific block of a file, we need to search the MTF sequentially.</p></p></section></article><article class=post-entry><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/disks/>Disk Introduction</a></h2></header><footer class=entry-footer>October 25, 2017&nbsp;·&nbsp;4 min</footer><section class=entry-content><p><p>This chapter is all about disk. Before we start. We won&rsquo;t go deep into the mechanical part of disk operation; rather we will be focusing on general concept related to disk and algorithms to improve disk performance.</p><p align=center><a href=https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter10/10_01_DiskMechanism.jpg><img src=https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter10/10_01_DiskMechanism.jpg width=80%></a></p><h2 id=the-evaluation-criteria>The Evaluation Criteria</h2><p>Here we are introduing the basic components used to evaluate the performance of disk operation.</p><h4 id=seek-time>Seek Time</h4><p>This is the time to position the head over the track. Maximum can be going from innermost track to outer most track. It usually ranges from 10ms to over 20 ms. However, the average seek time is usually to seek 1/3 of the way across the disk.</p><h4 id=head-swtitch-time>Head Swtitch Time</h4><p>This is time spent to move from one track on one surface to another track on a different surface. The range is similar to seek time.</p><h4 id=rotation-delay>Rotation Delay</h4><p>This is the time spend for the sector to spin underneath the head. It varies depends on how fast the disk rotates.</p><h4 id=transfer-time>Transfer Time</h4><p>The time spend to read or write sector as it spins by.</p><ul><li>Transfer time: time to move the bytes from disk to memory</li><li>Surface transfer time: time to transfer one or more
sequential sectors to/from surface after head reads/writes
first sector</li><li>Host transfer time: time to transfer data between host
memory and disk buffer</li></ul><h2 id=disk-head-scheduling-mainly-focusing-on-hdd>Disk Head Scheduling (Mainly focusing on HDD)</h2><p>Now we&rsquo;ve looked at the basic performance evaluation critiria for HDD, it&rsquo;s reasonable to discuss how to reduce head movement so that the amount of time spent on moving the head from on track to the other will decrease because the disk I/O request for can be stored in a queue. (Note the seek time takes the most amount of time so it&rsquo;s reasonable to reduce it.)</p><h4 id=fifo>FIFO</h4><p>This technique is easy to understand, the head will move to the corresponding track based the order of the queue of requests. Since the requested data can be read/writen on random tracks, the performance can be heavily affected.</p><h4 id=sstf-shortest-seek-time-first>SSTF (Shortest Seek Time first)</h4><p>The queue of requests is reordered such that the head will only look for the closest track it can move to and thus ignore the global state of locations of all requests. This is a greedy algorithm and thus can be trapped in local optimal value.</p><h4 id=scanelevatorlook>SCAN/Elevator/LOOK</h4><p>Simply move the head to one direction until the request that is closest to that end of the disk is reached, then reverse the direction of the head and find the rest of the requests.</p><ul><li>Optimization: the head is reset when no more requests exist between the
current head position and the approaching edge of the disk (LOOK scheduling)</li></ul><h4 id=c-scanc-look-circular-scan-scheduling>C-SCAN/C-LOOK (&ldquo;Circular Scan&rdquo; scheduling)</h4><p>Move the head in one direction until an edge of the disk is reached and then reset to the opposite edge.
optimization: the head is reset when no more requests exist between the current head position and the approaching edge of the disk (called C-LOOK
scheduling).</p><ul><li>Note the only difference between SCAN and C-SCAN is that in C-SCAN, after the head reaches one edge, an optimized jump implemented by the hardware is used to directly move the head to the opposite edge instead of reversing the movement direction.)</li></ul><h2 id=partitioning>Partitioning</h2><p>Disks are partitioned in order to minize the largest seek possible time since each partition is a logically seperate disk. (It&rsquo;s just merely a collection of cylinders.) More information covering partitioning will be covered in file system.</p><h2 id=other-techiniques-to-reduce-overhead>Other Techiniques to Reduce Overhead</h2><p>To minimize rotational latency and seek time, we can also:</p><ul><li>Make disks smaller (less movement distance)</li><li>Spin disks faster</li><li>Schedule disk operations to minimize head movement(we&rsquo;ve just discussed)</li><li>Lay out data on disk so that related data is on nearby tracks(locality and also less movement)</li><li>Place commonly used files on disk</li><li>Block size: (note disk is presented with sector address using logical block address converted by the controller)<ul><li>Too small: low transfer rate because we need to perform more seeks for same amount of data.</li><li>Too big: internal fragmentation</li></ul></li></ul><h2 id=ssd>SSD</h2><p>The basic advantage of SSD is that it doesn&rsquo;t have moving parts and thus random access is blazingly fast. It&rsquo;s implemented using NAND and is non-volatile.</p><h4 id=basic-nand-flash-units>Basic NAND Flash Units</h4><p align=center><a href=https://www.silicon-power.com/blog/wp-content/uploads/2018/04/NAND-Flash-Die-Architecture-1024x710.png><img src=https://www.silicon-power.com/blog/wp-content/uploads/2018/04/NAND-Flash-Die-Architecture-1024x710.png width=85%></a></p><p>The fundemental unit is a page which is 4KB. 128 pages are organized together forming a block of size 512KB. Each block is the unit forming a plane. There are 1024 blocks on one plane and the size of the plane is 512MB.</p><h4 id=operations>Operations:</h4><ul><li>Read page: fast in terms of nano seconds compared to micro seconds for spinning disk.</li><li>Write page: can only write to empty page, same as above.</li><li>Erace block: (ms) Before a page can be written, all bits in the page need to be set to 1. Note the only way to set bits in a page to 1 is to erase the whole block.</li><li>Read and Write occur in page unit.</li></ul></p></section></article><article class=post-entry><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/virtualaddressmechanism/>Virtual Memory Mechanisms</a></h2></header><footer class=entry-footer>October 19, 2017&nbsp;·&nbsp;10 min</footer><section class=entry-content><p><p>As we can see in the <a href=https://bdhu.github.io/2017/10/08/virtualaddress/>previous post</a>,
all allocation algorithms we discussed lead to external fragmentation. As time goes by, external fragmentation is going to get worse and we need solutions for the problem. We can use swap areas to swap out memory onto the disk, or move allocated memory together(a process named memory compaction), leaving empty spaces together. Even these approaches can reduce external fragmentation and allow a higher degree of multiprogramming, they are not perfect. In addition, it is possible to have a single process that is just too big to fit into memory. We are going to discuss methods used to completely eliminate external fragmentation. More than that, we will discuss how to make memory sharing possible and how to allow more processes to execute at once.</p><h2 id=too-big-to-fit>Too big to fit</h2><p>It&rsquo;s easy for us to assume the amount of available memory that can be allocated is not a big problem. It&rsquo;s easy for programmers to assume the available memory resource is almost infinite and thus we rarely care about the situation in which the code we wrote is going to occupy all memory resource. But let&rsquo;s just consider the scenario where we create a program which later create a process that is just too big to fit into memory, what should we do?</p><p>The natural response would be: just cut them into pieces! This is a technique called overlay: programmers manually cut the program into pieces, or <em>overlays</em>. When the program executes, a overlay manager is created to swap pieces in and out, allowing only necessary pieces in memory at a given time. But tell me, what is the last time you see an user-level application manually cut into &ldquo;pieces&rdquo; by the programmer? Doing things manually is not desired trait of a programmer. Programmers should always be lazy and automate things, or just leave it to someone else!</p><h2 id=paging>Paging</h2><p>I&rsquo;m pretty sure you don&rsquo;t like the idea of overlaying as it requires you to do things manually. That&rsquo;s where paging comes into play. Instead of dividing the program by the programmer, why don&rsquo;t we let the system do the dirty job? Before we start, I&rsquo;m going to throw two questions to you: why can a virtual address space be bigger than the physical memory? How are each piece of a process brought into the memory?</p><p>The technique to divide a address space into fixed-size <em>pages</em> is called paging. A copy of the address space is stored on the disk. The physical memory is viewed as a series of equal-sized <em>page frames</em>. We will discuss later about how the system choose to load a page into a frame and how to manage pages that are currently in memory.</p><p>So how do we use virtual addresses with the our recently introduced pages to find a location in memory? As we can see, a virtual address space is divided into pages, each with a fixed number of entries. In order to represent the number of pages and number of entries, we need two variables:</p><p>p - page number(p<sub>MAX</sub> pages)<br>o - page offset (difference between the byte address to search and the start of the page, <sub>MAX</sub> indicates the total number of entries in a table)<br>Virtual Address calculation: o<sub>MAX</sub> x p + o (here o is the offset in the last page)</p><p>The frame size is equal to that of a page. It&rsquo;s easy to understand since we need to put everything stored in a page into the frame, we need them both to be equally sized. Note that since virtual address space can be bigger than physical memory, the number of frames can be smaller than the number of pages, which means the number of bits reserved for frame number can be smaller than the number of bits used to indicate the number of pages.</p><p align=center><img src=https://www.bottomupcs.com/chapter05/figures/virtaddress.png></p><p align=center><a href=https://www.bottomupcs.com/virtual_addresses.xhtml>source</a></p><h2 id=from-virtual-to-physical-allocation-policy>From Virtual to Physical: Allocation Policy</h2><p>We&rsquo;ve discussed how that a process&rsquo;s virtual address space can be divided into pages and mapped to frames in physical memory. Here we are going to discuss some policies used to implement the mapping process. I&rsquo;m going to leave three questions to think here as well: why pages are arbitrarily located in physical memory? How do we find them if they are arbitrarily located in physical memory? Why aren&rsquo;t all pages mapped to frames? These questions will become more clear as we progress into further discussion.</p><p>Here&rsquo;s the solution: a page table. Each process has one page table that contains all mapping information for each possible virtual address belonged to that process. Even though we call it table, it&rsquo;s merely a data structure used by a virtual memory system in a computer operating system to store the mapping between virtual addresses and physical addresses. However, the mapping is invisible to the process. The protection mechanism is the same as dynamic relocation we&rsquo;ve discussed before.</p><h2 id=virtual-address-translation>Virtual Address Translation</h2><p>Now we are going through a step-by-step description of address how to translate virtual address to physical address.</p><ul><li><p>First, the process will give the CPU a virtual address to translate.</p></li><li><p>Then MMU will split the address into two parts, the page number and the offset.</p></li><li><p>Since the size of a page and a frame are the same, the offset of the virtual address is sent along without no modification to the physical memory.</p></li><li><p>Use page number to find the corresponding entry in the page table.</p></li><li><p>Check if the page exists in physical memory.</p></li><li><p>If the page does exist in physical memory, the frame number is sent along. If the requested page is on the disk, then the corresponding page is moved to memory and frame number is recorded now.</p></li><li><p>Offset is appended to the end of the frame number to get the physical address.</p></li></ul><p align=center><img src=https://upload.wikimedia.org/wikipedia/commons/8/8d/Memory_paging.jpg></p><p>So, we&rsquo;ve achieved several goals now by using paging technique:</p><ul><li><p>Reduce or even eliminate external fragmentation.</p></li><li><p>Easy to grow processes.</p></li><li><p>Allow more process that is too big to fit into memory to be able to execute.</p></li><li><p>Easy to allocate and deallocate.</p></li><li><p>Share memory is between processes is easy since memory used by difference processes no longer has to be contiguous. Even pages may exist in different position, they can be mapped to the same physical address in memory.</p></li></ul><h3 id=more-about-page-table>More about Page Table</h3><p>One thing to notice is that there&rsquo;s only one page table for each process. The page table is part of the process&rsquo;s state and serves as protection mechanism to prevent processes accessing each other&rsquo;s memory. There&rsquo;re several elements in each page table entry as well:</p><ul><li><p>Flags: dirty bit, resident bit, and dirty bit (we will talk about them later). Flag is stored at the beginning of each entry.</p></li><li><p>Frame number: stored in the remaining bits. It tells where the page lives in physical memory.</p></li></ul><p>However, page table still has its disadvantages, the most important thing to notice is that we need two memory accesses to implement virtual memory reference, first access is to get the page table entry, the second access is used to get the actual data from memory, if it&rsquo;s present. As we know, memory access is extremly slow and expensive, thus we need something faster.</p><h2 id=translation-lookaside-buffer-tlb>Translation Lookaside Buffer (TLB)</h2><p>Since it&rsquo;s hard to improve the speed from the algorithm side, let&rsquo;s just drop the algorithm for a minute and switch our focus onto the hardware. Here we will discuss how to improve the speed of memory reference by adding a hardware component called TLB. Here are several basic characteristics of TLB:</p><ul><li><p>The TLB holds recently used frame/page pairings.</p></li><li><p>It has high hit ratios due to locality.</p></li><li><p>For a TLB hit, translation can be finished in one cycle.</p></li></ul><p>So how does TLB help with efficiency? It&rsquo;s actually really simple. The system simultaneously sends the page number to both page table and TLB. If there&rsquo;s TLB hit, then the TLB cache sends the frame number to the memory without having to look into the page table, which avoids the first reference into the memory to find the page table. If there&rsquo;s missing TLB, everything stays the same: look for the page table in memory and update the TLB.</p><p align=center><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/b/be/Page_table_actions.svg/500px-Page_table_actions.svg.png></p><p align=center><a href=https://commons.wikimedia.org/wiki/File:Page_table_actions.svg>source</a></p><h2 id=problems-with-page-table>Problems with Page Table</h2><p>Now we solved the problems of external fragmentation. It seems paging works like a charm and makes things a lot easier. However, we notice it&rsquo;s still not perfect in terms of space usage:</p><ul><li><p>Data structure overhead (The page table can be huge!)</p></li><li><p>Inappropriate page size can lead to internal fragmentation, and less processes to exist in memory in the same(page too big)!</p></li></ul><p>Thus we need more complex methods to solve the above issues.</p><h2 id=multi-level-page-tables>Multi-level Page Tables</h2><p>The basic concept of multi-level page table is to subdivide page number into n parts(n stands for number levels of pages tables). n is decided by the architecture of the system. Each entry in each page table which is exists each entry points to the corresponding page table in the next level. The last level page table entries hold the bit/frame information associated to the page table entry.</p><p>SO how does it work exactly? First, we have only one first-level page table. We extract the first subdivision of the virtual address, added to the PTBR to get the corresponding entry in the first-level page table. Then we extract the second subdivision of the virtual address, add it to the address of the beginning of the second-level page table which we got from the corresponding first-level page table entry. This process continues until we reach the last-level page table. From the corresponding entry we can get the frame number. The offset is preserved so we just need to append the offset to the frame number and we&rsquo;re done! One reminder is that multi-level page table requires several lookups to eventually find the frame number, so TLB becomes extremely important here in terms of improving performance.</p><h3 id=how-does-multi-level-page-table-save-space>How does multi-level page table save space?</h3><p>You&rsquo;re probably still confused why multi-level page table saves space by adding <strong>more tables</strong>. Don&rsquo;t worry, I will walk you through an example to illustrate the magic behind the scene:)</p><p>Assume a process has a ((2^{10}\) pages, each PTE occupying 4 bytes (32-bit system). Without multilevel page table, we need \(2^{20} \times 4 = 4MB\) for one page table stored in memory. Even we just need a portion of all pages, we need the whole page table present in memory to find the corresponding frames. Now, if we divide the virtual address into 3 sections with last one being the offset, we have a two-level page table. The first 10 bits are used to index the page table in the first level and the next 10 bits are used to index the page table in the second level. If we only need virtual addresses that have the second 10 bits modified and leave the first 10 bits untouched, then we only need to find one entry in the first-level page table. Since the first-level page table has to be always present in memory, it will consume \(2^{10} \times 4=4KB\) memory space. Now, since we need every entry in a second-level page table pointed by the entry we just found in an entry in the first-level page table, it requires \(2^{10} \times 4bytes = 4KB\) memory. So we only need to use 4 + 4 = 8KB for all memory we need instead of 4MB without multi-level page tables.</p><p>Another interesting fact is that, even if we need to use all pages of a process, multi-level page table will potentially increase the space needed. Let&rsquo;s take the above example and assume we need every single pages from a process. Then we need to store the first-level page table, which takes $2^{10} \times 4bytes = 4KB$. Then, for each entry in the first-level page table, there&rsquo;s a corresponding second-level page table, each with the size of \(2^{10} \times 4 = 4KB\). Since the first-level table has \(2^{10}\) entries, the total number of second-level page tables is \(2^{10}\), each with the size of 4KB, so the total amount of spaces is \(2^{10} \times 4kb + 4kb = 4MB + 4KB\). Then bottom line is: if we need to map every pages to its frames, then the total amount of entries in the last level will be the number of pages regardless of how many levels we use since each page has to have a mapping. Under such case, the total amount of memory used by the last-level page tables will be equivalent to the amount used when we use only one huge page table. The additional space comes from the upper levels, but the previous level will only save the corresponding number of entries. (number of table in the next level).</p><p align=center><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/X86_Paging_PAE_4K.svg/440px-X86_Paging_PAE_4K.svg.png></p><p align=center><a href=https://en.wikipedia.org/wiki/Page_table>source</a></p></p></section></article><article class=post-entry><header class=entry-header><h2><a href=https://www.bodunhu.com/blog/posts/virtualaddress/>Virtual Memory Overview</a></h2></header><footer class=entry-footer>October 8, 2017&nbsp;·&nbsp;7 min</footer><section class=entry-content><p><p>I love pointers. Pointer is very a useful feature in programming languages like C/C++. I can pass weird hexidecimal numbers to a function and then it will magically locate where the program is in memory. However, all those values we see are merely virtual addresses, a running program&rsquo;s view of memory in system. Any address we can see while programming user-level programs is a virtual address. It is no more than an illusion of where the data is actually laid out in memory. Only the almighty OS knows where the data actually locates in physical memory.</p><h2 id=three-goals-of-vm>Three Goals of VM</h2><p>When the OS provides this layer of abstraction to translate virtual address to physical address, we say that the OS is <em>virtualizing memory</em>. In order to achieve virtualization, there&rsquo;re three goals to keep in mind.</p><p>Transparency: The OS will implement transparency in a way that is invisible to the running program. Usually, transparency would suggest a clear understanding of how things work. However, when we are talking about VM, it means the running program is unaware of the fact its memory is translated by the OS(and hardware) behind the scene.</p><p>Efficiency: The OS should do its best to ensure the efficiency of virtualization, in both time and space. Some of the methods used to improved efficiency, including hardware components like translation lookaside buffer(TLB), will be discussed in the following chapter.</p><p>Protection: Being able to protect processes from interfering each other or even the OS is important. When one process performs actions like read and write, the process should be isolated so that it&rsquo;s unable to modify the data of other processes or behave maliciously.</p><h2 id=basic-concept-address-space>Basic Concept: Address Space</h2><p>Before we start, there are several terms I will use throughout the discussion so it&rsquo;s better to get familiar with them.</p><p>Physical address space: it is merely a collection of physical addresses used by the hardware. The value of the address can range from 0 to the MAX<sub>sys</sub>. The address is actually utilized by the physical memory to fetch the contents inside.</p><p>Logical/Virtual address: It a collection of address a process is able to access (the process is not aware of the actual physical location). It can be bigger than physical address due a technique called <strong>paging</strong> which will be discussed later.</p><p>Segment: A chunk of memory assigned to the process to use.</p><p align=center><img src=https://upload.wikimedia.org/wikipedia/commons/d/dc/MMU_principle_updated.png></p><p align=center><a href=https://en.wikipedia.org/wiki/Memory_management_unit>Image source</a></p><h2 id=how-does-an-address-get-generated>How does an address get generated?</h2><p>Uniprogramming: This is a very simple scenario, there&rsquo;s only one process executing at the moment, its address always starts at 0 and the OS is loaded in a fixed part of the memory. The process executes in contiguous section of memory. The process is able to use the all the available portions of the memory as long as it doesn&rsquo;t touch the OS&rsquo;s property.</p><h3 id=relocation>Relocation</h3><p>Same as uniprogramming, the OS locates at the highest location in the memory. The OS allocates a contiguous segment of memory for processes to use. If it doesn&rsquo;t have enough space, it simply wait the process to terminate. The relocation address (also known as base address) is the first physical address a process can use. Limit address is the largest physical address the process can use.</p><p>Also note there&rsquo;re two types of relocation:</p><ul><li><p>static: The address is produced during load time and the process is load into the given location to execute. The OS can&rsquo;t move it as soon as the process is loaded in. Note the static address can be changed in both linking and loading stage. Linking stage might involving library routines and loading stage might increment the base address by the amount of memory used by previous process already present in memory. Note the printed value here is the actual physical address, not a virtual address.</p></li><li><p>dynamic: Physical address is obtained by adding the base register to virtual address. The result has to be less than bound register or an exception will be raised by the processor.</p></li></ul><p>Problem with relocation:</p><p>Even though the concept of relocation is easy to understand, it can easily lead to problems with unused memory space. The biggest problem is <strong>external fragmentation</strong>. When a process finished executing and the memory it occupied is deallocated, it will leave a &ldquo;hole&rdquo; behind and become available for other process to use. The problem is, as the size of the emory of the previously running process can be any random number, the &ldquo;hole&rdquo; it leaves behind may be too small for other processes to fit in. Even if it is big enough for other processes to fit in, they may eventually lead to smaller fragments that are too small for a process to use, which leads to external fragmentation. There&rsquo;s also another problem called <strong>internal fragmentation</strong>, but it won&rsquo;t be discussed here.</p><h2 id=how-to-minimize-external-fragmentation>How to minimize external fragmentation?</h2><p>As we can see, relocation leaves rooms for external fragmentation, which can be a problem since unused spaces will be a waste. Are there any methods we can use to reduce the external fragmentation to minimize external fragmentation and better utilize the free blocks? There&rsquo;re three policies we will discuss that can be used to achieve our goals. A reminder is that they can&rsquo;t completely eliminate external fragmentation but merely minimize it to a certain level.</p><p align=center><img src="https://developer.brewmp.com/sites/default/files/resources/Fragmentation%20and%20low%20memory%20conditions=GUID-99171083-12C5-4E13-8CF6-7500737A188E=1=en=.xml/seq_allocation_deallocation=GUID-0C6A1777-6986-446E-9BC1-771B96F21B68=1=en=Low.png"></p><h2 id=policies>Policies</h2><h3 id=first-fit-policy>First-Fit policy</h3><p>Finding the first free block that can hold the requested amount of memory.</p><p><strong>Requirements</strong>:</p><ul><li><p>All free blocks are tracked in a list and sorted by address.</p></li><li><p>Allocation requires a search throughout the list to find the first free block to fit in the memory.</p></li><li><p>After deallocation, the freed block might need to be merged with other free ones.</p></li></ul><p><strong>Pros</strong>:</p><ul><li><p>very easy to implement.</p></li><li><p>Tend to produces large free block towards the end of the address space.</p></li></ul><p><strong>Cons</strong>:</p><ul><li><p>Allocation is slow.</p></li><li><p>It will eventually lead to fragmentation.</p></li></ul><h3 id=best-fit-policy>Best-Fit policy</h3><p>Finding the smallest free block to allocate the request memory.</p><p><strong>Goal</strong>:</p><ul><li><p>Avoid fragmenting huge free block.</p></li><li><p>Minimize the size of external fragmentation.</p></li></ul><p><strong>Requirements</strong>:</p><ul><li><p>All free blocks are tracked in a list and sorted by address.</p></li><li><p>Allocation still needs a search in the list to find a suitable block to fit in.</p></li><li><p>After deallocation, the freed block may need to be merged with other free blocks.</p></li></ul><p><strong>Pros</strong>:</p><ul><li><p>Kinda simple to implement</p></li><li><p>Reduce external fragmentation, works well when allocations size gets smaller.</p></li></ul><p><strong>Cons</strong>:</p><ul><li><p>Still leaves room for external fragmentation.</p></li><li><p>When it is deallocated and merged back, the new free block may require to be resorted.</p></li></ul><h3 id=worst-fit-policy>Worst-Fit policy</h3><p>Finding the largest free block to allocate request amount of bytes.</p><p><strong>Goal</strong>:</p><ul><li>To avoid having too many small free segments(reduce external fragmentation).</li></ul><p><strong>Requirement</strong>:</p><ul><li><p>Free blocks sorted by size</p></li><li><p>Allocation is fast since the first one is always the largest one.</p></li><li><p>After dealloaction, it needs to check if the new free block needs to be merged back and resort the list.</p></li></ul><p><strong>Pros</strong>:</p><ul><li>Works great if all allocations are of medium size.</li></ul><p><strong>cons</strong>:</p><ul><li><p>Still external fragmentation</p></li><li><p>Deallocation is slow(need to resort and merge)</p></li><li><p>Tends to break down large free blocks, which can lead to failure to allocate large blocks.</p></li></ul><p>Technique to further reduce and eliminate external fragmentation will be discussed later.</p><h2 id=dynamic-relocation>Dynamic Relocation</h2><h3 id=advantages>Advantages</h3><ul><li><p>Processes can move during execution.</p></li><li><p>Processes can grow over time.</p></li><li><p>It&rsquo;s easy to provide protection since we only need two registers.</p></li><li><p>Fast and simple</p></li></ul><h3 id=disadvantages>Disadvantages</h3><ul><li><p>Allocation is contiguous.</p></li><li><p>Sharing memory is hard since there&rsquo;re no way to set base and bound register to be the same for more than one processes.</p></li><li><p>Multiprogramming is limited since all active processes have to be loaded into the memory, which creates another problem that physical memory becomes the limit for how many processes can be loaded. (Swapping might help but the number of active processes still needs to be in the memory.)</p></li><li><p>Need to add memory references every allocation.</p></li><li><p>Memory management is a mess.</p></li><li><p>Everyone has the same permission throughout the address space, which can potential create problems.</p></li></ul></p></section></article><footer class=page-footer><nav class=pagination><a class=prev href=https://www.bodunhu.com/blog/page/4/>« Prev Page</a></nav></footer></main><footer class=footer><span>&copy; 2021 <a href=https://www.bodunhu.com/blog/>std::bodun::blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a> ·
<a href=https://github.com/BDHU rel=noopener>GitHub</a> ·
<a href=/blog/index.xml rel=noopener target=_blank>RSS</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>